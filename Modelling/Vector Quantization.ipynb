{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db207325-e3dc-4afc-bea7-25bc76dac2b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Vector quantization (VQ)\n",
    "\n",
    "\n",
    "Suppose you have recorded sounds at different locations and want to\n",
    "categorize them into similar groups. In other words, you have a\n",
    "stochastic vector $x$ which you want to characterize with a simple\n",
    "description. For example, categories could correspond to office, street,\n",
    "hallway and cafeteria. A classic way for this task is to choose template\n",
    "vectors $c_{k}$, which represents a typical sound in each\n",
    "environment $k$. To categorize the sounds, you then find that template\n",
    "vector which is closest to your recording $x$. In mathematical notation,\n",
    "you search for a $k^{^*}$ by\n",
    "\n",
    "$$ k^* = \\arg\\min_k \\|x-c_k\\|^2. $$\n",
    "\n",
    "The above expression thus calculates the squared error between $x $ and\n",
    "each of the vectors $c_{k}$ and chooses the index $k $ of the\n",
    "vector with the smallest error. The vectors $c_{k}$ then\n",
    "represent a codebook and the vector $x$ is quantized to\n",
    "$c_{k^*}$. This is the basic idea behind *vector quantization,*\n",
    "which is also known as *k-means*. \n",
    "\n",
    "A illustration of a simple vector codebook is shown on the right. The\n",
    "input data is a Gaussian distribution shown with grey dots and the\n",
    "codebook vectors $c_{k}$ with red circles. For each input vector\n",
    "we thus search for the nearest codebook vector and the borders of the\n",
    "regions where input vectors are assigned to a particular codebook vector\n",
    "are illustrated with blue lines. These regions are known as [Voronoi\n",
    "regions](https://en.wikipedia.org/wiki/Voronoi_diagram) and the blue\n",
    "lines are the decision-boundaries between codebook vectors.\n",
    "\n",
    "![vq.png](attachments/175511825.png) \n",
    "\n",
    "Example of a codebook for a 2D Gaussian with 16 code vectors.\n",
    "\n",
    "## Metric for codebook quality\n",
    "\n",
    "Suppose then that you have a large collection of\n",
    "vectors $x_{h}$, and you want to find out how well this codebook\n",
    "represents the input data. The expectation of the squared error is\n",
    "approximately the mean over your data, such that\n",
    "\n",
    "$$ E_h\\left[ \\min_k \\|x_h-c_k\\|^2 \\right] \\approx \\frac 1N\n",
    "\\sum_{h=1}^N \\min_k \\|x_h-c_k\\|^2, $$\n",
    "\n",
    "where $E[ ]$ is the expectation operator and $N$ is the number of\n",
    "input vectors $x_{h}$. Above, we thus find the codebook vector\n",
    "which is closest to $x_{h}$, find its squared error and take the\n",
    "expectation over all possible inputs. This is approximately equal to the\n",
    "mean of those squared errors over a set of input vectors.\n",
    "\n",
    "To find the best set of codebook vectors $c_{k}$, we then need\n",
    "to minimize the mean squared error as\n",
    "\n",
    "$$ \\{c_k^*\\} := \\arg\\min_{\\{c_k\\}}\\, E_h\\left[ \\min_k\n",
    "\\|x_h-c_k\\|^2 \\right]  $$\n",
    "\n",
    "or more specifically, for a dataset as\n",
    "\n",
    "$$ \\{c_k^*\\} := \\arg\\min_{\\{c_k\\}} \\sum_{h=1}^N \\min_k\n",
    "\\|x_h-c_k\\|^2. $$\n",
    "\n",
    "Unfortunately we do not have an analytic solution for this optimization\n",
    "problem, but have to use numerical, iterative methods.\n",
    "\n",
    "## Codebook optimization\n",
    "\n",
    "### Expectation maximization (EM)\n",
    "\n",
    "Classical methods for finding the best codebook are derivatives of\n",
    "expectation maximization (EM), which is based on two alternating steps:\n",
    "\n",
    "*Expectation Maximation (EM)* algorithm:\n",
    "\n",
    "1.  For every vector $x_{h}$ in a large database, find the best\n",
    "    codebook vector $c_{k}$.\n",
    "2.  For every codebook vector $c_{k}$;  \n",
    "    1.  Find all vectors $x_{h}$ assigned to that codevector.\n",
    "    2.  Calculate mean of those vectors.\n",
    "    3.  Assign the mean as a new value for the codevector.\n",
    "3.  If converged then stop, otherwise go to 1.\n",
    "\n",
    "This algorithm is guaranteed to give a codebook at every step which is\n",
    "*not worse* than the previous codebook. That is, at each iteration will\n",
    "improve until it finds a local minimum, where it stops changing. The\n",
    "reason is that each step in the iteration finds a partial best-solution.\n",
    "In the first step, we find the best matching codebook vectors for each\n",
    "data vectors $x_{h}$. In the second step, we find the\n",
    "within-category mean. That is, the new mean is more accurate than the\n",
    "previous codevector in that it reduces the average squared error. If the\n",
    "mean is equal to the previous codevector, then there is no improvement. We prepared a sample code of this algorithm in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6f618e-e1c7-486c-8f19-487af307e31d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b28edcb-7924-486a-9123-46c89deaf7f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d068331dbae741e48b3257fe52138fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectionSlider(continuous_update=False, description='Dataset', options=('islands', 'moons', 'circles', 'swiss…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06dca2d7253749de8b9cb6ca93fc0d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectionSlider(continuous_update=False, description='No. codebook vectors', index=5, options=(1, 2, 4, 8, 16,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slider_dataset = widgets.SelectionSlider(\n",
    "    options=['islands', 'moons', 'circles', 'swiss_roll'],\n",
    "    value='islands',\n",
    "    description='Dataset',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True)\n",
    "\n",
    "slider_codebooks = widgets.SelectionSlider(\n",
    "    options=[1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "    value=32,\n",
    "    description='No. codebook vectors',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True)\n",
    "\n",
    "def on_value_change1(change):\n",
    "    global dataset_name\n",
    "    dataset_name = change['new']\n",
    "slider_dataset.observe(on_value_change1, names='value')\n",
    "display(slider_dataset)\n",
    "\n",
    "def on_value_change2(change):\n",
    "    global num_codebooks\n",
    "    num_codebooks = change['new']\n",
    "slider_codebooks.observe(on_value_change2, names='value')\n",
    "display(slider_codebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce258c8e-9307-4e87-9937-2d3d1716e412",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./attachments/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m num_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      3\u001b[0m data_dim \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_name' is not defined"
     ]
    }
   ],
   "source": [
    "data = np.load(f'./attachments/{dataset_name}.npy')\n",
    "num_iterations = 15\n",
    "data_dim = data.shape[1]\n",
    "\n",
    "permuted_indices = np.random.permutation(data.shape[0])\n",
    "codebooks = data[permuted_indices[0:num_codebooks]]\n",
    "print('codebook vectors = ', codebooks.shape[0])\n",
    "\n",
    "expanded_data = np.expand_dims(data, axis=2)\n",
    "codebook_list = []\n",
    "\n",
    "for iter in range(num_iterations):\n",
    "        new_codebooks = codebooks.copy()\n",
    "        codebook_list.append(new_codebooks)\n",
    "        expanded_codebooks = np.expand_dims(codebooks.T, axis=0)\n",
    "        euclidean_distances = np.sum(np.square(expanded_data - expanded_codebooks), axis=1)\n",
    "        best_codebooks_indices = np.argmin(euclidean_distances, axis=1)\n",
    "\n",
    "        for i in range(codebooks.shape[0]): # codebook update step\n",
    "            indices = np.where(best_codebooks_indices == i)[0]\n",
    "            mean_assigned_codebooks = np.mean(data[indices], axis=0, keepdims=True)\n",
    "            codebooks[i] = mean_assigned_codebooks.copy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1 = ax.scatter([],[])\n",
    "line2 = ax.scatter([],[],marker='X', c='red')\n",
    "ax.set_xlim(data[:,0].min()-1, data[:,0].max()+1)\n",
    "ax.set_ylim(data[:,1].min()-1, data[:,1].max()+1)\n",
    "plt.close()\n",
    "\n",
    "def animate(frame_num):\n",
    "    line1.set_offsets(data)\n",
    "    line2.set_offsets(codebook_list[frame_num])\n",
    "    return (line1, line2)\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=len(codebook_list), interval=500)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c9b3c-809e-42f8-abb5-fa02cb386de6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As noted above, this algorithm is the basis to most vector quantization\n",
    "codebook optimization algorithms. There are a multiple reasons why this\n",
    "simple algorithm is usually not sufficient alone. Most importantly, the\n",
    "above algorithm is slow to converge to a stable solution *and* it often\n",
    "finds a local minimum instead of a global minimum.\n",
    "\n",
    "To improve performance, we can apply several heuristic approaches. For\n",
    "example, we can start with a small codebook $ \\{ c_k \\}_{k=1}^K $\n",
    "of $K$ elements and optimize it with the EM algorithm. We then split the\n",
    "codebook into two, offset by a small delta $d$, such that $\n",
    "\\|d\\|<\\epsilon $ and make the new codebook $ \\{ \\hat c_k\n",
    "\\}_{k=1}^{2K} := \\{ c_k,\\, c_k+d \\}_{k=1}^K $ of 2$K$ elements.\n",
    "We then rerun the EM algorithm on the new codebook. The codebook thus\n",
    "doubles in size at every iteration and we continue until we have the\n",
    "desired codebook size. There is a sample code for this approach in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e5f63-628f-426a-8bf7-47fd564a0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8553aa-d5c6-42ed-a27a-c82ec07c2220",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "slider_dataset = widgets.SelectionSlider(\n",
    "    options=['islands', 'moons', 'circles', 'swiss_roll'],\n",
    "    value='islands',\n",
    "    description='Dataset',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True)\n",
    "\n",
    "slider_init_codebooks = widgets.SelectionSlider(\n",
    "    options=[1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
    "    value=1,\n",
    "    description='Init codebook vectors',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True)\n",
    "\n",
    "slider_desired_codebooks = widgets.SelectionSlider(\n",
    "    options=[2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "    value=32,\n",
    "    description='Desired codebook vectors',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True)\n",
    "\n",
    "def on_value_change1(change):\n",
    "    global dataset_name\n",
    "    dataset_name = change['new']\n",
    "slider_dataset.observe(on_value_change1, names='value')\n",
    "display(slider_dataset)\n",
    "\n",
    "def on_value_change2(change):\n",
    "    global initial_num_codebooks\n",
    "    initial_num_codebooks = change['new']\n",
    "slider_init_codebooks.observe(on_value_change2, names='value')\n",
    "display(slider_init_codebooks)\n",
    "\n",
    "def on_value_change3(change):\n",
    "    global desired_num_codebooks\n",
    "    desired_num_codebooks = change['new']\n",
    "slider_desired_codebooks.observe(on_value_change3, names='value')\n",
    "display(slider_desired_codebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f96442-1ffc-4b5f-947e-67918cab6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if desired_num_codebooks <= initial_num_codebooks:\n",
    "    raise ValueError(\"Initial No. of codebook vectors must be smaller than desired No. of codebook vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81acb94a-5e3e-45a7-9162-2e49212a30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'./attachments/{dataset_name}.npy')\n",
    "num_iterations = 8\n",
    "data_dim = data.shape[1]\n",
    "delta = 1e-2\n",
    "expansion_times = int(np.round(np.log2(desired_num_codebooks) - np.log2(initial_num_codebooks)))\n",
    "\n",
    "permuted_indices = np.random.permutation(data.shape[0])\n",
    "codebooks = data[permuted_indices[0:initial_num_codebooks]]\n",
    "print('initial codebook vectors = ', initial_num_codebooks)\n",
    "print('desired codebook vectors = ', desired_num_codebooks)\n",
    "\n",
    "data_reshaped = np.expand_dims(data, axis=2)\n",
    "codebook_list = []\n",
    "\n",
    "for expansion_idx in range(expansion_times+1):\n",
    "\n",
    "    for iter in range(num_iterations):\n",
    "        new_codebooks = codebooks.copy()\n",
    "        codebook_list.append(new_codebooks)\n",
    "        codebooks_reshaped = np.expand_dims(codebooks.T, axis=0)\n",
    "        euclidean_distances = np.sum(np.square(data_reshaped - codebooks_reshaped), axis=1)\n",
    "        best_codebooks_indices = np.argmin(euclidean_distances, axis=1)\n",
    "\n",
    "        for i in range(codebooks.shape[0]): # codebook update step\n",
    "            indices = np.where(best_codebooks_indices == i)[0]\n",
    "            mean_assigned_codebooks = np.mean(data[indices], axis=0, keepdims=True)\n",
    "            codebooks[i] = mean_assigned_codebooks.copy()\n",
    "\n",
    "    expanded_codebooks = np.concatenate((codebooks, codebooks + delta), axis=0)\n",
    "    codebooks = expanded_codebooks.copy()\n",
    "\n",
    "final_codebook_list = [codebook_list[0]] + [codebook_list[num_iterations-1]] + codebook_list[num_iterations:]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1 = ax.scatter([],[])\n",
    "line2 = ax.scatter([],[],marker='X', c='red')\n",
    "ax.set_xlim(data[:,0].min()-1, data[:,0].max()+1)\n",
    "ax.set_ylim(data[:,1].min()-1, data[:,1].max()+1)\n",
    "plt.close()\n",
    "\n",
    "def animate(frame_num):\n",
    "    line1.set_offsets(data)\n",
    "    line2.set_offsets(final_codebook_list[frame_num])\n",
    "    return (line1, line2)\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=len(final_codebook_list), interval=500)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb866bc-0549-4fce-bd51-9fc496f66e89",
   "metadata": {},
   "source": [
    "The advantage of this approach is that it focuses attention to the big\n",
    "bulk of datapoints $x_{k}$, and ignores outliers. The outcome is\n",
    "then expected to be more stable and the likelihood of converging to a\n",
    "local minimum is smaller. The downside is that with this approach it is\n",
    "then more difficult to find small separated islands. That is, because\n",
    "the initial codebook is near the center of the whole mass of datapoints,\n",
    "adding a small delta to the codebook vectors keeps the new codevectors\n",
    "near the center-of-mass. \n",
    "\n",
    "Conversely, we can start with a large codebook, say treat the whole\n",
    "input database $x_{k}$ as a codebook. We can then iteratively\n",
    "merge pairs of points which are close to each other, until the codebook\n",
    "is reduced to the desired size. Needless to say, this will be a slow\n",
    "process if the database is large, but will be very efficient in finding\n",
    "separated islands of points. There is a sample code for this approach in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a654ed-6325-44ff-a679-27334442767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ae744-28ea-4fbe-82b6-3d6849cda424",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "slider_dataset = widgets.SelectionSlider(\n",
    "    options=['islands', 'moons', 'circles', 'swiss_roll'],\n",
    "    value='islands',\n",
    "    description='Dataset',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True)\n",
    "\n",
    "slider_init_codebooks = widgets.SelectionSlider(\n",
    "    options=[2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "    value=64,\n",
    "    description='Init codebook vectors',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True)\n",
    "\n",
    "slider_desired_codebooks = widgets.SelectionSlider(\n",
    "    options=[1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
    "    value=4,\n",
    "    description='Desired codebook vectors',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True)\n",
    "\n",
    "def on_value_change1(change):\n",
    "    global dataset_name\n",
    "    dataset_name = change['new']\n",
    "slider_dataset.observe(on_value_change1, names='value')\n",
    "display(slider_dataset)\n",
    "\n",
    "def on_value_change2(change):\n",
    "    global initial_num_codebooks\n",
    "    initial_num_codebooks = change['new']\n",
    "slider_init_codebooks.observe(on_value_change2, names='value')\n",
    "display(slider_init_codebooks)\n",
    "\n",
    "def on_value_change3(change):\n",
    "    global desired_num_codebooks\n",
    "    desired_num_codebooks = change['new']\n",
    "slider_desired_codebooks.observe(on_value_change3, names='value')\n",
    "display(slider_desired_codebooks)\n",
    "\n",
    "def merging_codebooks(codebooks):\n",
    "    codebooks1 = np.expand_dims(codebooks, axis=2)\n",
    "    codebooks2 = np.expand_dims(codebooks.T, axis=0)\n",
    "\n",
    "    distances = np.sum(np.square(codebooks1 - codebooks2), axis=1)\n",
    "    distances[distances == 0] = np.inf\n",
    "    distances = distances.reshape(-1, 1)\n",
    "\n",
    "    sorted_indcies = np.argsort(distances, axis=0)\n",
    "    merged_codebooks = np.zeros((codebooks.shape[0] // 2, data_dim))\n",
    "    selected_codebooks_indices = []\n",
    "    counter = 0\n",
    "\n",
    "    for j in range(sorted_indcies.shape[0]):\n",
    "        row_idx = np.floor(sorted_indcies[j] / codebooks.shape[0]).astype(np.int64)[0]\n",
    "        column_idx = np.remainder(sorted_indcies[j], codebooks.shape[0])[0]\n",
    "\n",
    "        if (row_idx in selected_codebooks_indices) or (column_idx in selected_codebooks_indices):\n",
    "            pass\n",
    "        else:\n",
    "            selected_codebooks_indices.extend((row_idx, column_idx))\n",
    "            merged_codebooks[counter] = codebooks[row_idx] # defines new codebooks which are exactly data points\n",
    "            counter += 1\n",
    "\n",
    "        if counter == codebooks.shape[0] // 2:\n",
    "            break\n",
    "\n",
    "    return merged_codebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04a0fa-1b1a-49eb-8e57-ec9847bae9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if desired_num_codebooks >= initial_num_codebooks:\n",
    "    raise ValueError(\"Desired No. of codebook vectors must be smaller than initial No. of codebook vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81f014-86e4-4385-aa5d-105e21f97ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'./attachments/{dataset_name}.npy')\n",
    "num_data_samples = data.shape[0]\n",
    "num_iterations = 10\n",
    "data_dim = data.shape[1]\n",
    "delta = 1e-2\n",
    "\n",
    "merging_times = int(np.round(np.log2(initial_num_codebooks) - np.log2(desired_num_codebooks)))\n",
    "\n",
    "splitted_data = np.array_split(data, initial_num_codebooks)\n",
    "\n",
    "codebooks = np.zeros((initial_num_codebooks, data_dim), dtype=np.float64)\n",
    "for i in range(initial_num_codebooks):\n",
    "    codebooks[i] = np.mean(splitted_data[i], axis=0)\n",
    "print('initial codebook vectors = ', initial_num_codebooks)\n",
    "print('desired codebook vectors = ', desired_num_codebooks)\n",
    "\n",
    "data_reshaped = np.expand_dims(data, axis=2)\n",
    "codebook_list = []\n",
    "\n",
    "for merge_idx in range(merging_times+1):\n",
    "\n",
    "    for iter in range(num_iterations):\n",
    "        new_codebooks = codebooks.copy()\n",
    "        codebook_list.append(new_codebooks)\n",
    "        reshaped_codebooks = np.expand_dims(codebooks.T, axis=0)\n",
    "\n",
    "        euclidean_distances = np.sum(np.square(data_reshaped - reshaped_codebooks), axis=1)\n",
    "\n",
    "        best_codebooks_indices = np.argmin(euclidean_distances, axis=1)\n",
    "        unique_best_indices = np.unique(best_codebooks_indices)\n",
    "\n",
    "        for i in range(codebooks.shape[0]):\n",
    "            indices = np.where(best_codebooks_indices == i)[0]\n",
    "            if indices.size == 0:\n",
    "                random_idx = np.random.choice(unique_best_indices)\n",
    "                codebooks[i] = codebooks[random_idx] + delta\n",
    "            else:\n",
    "                mean_assigned_codebooks = np.mean(data[indices], axis=0, keepdims=True)\n",
    "                codebooks[i] = mean_assigned_codebooks.copy()\n",
    "\n",
    "    merged_codebooks = merging_codebooks(codebooks)\n",
    "    codebooks = merged_codebooks.copy()\n",
    "\n",
    "    if codebooks.shape[0] < desired_num_codebooks:\n",
    "        break\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1 = ax.scatter([],[])\n",
    "line2 = ax.scatter([],[],marker='X', c='red')\n",
    "ax.set_xlim(data[:,0].min()-1, data[:,0].max()+1)\n",
    "ax.set_ylim(data[:,1].min()-1, data[:,1].max()+1)\n",
    "plt.close()\n",
    "\n",
    "def animate(frame_num):\n",
    "    line1.set_offsets(data)\n",
    "    line2.set_offsets(codebook_list[frame_num])\n",
    "    return (line1, line2)\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=len(codebook_list), interval=500)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0b9ff-bf4d-42c3-b0dd-e183b33da4ee",
   "metadata": {},
   "source": [
    "In any case, optimization of vector codebooks is a difficult task and we\n",
    "have no practical algorithms which would be guaranteed to find the\n",
    "global optimum. Like in many other machine learning problems, optimizing\n",
    "the codebook is very much about learning to know your data. You should\n",
    "first use one algorithm and then analyse the output to find out what can\n",
    "be improved, and keep repeating this optimization and analyse process\n",
    "until the output is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca236fd7-cacb-40a1-b676-4342a832e4b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Optimization with machine learning platforms\n",
    "\n",
    "A modern approach to modelling is [machine learning](content:nn), where complex phenomena are modelled with neural networks. Typically they are trained with [gradient-descent](https://en.wikipedia.org/wiki/Gradient_descent) type methods, where parameters are iteratively nudged towards the minimum, by following the steepest gradient. Since such gradients can be automatically derived on machine learning platforms (using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule)), they can be applied on very complex models. Consequently, they have become very popular and succesful. \n",
    "\n",
    "The same type of training can be readily applied to vector quantizers as well. However, there is a practical problem with this approach. Estimation of the gradients of the parameters with the chain-rule requires that *all* intermediate gradients are non-zero. Quantizers are however piece-wise constant such that their gradients are uniformly zero, thus disabling the chain rule and gradient descent for all parameters which lie behind the quantizer in the computational graph. A practical solution is known as straight through estimator, where gradients are passed unchanged through the quantizer. This approximation is simple to implement and provides often adequate performance. In the following, we prepared a sample code to optimize codebook of a vector quantizer with PyTorch machine learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2d89f-c1cd-4907-af64-719859937490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d074d62f-2006-466c-99e4-7e5c6295298f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "While this approach converges slower than the EM-algorithm, it is often beneficial since it allows optimization of an entire system to a single goal. That is, if we have several modules in a system, it is beneficial if their joint interaction is taken into account in the optimization. If modules are optimized independently, it is difficult to anticpate all interactions and the system performance can remain far from optimal.\n",
    "\n",
    "## Algorithmic complexity\n",
    "\n",
    "Vector quantization is manageable for relatively small codebooks of,\n",
    "say, $K=32$ codevectors. That corresponds to 5 bits of information. For\n",
    "many applications, that does not give sufficient accuracy - the mean\n",
    "squared error is too large. For example, the linear predictive models in\n",
    "speech coding could be quantized with 30 bits, which corresponds to $\n",
    "K=2^{30}\\approx 10^9 $ codevectors. To find the best codevector for a\n",
    "vector $x$ of length $N=16$, we would then need to calculate the\n",
    "distance between every codebook vector and $x$, which amounts to\n",
    "approximately $ 16\\times10^9= 1.6\\times10^{10} $ operations. That\n",
    "is infeasible in on-line applications on mobile devices. Instead, we\n",
    "need to find a simpler method which retains the best aspects of the\n",
    "algorithm, but reduces algorithmic complexity.\n",
    "\n",
    "A heuristic approach is to use successive codebooks, where at each\n",
    "iteration, we quantize the error of the last iteration. That is, let's\n",
    "say that on the first iteration we have 8 bits, corresponding to a\n",
    "codebook $c_{k}$ of $K=256$ vectors. We find the best matching\n",
    "codevector $c_{k^*}$ and calculate the residual $\n",
    "x':=x-c_{k^*} $ . In the second stage, we would then find the best\n",
    "matching vector for $x'$ from a second codebook $c_{k}'$. We can\n",
    "add as many layers of codebooks as we want until the desired number of\n",
    "bits has been consumed. This approach is known as a *multi-stage vector\n",
    "quantizer*.\n",
    "\n",
    "Where ordinary vector quantization can find the optimal solution, split\n",
    "vector quantization generally does not give a global optimum. It does\n",
    "give good solutions, though, but with an algorithmic complexity which\n",
    "very much lower than ordinary vector quantization. For example, in the\n",
    "above example of 30 bits, we could assign three consecutive layers of\n",
    "codebooks with 10 bits / $K=1024$ each, such that the overall complexity\n",
    "is $ 3\\times 16\\times 2^{10} \\approx 5\\times10^4, $ which gives\n",
    "an improvement with a factor of $ 3.5\\times10^5. $ Given that the\n",
    "reduction in accuracy is manageable, this is a major improvement in\n",
    "complexity.\n",
    "\n",
    "\n",
    "\n",
    "## Applications\n",
    "\n",
    "Probably the most important application where vector quantization is\n",
    "used in speech processing, is [speech\n",
    "coding](content:telecom) with [Code-excited\n",
    "linear prediction (CELP)](content:CELP), where\n",
    "\n",
    "-   [linear predictive coefficients (LPC)](content:linearprediction) are\n",
    "    transformed to line spectral frequencies (LSFs), which are often\n",
    "    encoded with multi-stage vector quantizers. \n",
    "-   gains (signal energy) of the residual and long term prediction are\n",
    "    jointly encoded with a single stage vector quantizer.\n",
    "\n",
    "Other typical applications include\n",
    "\n",
    "-   In optimization of [Gaussian mixture models\n",
    "    (GMMs)](content:gmm), it is useful to use vector\n",
    "    quantization to find a first-guess of the means of each mixture.\n",
    "\n",
    "\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The benefit of vector quantization is that it is a simple algorithm\n",
    "which gives high accuracy. In fact, for quantizing complicated data,\n",
    "vector quantization is (in theory) optimal in fixed-rate coding\n",
    "applications. It is simple in the sense that an experienced engineer can\n",
    "implement it in a matter of hours. Downsides with vector quantization\n",
    "include\n",
    "\n",
    "-   Complexity; for accurate quantization you need prohibitively large\n",
    "    codebooks. The method therefore does not scale up nicely to big\n",
    "    problems.\n",
    "-   Difficult optimization;  \n",
    "    -   Training data; The amount of data needed to optimize a vector\n",
    "        codebook is large. Each codebook vector must be assigned to a\n",
    "        large number of data vectors, such that calculation of the mean\n",
    "        (in the EM algorithm) is meaningful.\n",
    "    -   Convergence; we have no assurance that optimization algorithms\n",
    "        find the global optimum and we have no assurance that local\n",
    "        minima are \"good enough\".\n",
    "-   Lack of flexibility; the codebook has a fixed size. If we would like\n",
    "    to use codebooks of different sizes, for example, if we want to\n",
    "    transmit data with a variable bit-rate, then we have to optimize and\n",
    "    store a large codebook for *every possible bitrate*.\n",
    "-   Blindness to inherent structures; this model describes data with a\n",
    "    codebook, without any deeper understanding of what the data looks\n",
    "    like within each category. For example, say we have two classes,\n",
    "    speech and non-speech. Even if speech is very flexible, the\n",
    "    non-speech class is much, much larger. Speech is a very small subset\n",
    "    of all possible sounds. Therefore, the within-class variance will be\n",
    "    much larger in the non-speech class. Consequently, the accuracy in\n",
    "    the non-speech class would be much lower.  \n",
    "    As a consequence, we would be tempted to increase the number of\n",
    "    codevectors such that we get uniform accuracy in both classes. But\n",
    "    then we loose the correspondence between codevectors and natural\n",
    "    descriptions of the signal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
