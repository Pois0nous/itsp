
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3.6. Cepstrum and MFCC &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.7. Linear prediction" href="Linear_prediction.html" />
    <link rel="prev" title="3.5. Autocorrelation and autocovariance" href="Autocorrelation_and_autocovariance.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="Representations.html">
   3. Basic Representations
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="Waveform.html">
     3.1. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Windowing.html">
     3.2. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Signal_energy_loudness_and_decibel.html">
     3.3. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Spectrogram_and_the_STFT.html">
     3.4. Spectrogram and the STFT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Autocorrelation_and_autocovariance.html">
     3.5. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     3.6. Cepstrum and MFCC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear_prediction.html">
     3.7. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Fundamental_frequency_F0.html">
     3.8. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Zero-crossing_rate.html">
     3.9. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Deltas_and_Delta-deltas.html">
     3.10. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.11. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Jitter_and_shimmer.html">
     3.12. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
     7.2. Inverse filtering for glottal activity estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Analysis/Measurements_for_medical_applications.html">
     7.3. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Analysis/Glottal_inverse_filtering.html">
       7.3.1. Glottal inverse filtering
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Forensic_analysis.html">
     7.4. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Voice_activity_detection_VAD.html">
     8.1. Voice activity detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Recognition_and_Verification.html">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Paralinguistic_speech_processing.html">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Concatenative_speech_synthesis.html">
     9.2. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Statistical_parametric_speech_synthesis.html">
     9.3. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Basic_tools.html">
     10.2. Basic tools
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.3. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Entropy_coding.html">
       10.3.4. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.3.5. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Code-excited_linear_prediction_CELP.html">
     10.4. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Frequency-domain_coding.html">
     10.5. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://landbot.io/blog/guide-to-conversational-design/">
   Chatbots / Conversational design (external link)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Security_and_privacy_in_speech_technology.html">
   13. Security and privacy in speech technology
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Representations/Cepstrum_and_MFCC.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   3.6.1. Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-cepstrum">
   3.6.2. The cepstrum
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mel-frequency-cepstral-coefficients-mfccs">
   3.6.3. Mel-Frequency Cepstral Coefficients (MFCCs)
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Cepstrum and MFCC</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   3.6.1. Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-cepstrum">
   3.6.2. The cepstrum
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mel-frequency-cepstral-coefficients-mfccs">
   3.6.3. Mel-Frequency Cepstral Coefficients (MFCCs)
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="cepstrum-and-mfcc">
<h1><span class="section-number">3.6. </span>Cepstrum and MFCC<a class="headerlink" href="#cepstrum-and-mfcc" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2><span class="section-number">3.6.1. </span>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<p>Power <a class="reference internal" href="Spectrogram_and_the_STFT.html"><span class="doc std std-doc">spectra</span></a> of <a class="reference internal" href="Windowing.html"><span class="doc std std-doc">windows</span></a> of
speech signals contain information about the most important features of
speech signals like the identity of vowels. Unfortunately, the range of
values is very non-uniform. In fact, by directly looking at power
spectra, we do not really see anything informative.</p>
<p>The logarithmic spectrum, on the other hand, is a much more accessible
representation. It is not only more visual, but importantly, the
logarithm approximates roughly the sensitivity of the ear, such that
logarithmic spectra can be used to assess auditory importance of
spectral features. The logarithmic spectrum visualizes spectral content
such that the magnitude of values is approximately uniform throughout
the spectrum.</p>
<p>Windowed speech signal <img alt="window" src="../_images/149886069.png" /></p>
<p>The only exception is zeros and other very small values in the magnitude
spectrum, which give negative infinities or arbitrarily large negative
values in the log spectrum. Though such values are “difficult” for
visualizations, they are inconsequential for auditory perception and can
be often ignored. However, for computations in the log-spectrum,
arbitrarily large negative values are a problem.</p>
<p>To reduce the likelihood of such problematic values, we can use for
example an energy bias similar to the <a class="reference internal" href="Waveform.html"><span class="doc std std-doc">mu-law</span></a> rule or
integrate energies over frequencies. Specifically, instead of
<span class="math notranslate nohighlight">\(y=\log(\|x\|^2)\)</span>, we can use <span class="math notranslate nohighlight">\(y=\log(\|x\|^2+e)\)</span>,
where <span class="math notranslate nohighlight">\(e\)</span> is a small positive number. The output <span class="math notranslate nohighlight">\(y\)</span> will then never go
lower than a threshold <span class="math notranslate nohighlight">\(y\geq \log(e)\)</span>.</p>
<p>In addition, we can integrate (or sum) neighboring frequencies, for
example as</p>
<div class="math notranslate nohighlight">
\[
 y_k=\log\left(\gamma\|x_{k-1}\|^2+\|x_k\|^2+\gamma\|x_{k+1}\|^2+\epsilon\right),
\]</div>
<p>where 0&lt;γ&lt;1 is a scalar. The likelihood that all three coefficients,
<span class="math notranslate nohighlight">\(x_{k-1}\)</span>, <span class="math notranslate nohighlight">\(x_{k}\)</span> and <span class="math notranslate nohighlight">\(x_{k+1}\)</span>, are all
simultaneously near zero is much smaller than that one of them is near
zero. The likelihood that <span class="math notranslate nohighlight">\(y\)</span> gets near the threshold <span class="math notranslate nohighlight">\(y\geq \log(e)\)</span> is
therefore small. Such methods can be used to improve the robustness of
log-spectra, such that we can make computations to them with reliable
results.</p>
<p>Power spectrum of speech
segment <img alt="spectrum" src="../_images/149886068.png" /></p>
</div>
<div class="section" id="the-cepstrum">
<h2><span class="section-number">3.6.2. </span>The cepstrum<a class="headerlink" href="#the-cepstrum" title="Permalink to this headline">¶</a></h2>
<p>We now see that the log-spectrum has plenty of structure. It is a more
or less continuous signal, owing to a large part, to the smoothing
effect of windowing. In the illustration to the right, it has also a
periodic structure, which corresponds to the harmonic structure of the
signal caused by the fundamental frequency. Importantly, it has also a
macro-level structure; by connecting the peaks of the harmonic
structure, we see that the signal forms peaks and valleys, which
correspond to the resonances of the vocal tract. These peaks are known
as formants and they can be used to uniquely identify all vowels. They
are therefore of particular interest. Capturing or quantifying such
macro-level structures is important because of the connection with the
vowel-identity.</p>
<p>One way of evaluating periodic structures in a signal on different
scales is to use the Fourier transform. Specifically, we can take the
discrete Fourier transform (DFT) or the discrete cosine transform (DCT)
of the the log-spectrum, to obtain a representation known as
the <em>cepstrum</em>. The name attempts to be an amusing reflection of the
fact that this representation is a complicated rearrangement of
time-frequency transforms. In technical terms, for a time signal <span class="math notranslate nohighlight">\(x(t)\)</span>,
the cepstrum is defined as</p>
<div class="math notranslate nohighlight">
\[ \text{power cepstrum of signal} {\displaystyle
=\left\|{\mathcal {F}}^{-1}\left\{\log \left(\left\|{\mathcal
{F}}\{x(t)\}\right\|^{2}\right)\right\}\right\|^{2},} \]</div>
<p>where <span class="math notranslate nohighlight">\( {\mathcal {F}}\{\cdot\} \)</span> represent the Fourier
transform and <span class="math notranslate nohighlight">\( {\mathcal {F}}^{-1}\{\cdot\} \)</span> its inverse.</p>
<p>It is worth repeating that the cepstrum involves two time-frequency
transforms. The cepstrum of a time-signal is therefore in some sense
similar to the time-domain. The x-axis of a cepstrum is known as
the *quefrency-*axis and it is expressed typically in the
unit *seconds. *</p>
<p>In the cepstrum, the low quefrencies contain information about the
slowly-changing features of the log-spectrum. That is, information of
the formants will lie at the low-quefrency end of the cepstrum.
Interpretation of formant information in the cepstrum is, however,
non-trivial. For example, locations of the formants, on the
frequency-axis, are encoded in the cepstrum, but the information is
distributed over several coefficients such that extracting that
information is not easy.</p>
<p>Log-spectrum of speech
segment <img alt="dbdft" src="../_images/149886067.png" /></p>
<p>Cepstrum of speech segment
<img alt="cepstrum" src="../_images/149886066.png" /></p>
<p>The most visually prominent feature in this cepstrum is the peak near
quefrency 7 ms. It corresponds to a fundamental frequency of 1000/(7 s)
= 143 Hz. That fundamental frequency is clearly visible also in the
log-spectrum above, where the comb-structure has peaks at approximately
multiples of 143 Hz.</p>
<p>A second useful piece of information in the cepstrum is the harmonic
structure of the log-spectrum. Recall that the fundamental frequency is
visible as a comb-structure in the log-spectrum. The comb-structure, in
turn, is a periodic structure and the Fourier transform is an excellent
tool for extracting such structures. We can thus expect to see a peak in
the cepstrum at the quefrency corresponding to the pitch-period length
(in seconds). If we assume that fundamental frequencies <span class="math notranslate nohighlight">\(F_{0}\)</span>
are in the range 80 to 450 Hz, then the corresponding peak in the
cepstrum should lie at quefrency <span class="math notranslate nohighlight">\(1/F_{0}\)</span> and they range from
2.2 to 12.5 milliseconds.</p>
<p>Estimating the fundamental frequency in the cepstrum is, in fact, very
simple and relatively robust. We would just need to find the highest
peak of the cepstrum in the appropriate quefrency-range. <span class="xref myst">Fundamental
frequency estimation</span> will be
discussed further in a separate section.</p>
</div>
<div class="section" id="mel-frequency-cepstral-coefficients-mfccs">
<h2><span class="section-number">3.6.3. </span>Mel-Frequency Cepstral Coefficients (MFCCs)<a class="headerlink" href="#mel-frequency-cepstral-coefficients-mfccs" title="Permalink to this headline">¶</a></h2>
<p>To further improve on the cepstral representation, we can include more
information about auditory perception into the model. Specifically, by
introducing information about human perception, we focus the model on
that part of the information which human listeners would find important.
The log-spectrum already takes into account perceptual sensitivity on
the magnitude axis, by expressing magnitudes on the logarithmic-axis.
The other dimension is then the frequency axis.</p>
<p>There exists a multitude of different criteria with which to quantify
accuracy on the frequency scale and there are, correspondingly, a
multitude of perceptually motivated frequency scales including the
<a class="reference external" href="https://en.wikipedia.org/wiki/Equivalent_rectangular_bandwidth">equivalent rectangular
bandwidth</a>
(ERB) scale, the <a class="reference external" href="https://en.wikipedia.org/wiki/Bark_scale">Bark</a> scale,
and the <a class="reference external" href="https://en.wikipedia.org/wiki/Mel_scale">mel</a>-scale. Probably
through an abritrary choice mainly due to tradition, in this context we
will focus on the mel-scale. This scale describes the perceptual
distance between pitches of different frequencies.</p>
<p>A classical approximation is to define the frequency-to-mel transform
function for a frequency <span class="math notranslate nohighlight">\(f \)</span> as</p>
<div class="math notranslate nohighlight">
\[ m=2595\,\log_{10}\left(1+\frac f{700}\right). \]</div>
<p>The inverse transform can be readily derived as</p>
<div class="math notranslate nohighlight">
\[ f = 700\,\left(10^{\frac m{2595}}-1\right). \]</div>
<p>By taking equally spaced points <span class="math notranslate nohighlight">\(m_{k}\)</span>, using the above
formula, we can then find frequency points <span class="math notranslate nohighlight">\(f_{k }\)</span> whose
perceptual distance is equal. In other words, to sample the log-spectrum
with a perceptual scale, we pick samples at frequencies <span class="math notranslate nohighlight">\(f_{k}\)</span>.
(An implementation detail is that usually, we want to avoid that the
distance between subsequent <span class="math notranslate nohighlight">\(f_{k}\)</span> would be smaller than the
highest distance between harmonic peaks, such that the model focuses on
the macro structure and ignores the fundamental frequency. Usually, a
minimum threshold is therefore applied on <span class="math notranslate nohighlight">\(f_{k}\)</span>.)</p>
<p>Mel scale
<img alt="melscale" src="../_images/149886076.png" /></p>
<p>However, if we would only pick samples at frequencies <span class="math notranslate nohighlight">\(f_{k}\)</span>,
we would loose all the other information. Therefore, similarly as in the
frequency-integration approach above, we take a weighted sum of energies
near the target frequency <span class="math notranslate nohighlight">\(f_{k}\)</span> as</p>
<div class="math notranslate nohighlight">
\[ u_k = \sum_{h=f_{k-1}+1}^{f_{k+1}-1} w_{k,h} \|x_h\|^2, \]</div>
<p>where scalars <span class="math notranslate nohighlight">\(w_{k,h}\)</span> are a weighting parameters. Then we get,
simultaneously, the benefit of a robust estimate due to energy
integration, but also apply a perceptual frequency scale.</p>
<p>Finally, by taking the discrete cosine transform (DCT) of the parameters
<span class="math notranslate nohighlight">\(u_{k}\)</span>, we obtain the representation known as *mel-frequency
cepstral coefficients *(MFCCs). The benefit of the DCT at the end is to
approximately decorrelate the signal, such that the MFC coefficients are
not correlated with each other.</p>
<p>The weighting coefficients <span class="math notranslate nohighlight">\(w_{k,h}\)</span> are usually chosen as
triangular functions as</p>
<div class="math notranslate nohighlight">
\[\begin{split} w_{k,h} = \begin{cases} \frac{h-f_{k-1}}{f_k - f_{k-1}} &amp;
\text{for}\quad f_{k-1} &lt; h\leq f_k, \\ \frac{f_{k+1}
-h}{f_{k+1} - f_{k}} &amp; \text{for}\quad f_{k}&lt; h\leq f_{k+1},
\\0 &amp; \text{otherwise}. \end{cases} \end{split}\]</div>
<p>The process of acquiring MFCCs from a spectrogram is illustrated on the
right, where on the top, there is a triangular filterbank placed at
linear steps on the mel-frequency scale. The second figure shows the
spectrogram of a speech segment. When each window of that spectrogram is
multiplied with the triangular filterbank, we obtain the mel-weighted
spectrum, illustrated in the third figure. Here we see that the
gross-shape of the spectrogram is retained, but the fine-structure has
been smoothed out. In essence, this process thus removes the details
related to the harmonic structure. Since the identity of phonemes such
as vowels is determined based on macro-shapes in the spectrum, the MFCCs
thus preserve that type of information and remove “unrelated”
information such as the pitch.</p>
<p>The fourth figure illustrates the outcome once the mel-weighted
spectrogram is multiplied with a DCT to obtain the final MFCCs. Where
the mel-weighted spectrogram does retain the original shape of the
spectrum, the MFCCs do not offer such easy interpretations. It is an
abstract domain, which contains information about the spectral envelope
of the speech signal.</p>
<p>Though the argumentation for the MFCCs is not without problems, it has
become <em>the most used feature</em> in speech and audio recognition
applications. It is used because it works and because it has relatively
low complexity and it is straightforward to implement. Simply stated,</p>
<p>if you’re unsure which inputs to give to a speech and audio recognition
engine, try first the MFCCs.</p>
<p>The beneficial properties of the MFCCs include:</p>
<ul class="simple">
<li><p>Quantifies the gross-shape of the spectrum (the spectral envelope),
which is important in, for example, identification of vowels. At the
same time, it removes fine spectral structure (micro-level
structure), which is often less important. It thus focuses on that
part of the signal which is typically most informative.</p></li>
<li><p>Straightforward and computationally reasonably efficient
calculation.</p></li>
<li><p>Their performance is well-tested and -understood.</p></li>
</ul>
<p>Some of the issues with the MFCC include:</p>
<ul class="simple">
<li><p>The choice of perceptual scale is not well-motivated. Scales such as
the ERB or gamma-tone filterbanks might be better suited. However,
these alternative filterbanks have not demonstrated consistent
benefit, whereby the mel-scale has persisted.</p></li>
<li><p>MFCCs are not robust to noise. That is, the performance of MFCCs in
presence of additive noise, in comparison to other features, has not
always been good.</p></li>
<li><p>The choice of triangular weighting filters <span class="math notranslate nohighlight">\(w_{k,h}\)</span> is
arbitrary and not based on well-grounded motivations. Alternatives
have been presented, but they have not gained popularity, probably
due to minor effect on outcome.</p></li>
<li><p>The MFCCs work well in analysis but for synthesis, they are
problematic. Namely, it is difficult to find an inverse transform
(from MFCCs to power spectra) which is simultaneously unbiased
(=accurate) and congruent with its physical representation (=power
spectrum must be positive).</p></li>
</ul>
<p>Triangular filterbank <span class="math notranslate nohighlight">\(w_{k,h}\)</span>
<img alt="melfilterbank" src="../_images/149886080.png" /></p>
<p>Spectrogram of a segment of speech
<img alt="spectrogram" src="../_images/151503131.png" /></p>
<p>Spectrogram after multiplication with mel-weighted filterbank
<img alt="melspectrogram" src="../_images/151503197.png" /></p>
<p>Corresponding MFCCs
<img alt="mfcc" src="../_images/151503198.png" /></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Representations"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Autocorrelation_and_autocovariance.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">3.5. </span>Autocorrelation and autocovariance</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Linear_prediction.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3.7. </span>Linear prediction</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>