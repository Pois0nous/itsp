
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6.1. Subjective quality evaluation &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6.2. Objective quality evaluation" href="Objective_quality_evaluation.html" />
    <link rel="prev" title="6. Evaluation of speech processing methods" href="../Evaluation_of_speech_processing_methods.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Representations/Representations.html">
   3. Basic Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_analysis.html">
     3.1. Short-time analysis of speech and audio signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_processing.html">
     3.2. Short-time processing of speech signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Waveform.html">
     3.3. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Windowing.html">
     3.4. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Signal_energy_loudness_and_decibel.html">
     3.5. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Spectrogram_and_the_STFT.html">
     3.6. Spectrogram and the STFT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Autocorrelation_and_autocovariance.html">
     3.7. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Melcepstrum.html">
     3.8. The cepstrum, mel-cepstrum and mel-frequency cepstral coefficients (MFCCs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Linear_prediction.html">
     3.9. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Fundamental_frequency_F0.html">
     3.10. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Zero-crossing_rate.html">
     3.11. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Deltas_and_Delta-deltas.html">
     3.12. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.13. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Jitter_and_shimmer.html">
     3.14. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Analysis/Measurements_for_medical_applications.html">
     7.2. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
       7.2.1. Inverse filtering for glottal activity estimation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Forensic_analysis.html">
     7.3. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Voice_activity_detection.html">
     8.1. Voice Activity Detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Recognition_and_Verification.html">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Paralinguistic_speech_processing.html">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Concatenative_speech_synthesis.html">
     9.1. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Statistical_parametric_speech_synthesis.html">
     9.2. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Basic_tools.html">
     10.2. Basic tools
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.3. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Entropy_coding.html">
       10.3.4. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.3.5. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Code-excited_linear_prediction_CELP.html">
     10.4. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Frequency-domain_coding.html">
     10.5. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://landbot.io/blog/guide-to-conversational-design/">
   Chatbots / Conversational design (external link)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Security_and_privacy_in_speech_technology.html">
   13. Security and privacy in speech technology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../References.html">
   14. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Evaluation/Subjective_quality_evaluation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#aspects-of-quality">
   6.1.1. Aspects of quality
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sound-and-speech-quality">
     6.1.1.1. Sound and speech quality
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interaction-quality">
     6.1.1.2. Interaction quality
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#choosing-subjects-naive-or-expert">
   6.1.2. Choosing subjects - Naïve or expert?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experimental-design">
   6.1.3. Experimental design
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-use-cases">
   6.1.4. Some use cases
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#frequently-used-standards-and-recommendations-for-quality-evaluation">
   6.1.5. Frequently used standards and recommendations for quality evaluation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expert-listeners">
     6.1.5.1. Expert listeners
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#naive-listeners">
     6.1.5.2. Naïve listeners
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intelligibility-testing">
   6.1.6. Intelligibility testing
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Subjective quality evaluation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#aspects-of-quality">
   6.1.1. Aspects of quality
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sound-and-speech-quality">
     6.1.1.1. Sound and speech quality
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interaction-quality">
     6.1.1.2. Interaction quality
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#choosing-subjects-naive-or-expert">
   6.1.2. Choosing subjects - Naïve or expert?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experimental-design">
   6.1.3. Experimental design
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-use-cases">
   6.1.4. Some use cases
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#frequently-used-standards-and-recommendations-for-quality-evaluation">
   6.1.5. Frequently used standards and recommendations for quality evaluation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expert-listeners">
     6.1.5.1. Expert listeners
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#naive-listeners">
     6.1.5.2. Naïve listeners
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intelligibility-testing">
   6.1.6. Intelligibility testing
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="subjective-quality-evaluation">
<h1><span class="section-number">6.1. </span>Subjective quality evaluation<a class="headerlink" href="#subjective-quality-evaluation" title="Permalink to this headline">¶</a></h1>
<p>In speech processing applications where humans are the end-users, there
humans are also the ultimate measure of performance and quality.
<em>Subjective evaluation</em> refers to evaluation setups where human subjects
measure or quantify performance and quality. From a top-level
perspective the task is simple; subjects are asked to evaluate questions
such as</p>
<ul class="simple">
<li><p>Does X sound good?</p></li>
<li><p>How good does X sound?</p></li>
<li><p>Does X or Y sound better?</p></li>
<li><p>How intelligible is X?</p></li>
</ul>
<p>However like always, the devil is in the details. Subjective evaluation
must be designed carefully such that questions address information which
is <em>useful</em> for measuring performance and such that information
extracted is <em>reliable</em> and <em>accurate</em>.</p>
<p>Most commonly, subjective evaluation in speech and audio refers to
<em>perceptual evaluation</em> of sound samples. In some cases evaluation can
also involve interactive elements, such as participation in a dialogue
over a telecommunication connection. In any case, perceptual evaluation
refers to evaluation through the subjects’ senses, which in this context
refers primarily to <em>hearing</em>. In other words, in a typical experiment
setup, subjects listen to sound samples and evaluate their quality.</p>
<p><img alt="pic1" src="../_images/155472510.jpg" /></p>
<p>Photo by Anthony Brolin on Unsplash</p>
<div class="section" id="aspects-of-quality">
<h2><span class="section-number">6.1.1. </span>Aspects of quality<a class="headerlink" href="#aspects-of-quality" title="Permalink to this headline">¶</a></h2>
<p>Observe that the appropriate evaluation questions are tightly linked to
the application and context. For example, when designing a
teleconferencing system for business applications, we are interested in
entirely different types and aspects of quality than in design of
hearing-aids.</p>
<p>From a top-level perspective, we discuss quality for example, with
respect to</p>
<ul class="simple">
<li><p><em>sound or speech quality</em>, relating to an intrinsic property of the
audio signal,</p></li>
<li><p><em>interaction and communication quality</em>, as the experience of
quality in terms of dynamic interaction,</p></li>
<li><p><em>service quality and user experience</em>, which is usually meant to
include beyond sound and interaction quality, the whole experience
of using a service or device, including responsiveness of system,
network coverage, user interface, visual and tactual design of
devices etc.</p></li>
</ul>
<div class="section" id="sound-and-speech-quality">
<h3><span class="section-number">6.1.1.1. </span>Sound and speech quality<a class="headerlink" href="#sound-and-speech-quality" title="Permalink to this headline">¶</a></h3>
<p>The acoustic quality of the signal can be further described, for
example, through concepts such as</p>
<ul class="simple">
<li><p><em>noisiness</em> or the amount of noise the speech signal is perceived to
have (perceptually uncorrelated noise)</p></li>
<li><p><em>distortion</em> describes how much parts of the speech signal are
destroyed (perceptually correlated noise), though often also
uncorrelated noises are referred to as distortions</p></li>
<li><p><em>intelligibility</em> is the level to which the meaning of the speech
signal can be understood</p></li>
<li><p><em>listening effort</em> refers to the amount of work a listener has to
use in listening to the signal and how much <em>listening fatigue</em>
and <em>annoyance</em> a user experiences</p></li>
<li><p><em>pleasantness</em> describes how much the speech signal annoys the
listener</p></li>
<li><p><em>resemblance</em> describes how close the speech signal is to the
original signal</p></li>
<li><p><em>naturalness</em> describes how natural an artificial speech source
sounds, often used as a last resort, when no other adjective feels
suitable, then we can see that “<em>It sounds aunnatural”</em>.</p></li>
<li><p><em>acoustic distance</em> or how near or far the far-end speaker is
perceived to be, which is closely related to the amount of
reverberation the signal has (and how much delay the communication
path has)</p></li>
</ul>
<p>Note that we can also think of intelligibility and listening effort as
separate aspects of sound quality. That is, we can perceive noisiness
and distortions, but still not be annoyed by the quality at all and be
able to listen to the sound effortlessly. Listening effort is usually a
prerequisite for loss of intelligibility; if we have to listen carefully
than it is exhausting in the long run. Quality, effort and
intelligibility are, in this sense, addressing different quality levels,
where intelligibility is an issue only at very bad quality, effort in
the middle range whereas quality is always relevant (see figure on the
right).</p>
</div>
<div class="section" id="interaction-quality">
<h3><span class="section-number">6.1.1.2. </span>Interaction quality<a class="headerlink" href="#interaction-quality" title="Permalink to this headline">¶</a></h3>
<p>The quality of interaction when using speech technology can be further
described, for example, through concepts such as</p>
<ul class="simple">
<li><p><em>delay</em> describes the delay between the acoustic event at the
speakers end, to the time the event is perceived at the receiving
end. It is further divided into algorithmic delay caused by
processing, as well as delays caused by the transmission path.</p></li>
<li><p><em>echo</em> is the feedback loop of sound in the sound system, where a
sound loudspeaker is picked by a microphone</p></li>
<li><p><em>presence or distance</em> is the feeling of proximity (and intimacy)
that a user experiences in communication</p></li>
<li><p><em>naturalness</em> can also here be used to describe communication with
the obvious meaning.</p></li>
</ul>
<p><img alt="pic2" src="../_images/175522003.png" /></p>
</div>
</div>
<div class="section" id="choosing-subjects-naive-or-expert">
<h2><span class="section-number">6.1.2. </span>Choosing subjects - Naïve or expert?<a class="headerlink" href="#choosing-subjects-naive-or-expert" title="Permalink to this headline">¶</a></h2>
<p>When designing a subjective listening test, one of the first important
choices is whether listeners are naïve or expert listeners. With naïve
we refer to listeners who do not have prior experience in analytic
listening nor do they have other related expertise. Expert listeners are
then, obviously, subjects who are trained in the art of listening and
have the ability to analytically evaluate small differences in sounds
samples. Good expert listeners are typically researchers in the field
who have years of experience in developing speech and audio processing
and listening to sound samples. Some research labs even have their own
training for expert listeners.</p>
<p>Expert listeners are the best listeners in the sense that they can give
<em>accurate</em> and <em>repeatable</em> results. They can hear the smallest audible
differences and they can grade the differences <em>consistently</em>. In other
words, if an expert hears the same sounds in two tests, he or she can
give them exactly the same score both times, and (in an ideal world) his
or her expert colleagues will grade the sounds similarly. However, the
problem is that expert listeners have skills far beyond those of average
user of speech and audio products. If we want average users to enjoy our
products, then we should be measuring the preferences of average users.
It is uncertain whether the preferences of expert listeners align with
average users. For example, expert listeners might notice a highly
annoying distortion which an average user never discovers. The expert
would therefore be unable to enjoy the product which is great for the
average user.</p>
<p>Naïve listeners are therefore the best listeners in the sense that they
reflect best the <em>preferences of the average population</em>. The downside
is that since naïve listeners do not have experience in subjective
evaluation, their answers have usually a high variance (the measurement
is noisy). That is, if they hear the same samples twice, they are often
unable to repeat the same grade (intra-listener variance) and the
difference in grades between listeners is often high (inter-listener
variance). Consequently, to get useful results from naïve listeners, you
need a large number of subjects. To get statistically significant
results, you often need a minimum of <em>10 expert</em> listeners, where you
frequently need more than <em>50 naïve</em> listeners. The difficulty of the
task naturally can have a large impact on the required number of
subjects (difficult tasks require a larger number of listeners for both
expert and naïve listeners).</p>
<p>Note that the above definition leaves a large grey area between naïve
and expert listeners. A naïve listener looses the naïve status when
participating in a listening test. However, a naïve listener needs years
of training to become an expert listener. This is particularly
problematic in research labs, where there are plenty of young
researchers available for listening tests. They are not naïve listeners
any more, but they need training to become expert listeners. A typical
approach is then to include the younger listeners regularly in listening
tests, but remove those listeners in post-screening if their inter- or
intra-listener variance is too large. That way the listeners slowly gain
experience, but their errors do not get too much weight in the results.
This approach however has to be very clearly monitored such that it does
not lead to tampering of results (i.e. scientific misconduct). Any
listeners removed in post-screening and the motivations for
post-screening have to therefore be documented accurately.</p>
</div>
<div class="section" id="experimental-design">
<h2><span class="section-number">6.1.3. </span>Experimental design<a class="headerlink" href="#experimental-design" title="Permalink to this headline">¶</a></h2>
<p>To get accurate results from an experiment, we have to design the
experiment such that it matches the performance qualities we want to
quantify. For example, in an extreme case, an evaluation of the
performance of noise reduction can be hampered, if a sound sample
features a speaker whose voice is annoying to the listeners. The
practical questions are however more nuanced. We need to consider for
example:</p>
<ul class="simple">
<li><p>To which extent does the language of speech samples affects
listening test results? Can naïve listeners grade accurately speech
samples in a foreign language? Does the text-content of speech
samples affect grading: for example, if the spoken text is
politically loaded, would the grades of politically left- and
right-leaning subjects give different scores?</p></li>
<li><p>Does the text material, range of speakers and recording conditions
reflect the target users and environments? For example, if we test a
system with English-speaking listeners with speech samples in
English, do the results reflect performance for Chinese users? Are
all phonemes present in the material and do they appear equally
often as they do in the target languages? Does the material feature
background noises and room acoustics in the same proportion as
real-world scenarios? Does gender of the speaker or listener play a
role? Or their cultural background?</p></li>
<li><p>Is the subject learning from previous sounds, such that the answers
regarding the current sound are different from the previous sound?
That is, if the subject hears the same sound with different
distortions several times, then he already knows how the sound is
supposed to sound like. Then perhaps he evaluates distortions
differently, because he know how the sound is supposed to sound
like.</p></li>
</ul>
<p>To take into account such considerations, experiments can be designed in
different ways, for example:</p>
<ul>
<li><p>We can measure <em>absolute</em> quality (How good is X?), or <em>relative</em>
quality (How good is X in comparison to Y?) or we can <em>rank</em> samples
(“Which one is better, A or B?” or “Order samples A, B and C from
best to worst.”). Clearly absolute quality is often the most
important quality for users, since usually users do not have the
opportunity to test products side by side. However, for example
during development, two version of an algorithm could have very
similar quality, such that it would be difficult to determine
preference with an absolute quality measure. Relative quality
measures then give more detailed information, explicitly quantifying
the difference in performance. Ranking samples is usually used in
competitions, for example, when a company wants to choose a supplier
for a certain product, it is then useful to be able to measure the
ordering of products. It is thus more refined than relative measures
in terms of finding which one is better, but at the same time, it
does not say how large the difference is between particular samples.</p></li>
<li><p>In many applications, the target is to recover a signal after
transmission or from a noisy recording. The objective is thus to
obtain a signal which is as close as possible to the original
signal. It can then be useful to play the original signal to
subjects as a <em>reference</em>, such that they can compare performance
explicitly with the target signal. While this then naturally gives
listeners the opportunity to make more accurate evaluations, it is
also not realistic. In real life, we generally do not have access to
the original; for example, when speaking on the phone, we cannot
directly hear the original signal, but can hear only the transmitted
signal. We would therefore never be able to compare performance to
the target, but only absolute quality.<br />
In some cases it is also possible that some <span class="xref myst">speech
enhancement</span> methods improve quality such that
the output sounds better than the original non-distorted sound!</p></li>
<li><p>In some experimental designs, the subject can listen to sounds many
times, even in a loop. That way we make sure that the subject hears
all the minute details. However, that is unrealistic since in a real
scenario, like a telephone conversation, you usually can hear sounds
only once. Repeated sounds are therefore available only for expert
listeners.</p></li>
<li><p>In choice of samples, the length of samples should be chosen with
care. Longer samples reflect better real-life situations, but bring
many problems, for example:</p>
<ul class="simple">
<li><p>The ability to listeners to remember particular features of the
sound, especially in comparison to other sounds, is very
limited. This would reduce the accuracy of results.</p></li>
<li><p>Longer samples can have multiple different characteristics,
which would warrant a different score. The listener would then
have to perform a judgement; which part of the sentence or
sample is more important, and which type of features are more
important for quality? This can lead to ambiguous situations.</p></li>
</ul>
<p>Listening to very short samples can, in turn, make features of the
sound audible which a listener could not hear in real-life setting.</p>
</li>
<li><p>Usually we prefer to have speech samples in the same language as the
listeners. With expert listeners this constraint might not be so
strict.</p></li>
<li><p>Speech samples should generally be <em>phonetically balanced</em>
“nonsense” sentences. With phonetically balanced, we refer to
sentences where all phonemes appear with the same frequency as they
appear on average in that particular language. With nonsense
sentences, we refer to text content which does not convey any
particular, loaded or surprising meaning. For example, “An apple on
the table” is a good sentence in the sense that it is grammatically
correct and there is nothing strange with it. Examples of bad
sentences would be “Elephants swimming in champagne”, “Corporations
kill babies” and “The dark scent of death and mourning”.</p></li>
<li><p>When playing samples to subjects, they will both <em>learn</em> more about
the samples, but also experience <em>fatigue</em>. Especially for naïve
listeners, the performance of subjects will therefore change during
an experiment. It is very difficult to take such changes in
performance into account in analysis and it is therefore usually
recommended to randomize the ordering of samples separately for each
listener. That way the effects of learning and fatigue will be
dispersed evenly across all samples, such that they have a uniform
effect on all samples.</p></li>
<li><p>To measure the ability of listeners to consistently grade samples,
it is common practice to include items in the test whose answers are
known. For example, we can</p>
<ul class="simple">
<li><p>repeat a sample twice, such that we measure the listeners
ability to give the same grade twice,</p></li>
<li><p>have the original <em>reference</em> signal hidden among test samples
(known as the <em>hidden reference</em>), such that we can measure the
listeners ability to give the perfect score to the perfect
sample,</p></li>
<li><p>include samples with known distortions among the test items,
such that we can compare results with prior experiments which
included the same known samples. Typically such known
distortions include for example low-pass filtered versions of
the original signal. Such samples are known as <em>anchors</em> and low
and high quality anchors are then respectively known as
<em>low-anchor</em> and <em>high-anchor</em>.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="some-use-cases">
<h2><span class="section-number">6.1.4. </span>Some use cases<a class="headerlink" href="#some-use-cases" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><em>During research and development</em> of speech and audio processing
methods, researchers have to evaluate the performance of their
methods. Most typically such evaluations are quite informal; when
you get output from a new algorithm, the first thing to do is to
listen to the output - is it any good? In some stages of
development, such evaluations are an ongoing process; tweak a
parameter and listen how it affects the output. In early
development, listening is in practice also a sanity check;
programming errors often cause bad distortions on the output, which
can be caught by listening.</p></li>
<li><p><em>When publishing results</em> and <em>at later stages of development</em>, it
is usually necessary to evaluate quality in a more formal manner.
The engineer developing an algorithm is not a good listener, because
he has extremely detailed knowledge about the performance and could
often spot his or her own method, from a set of sound samples. The
developer is therefore <em>biased</em> and not a reliable listener.<br />
Therefore, when publishing results we need <em>reproducible</em>
experiments in the sense that if another team would repeat the
listening experiment, then they could draw the same conclusions.
Listening tests therefore have to have a sufficient number of
listeners (expert or naïve) such that the outcome is statistically
significant (see <a class="reference internal" href="Analysis_of_evaluation_results.html"><span class="doc std std-doc">Analysis of evaluation
results</span></a>).</p></li>
<li><p><em>When selecting a product among competing candidates</em> we would like
to make a good evaluation. The demands are naturally very different
depending on the scenario; 1) if you want to choose between Skype,
Google and Signal for your personal VoIP calls during a visit
abroad, you are probably content with informal listening. 2) If on
the other hand, you are an engineer and assigned with the task of
choosing a codec for all VoIP calls within a multi-national company,
then you probably want to do a proper formal listening test.</p></li>
<li><p><em>Monitoring quality of in-production system</em>; The quality of a
running system can abruptly or gradually change due to bugs and
equipment-failures, including memory-errors. To detect such errors,
we need to monitor quality. Often such monitoring is based on
automated objective tests.</p></li>
</ul>
</div>
<div class="section" id="frequently-used-standards-and-recommendations-for-quality-evaluation">
<h2><span class="section-number">6.1.5. </span>Frequently used standards and recommendations for quality evaluation<a class="headerlink" href="#frequently-used-standards-and-recommendations-for-quality-evaluation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="expert-listeners">
<h3><span class="section-number">6.1.5.1. </span>Expert listeners<a class="headerlink" href="#expert-listeners" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>By far the most commonly used standard applied with expert listeners
is known as <a class="reference external" href="https://en.wikipedia.org/wiki/MUSHRA">MUSHRA, or MUltiple Stimuli with Hidden Reference and
Anchor</a>, defined by
<a class="reference external" href="https://en.wikipedia.org/wiki/ITU-R">ITU-R</a> recommendation
<a class="reference external" href="https://www.itu.int/rec/R-REC-BS.1534/en">BS.1534-3</a>. It offers
direct comparison of multiple target samples, with a reference
signal, hidden reference and anchor. Users can switch between
samples on the fly and many interfaces also allow looping short
segments of the signal.<br />
Each sample is rated on an integer scale 1-100.<br />
MUSHRA is very useful for example when publishing results, because
it is simple to implement and well-known. Open source
implementations such as
<a class="reference external" href="https://www.audiolabs-erlangen.de/resources/webMUSHRA">webMUSHRA</a>
are available.<br />
Practical experience have shown that MUSHRA is best applied for
intermediate quality samples, where a comparison of 2-5 samples is
desired. Moreover, a suitable length of sound samples ranges from 2
to approximately 10 seconds. Furthermore, if the overall length of a
MUSHRA test is more than, say, 30 minutes, then the fatigue of
listeners starts be a significant problem. With 10 good listeners it
is usually possible to achieve statistically significant results,
whereas 6 listeners can be sufficient for informal tests (e.g.
during testing). These numbers should not be taken as absolute, but
as practical guidance to give a rough idea of what makes a usable
test.<br />
MUSHRA is however often misused by omitting the anchors; a valid
argument for omitting anchors is that if the distortions in the
target samples are of a very different type then the typical
anchors, then anchors do not provide an added value. Still, such
omissions are not allowed by the MUSHRA standard.</p></li>
<li><p>For very small impairments in audio quality, Recommendation ITU-R
BS.1116-3 (ABC/HR) is recommended instead of MUSHRA (see
<a class="reference external" href="https://www.itu.int/rec/R-REC-BS.1116-3-201502-I/en">https://www.itu.int/rec/R-REC-BS.1116-3-201502-I/en</a>).</p></li>
</ul>
<p>For illustrations and examples of the MUSHRA test, see
<a class="reference external" href="https://www.audiolabs-erlangen.de/resources/webMUSHRA">https://www.audiolabs-erlangen.de/resources/webMUSHRA</a></p>
</div>
<div class="section" id="naive-listeners">
<h3><span class="section-number">6.1.5.2. </span>Naïve listeners<a class="headerlink" href="#naive-listeners" title="Permalink to this headline">¶</a></h3>
<p><strong>P.800</strong> is the popular name of a set of listening tests defined in the standard
<a class="reference external" href="https://www.itu.int/en/ITU-T/Pages/default.aspx">ITU-T</a><a class="reference external" href="https://www.itu.int/rec/T-REC-P.800-199608-I/en">Recommendation P.800 “Methods for subjective determination of transmission quality”</a>. It is intended to be a test which gives as realistic results as possible, by assessing performance in setups which resemble real use-cases.    The most significant consequences are that P.800 focuses on naïve    listeners and, since telecommunication devices are typically    hand-held over one ear, P.800 mandates tests with headphones which    are held only on one ear.      To make the test simpler for naïve listeners, P.800 most typically    uses an integer scale 1-5 known as <a class="reference external" href="https://en.wikipedia.org/wiki/Mean_opinion_score">mean opinion score (MOS)</a>. Each grade    is given a characterisation such as</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Rating</p></th>
<th class="head"><p>Label</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Excellent</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Good</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>Fair</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>Poor</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>Bad</p></td>
</tr>
</tbody>
</table>
<p>This makes the ratings more concrete and easier to understand for users. A downside of labelling the ratings is that such labels are   specific to each language and the MOS scores given in different    languages might thus not be directly comparabale. Who is to know    whether <em>excellent</em>, <em>erinomainen</em>, <em>ممتاز</em>, and <em>маш сайн</em> mean exactly     the same thing? (Those are english, finnish, arabic and mongolian,    in case you were wondering.)</p>
<p>P.800 is further split into</p>
<ul class="simple">
<li><p><em>Conversation opinion tests</em>, where participants grade the        quality after <em>using</em> telecommunication system for a        conversation. Typically the question posed to participants is        “Opinion of the connection you have just been using: Excellent,        God, Fair, Poor, Bad”. An alternative is “Did you or your        partner have any difficulty in talking or hearing over the        connection? Yes/No”.</p></li>
<li><p><em>Listening opinion tests</em>, where participants grade the quality        after <em>listening</em> to the output of a telecommunication system.</p></li>
</ul>
<p>The grading of listening opinion tests can, more specifically, be    one of the following:</p>
<ul class="simple">
<li><p><em>Absolute category rating (ACR)</em>, where the above MOS scale is        used to answer questions like “How good is system X?”</p></li>
<li><p><em>Degradation category rating (DCR)</em>, where the objective is to        evaluate the amount of degradation caused by some processing.        Samples are preseted to listeners by pairs (A-B) or repeated        pairs (A-B-A-B) where A is the quality reference and B the        degraded sample. Rating labels cane be for example</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Rating</p></th>
<th class="head"><p>Label</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Degradation is inaudible</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Degradation is audible but not annoying</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>Degradation is slightly annoying</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>Degradation is annoying</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>Degradation is very annoying</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p><em>Comparison category rating (CCR)</em>, is similar to DCR, but such        that the processed sample B can be also better than A. Rating        labels can then be for example</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Rating</p></th>
<th class="head"><p>Label</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>3</p></td>
<td><p>Much better</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Better</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>Slightly better</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>About the same</p></td>
</tr>
<tr class="row-even"><td><p>-1</p></td>
<td><p>Slightly worse</p></td>
</tr>
<tr class="row-odd"><td><p>-2</p></td>
<td><p>Worse</p></td>
</tr>
<tr class="row-even"><td><p>-3</p></td>
<td><p>Much worse</p></td>
</tr>
</tbody>
</table>
<p><strong>P.804</strong> Subjective diagnostic test method for conversational speech
quality analysis</p>
<p><strong>P.805</strong> Subjective evaluation of conversational quality</p>
<p><strong>P.806</strong> A subjective quality test methodology using multiple rating
scales</p>
<p><strong>P.807</strong> Subjective test methodology for assessing speech
intelligibility</p>
<p><strong>P.808</strong> Subjective evaluation of speech quality with a crowdsourcing
approach</p>
<p><strong>P.835</strong> Subjective test methodology for evaluating speech
communication systems that include noise suppression algorithm</p>
<p>And many more, see <a class="reference external" href="https://www.itu.int/rec/T-REC-P/en">https://www.itu.int/rec/T-REC-P/en</a></p>
<p>Holding a phone on one ear</p>
<p><img alt="pic1" src="../_images/155472516.jpg" />Photo by
Fezbot2000 on Unsplash</p>
</div>
</div>
<div class="section" id="intelligibility-testing">
<h2><span class="section-number">6.1.6. </span>Intelligibility testing<a class="headerlink" href="#intelligibility-testing" title="Permalink to this headline">¶</a></h2>
<p>When a speech signal has a been corrupted by a considerable level of
noise and/or reverberation, it’s intelligibility starts to deteriorate.
We might miss-interpret or -understand words or entirely miss them.
Observe that the level of distortion is quite a bit higher than what we
usually consider in quality-tests.</p>
<p>Typically we have to choose between correctly interpreted words or
phonemes/letters. For example, if the sentence is</p>
<blockquote>
<div><p>How to recognize speech?</p>
</div></blockquote>
<p>and what we hear is</p>
<blockquote>
<div><p>How to wreck a nice beach?</p>
</div></blockquote>
<p>then we can count correct words for example as</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>How to recognize    speech?
How to wreck a nice beach?
       S     I I    S
</pre></div>
</div>
<p>where we use the notation S - substitution, I - insertion, D - deletion.
The <a class="reference external" href="https://en.wikipedia.org/wiki/Word_error_rate">word error rate</a>
would then be</p>
<div class="math notranslate nohighlight">
\[ WER = 100\times\frac{S+D+I}{N} \]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the total number of words. In the above example we thus have
WER = 100%.</p>
<p>If we would go letter by letter, than instead we would have</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>How to  rec..og.nize speech
How to wreck a  nice  beach
       I     SI   S  DS S
</pre></div>
</div>
<p>Here we would then define the letter error rate as</p>
<div class="math notranslate nohighlight">
\[ LER = 100\times\frac{S+D+I}{N} \]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the total number of letters. The value in the above example
would then be 33%.</p>
<p>It is clear that word error rate is thus much more strict than letter
error rate. A single incorrect letter will ruin a word, while the letter
error rate is affected much less. WER is much easier to compute than the
LER and also leads to fewer ambiguous situations. It is however
dependent on the application which measure is better suited.</p>
<p>Observe that both word and error rates are applicable as both objective
measures, in speech recognition experiments, as well as a subjective
measure, where human subjects evaluate the quality of sounds.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Evaluation"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../Evaluation_of_speech_processing_methods.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">6. </span>Evaluation of speech processing methods</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Objective_quality_evaluation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6.2. </span>Objective quality evaluation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>