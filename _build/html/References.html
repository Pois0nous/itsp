
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>14. References &#8212; Introduction to Speech Processing</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="13. Security and privacy in speech technology" href="Security_and_privacy.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Representations/Representations.html">
   3. Basic Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Short-time_analysis.html">
     3.1. Short-time analysis of speech and audio signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Short-time_processing.html">
     3.2. Short-time processing of speech signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Waveform.html">
     3.3. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Windowing.html">
     3.4. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Signal_energy_loudness_and_decibel.html">
     3.5. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Spectrogram_and_the_STFT.html">
     3.6. Spectrogram and the STFT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Autocorrelation_and_autocovariance.html">
     3.7. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Melcepstrum.html">
     3.8. The cepstrum, mel-cepstrum and mel-frequency cepstral coefficients (MFCCs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Linear_prediction.html">
     3.9. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Fundamental_frequency_F0.html">
     3.10. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Zero-crossing_rate.html">
     3.11. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Deltas_and_Delta-deltas.html">
     3.12. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.13. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Jitter_and_shimmer.html">
     3.14. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="Analysis/Measurements_for_medical_applications.html">
     7.2. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
       7.2.1. Inverse filtering for glottal activity estimation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Analysis/Forensic_analysis.html">
     7.3. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Voice_activity_detection.html">
     8.1. Voice Activity Detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Speaker_Recognition_and_Verification.html">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Paralinguistic_speech_processing.html">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Synthesis/Concatenative_speech_synthesis.html">
     9.1. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Synthesis/Statistical_parametric_speech_synthesis.html">
     9.2. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.2. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="Transmission/Entropy_coding.html">
       10.2.5. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.2.6. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Transmission/Code-excited_linear_prediction_CELP.html">
     10.3. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Transmission/Frequency-domain_coding.html">
     10.4. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Security_and_privacy.html">
   13. Security and privacy in speech technology
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   14. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/References.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>References</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="references">
<h1><span class="section-number">14. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<div class="docutils container" id="id1">
<dl class="citation">
<dt class="label" id="id43"><span class="brackets">1</span></dt>
<dd><p>Peter Noll. A comparative study of various quantization schemes for speech encoding. <em>Bell System Technical Journal</em>, 54(9):1597–1614, 1975. URL: <a class="reference external" href="https://doi.org/10.1002/j.1538-7305.1975.tb02053.x">https://doi.org/10.1002/j.1538-7305.1975.tb02053.x</a>.</p>
</dd>
<dt class="label" id="id42"><span class="brackets">2</span></dt>
<dd><p>Tom Bäckström, Jérémie Lecomte, Guillaume Fuchs, Sascha Disch, and Christian Uhle. <em>Speech coding: with code-excited linear prediction</em>. Springer, 2017. URL: <a class="reference external" href="https://doi.org/10.1007/978-3-319-50204-5">https://doi.org/10.1007/978-3-319-50204-5</a>.</p>
</dd>
<dt class="label" id="id44"><span class="brackets">3</span></dt>
<dd><p>Yundong Zhang, Naveen Suda, Liangzhen Lai, and Vikas Chandra. Hello edge: keyword spotting on microcontrollers. <em>arXiv preprint arXiv:1711.07128</em>, 2017. URL: <a class="reference external" href="https://doi.org/10.48550/arXiv.1711.07128">https://doi.org/10.48550/arXiv.1711.07128</a>.</p>
</dd>
<dt class="label" id="id45"><span class="brackets">4</span></dt>
<dd><p>Xiaohui Zhang. <em>Strategies for Handling Out-of-Vocabulary Words in Automatic Speech Recognition</em>. PhD thesis, Johns Hopkins University, 2019. URL: <a class="reference external" href="http://jhir.library.jhu.edu/handle/1774.2/62275">http://jhir.library.jhu.edu/handle/1774.2/62275</a>.</p>
</dd>
<dt class="label" id="id8"><span class="brackets">5</span></dt>
<dd><p>Björn Schuller and Anton Batliner. <em>Computational paralinguistics: emotion, affect and personality in speech and language processing</em>. John Wiley &amp; Sons, 2013. URL: <a class="reference external" href="https://www.wiley.com/en-us/9781118706626">https://www.wiley.com/en-us/9781118706626</a>.</p>
</dd>
<dt class="label" id="id9"><span class="brackets">6</span></dt>
<dd><p>Jouni Pohjalainen, Okko Räsänen, and Serdar Kadioglu. Feature selection methods and their combinations in high-dimensional classification of speaker likability, intelligibility and personality traits. <em>Computer Speech &amp; Language</em>, 29(1):145–171, 2015. URL: <a class="reference external" href="https://doi.org/10.1016/j.csl.2013.11.004">https://doi.org/10.1016/j.csl.2013.11.004</a>.</p>
</dd>
<dt class="label" id="id10"><span class="brackets">7</span></dt>
<dd><p>Florian Eyben, Martin Wöllmer, and Björn Schuller. Opensmile: the munich versatile and fast open-source audio feature extractor. In <em>Proceedings of the 18th ACM international conference on Multimedia</em>, 1459–1462. 2010. URL: <a class="reference external" href="https://doi.org/10.1145/1873951.1874246">https://doi.org/10.1145/1873951.1874246</a>.</p>
</dd>
<dt class="label" id="id7"><span class="brackets">8</span></dt>
<dd><p>Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: a framework for self-supervised learning of speech representations. <em>Advances in Neural Information Processing Systems</em>, 33:12449–12460, 2020. URL: <a class="reference external" href="https://doi.org/10.48550/arXiv.2006.11477">https://doi.org/10.48550/arXiv.2006.11477</a>.</p>
</dd>
<dt class="label" id="id6"><span class="brackets">9</span></dt>
<dd><p>Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James Glass. An unsupervised autoregressive model for speech representation learning. <em>arXiv preprint arXiv:1904.03240</em>, 2019. URL: <a class="reference external" href="https://doi.org/10.48550/arXiv.1904.03240">https://doi.org/10.48550/arXiv.1904.03240</a>.</p>
</dd>
<dt class="label" id="id5"><span class="brackets">10</span></dt>
<dd><p>Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal margin classifiers. In <em>Proceedings of the fifth annual workshop on Computational learning theory</em>, 144–152. 1992. URL: <a class="reference external" href="https://doi.org/10.1145/130385.130401">https://doi.org/10.1145/130385.130401</a>.</p>
</dd>
<dt class="label" id="id3"><span class="brackets">11</span></dt>
<dd><p>Zhizheng Wu, Cassia Valentini-Botinhao, Oliver Watts, and Simon King. Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis. In <em>2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>, 4460–4464. IEEE, 2015. URL: <a class="reference external" href="https://doi.org/10.1109/ICASSP.2015.7178814">https://doi.org/10.1109/ICASSP.2015.7178814</a>.</p>
</dd>
<dt class="label" id="id47"><span class="brackets">12</span></dt>
<dd><p>Lawrence R Rabiner and Ronald W Schafer. Introduction to digital speech processing. <em>Foundations and Trends in Signal Processing</em>, 1(1):1–194, 2007. URL: <a class="reference external" href="https://doi.org/10.1561/2000000001">https://doi.org/10.1561/2000000001</a>.</p>
</dd>
<dt class="label" id="id46"><span class="brackets">13</span></dt>
<dd><p>Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In <em>1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings</em>, volume 1, 373–376. IEEE, 1996. URL: <a class="reference external" href="https://doi.org/10.1109/ICASSP.1996.541110">https://doi.org/10.1109/ICASSP.1996.541110</a>.</p>
</dd>
<dt class="label" id="id38"><span class="brackets">14</span></dt>
<dd><p>Jacob Benesty, M Mohan Sondhi, Yiteng Huang, and others. <em>Springer handbook of speech processing</em>. Volume 1. Springer, 2008. URL: <a class="reference external" href="https://doi.org/10.1007/978-3-540-49127-9">https://doi.org/10.1007/978-3-540-49127-9</a>.</p>
</dd>
<dt class="label" id="id37"><span class="brackets">15</span></dt>
<dd><p>Rainer Martin. Noise power spectral density estimation based on optimal smoothing and minimum statistics. <em>IEEE Transactions on speech and audio processing</em>, 9(5):504–512, 2001. URL: <a class="reference external" href="https://doi.org/10.1109/89.928915">https://doi.org/10.1109/89.928915</a>.</p>
</dd>
<dt class="label" id="id39"><span class="brackets">16</span></dt>
<dd><p>Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. <em>IEEE Transactions on acoustics, speech, and signal processing</em>, 27(2):113–120, 1979. URL: <a class="reference external" href="https://doi.org/10.1109/TASSP.1979.1163209">https://doi.org/10.1109/TASSP.1979.1163209</a>.</p>
</dd>
<dt class="label" id="id33"><span class="brackets">17</span></dt>
<dd><p>William Havard, Laurent Besacier, and Olivier Rosec. Speech-coco: 600k visually grounded spoken captions aligned to mscoco data set. <em>arXiv preprint arXiv:1707.08435</em>, 2017. URL: <a class="reference external" href="https://doi.org/10.21437/GLU.2017-9">https://doi.org/10.21437/GLU.2017-9</a>.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">18</span></dt>
<dd><p>James L McClelland and Jeffrey L Elman. The trace model of speech perception. <em>Cognitive psychology</em>, 18(1):1–86, 1986. URL: <a class="reference external" href="https://doi.org/10.1016/0010-0285(86)90015-0">https://doi.org/10.1016/0010-0285(86)90015-0</a>.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">19</span></dt>
<dd><p>Okko Räsänen. Computational modeling of phonetic and lexical learning in early language acquisition: existing models and future directions. <em>Speech Communication</em>, 54(9):975–997, 2012. URL: <a class="reference external" href="https://doi.org/10.1016/j.specom.2012.05.001">https://doi.org/10.1016/j.specom.2012.05.001</a>.</p>
</dd>
<dt class="label" id="id34"><span class="brackets">20</span></dt>
<dd><p>Emmanuel Dupoux. Cognitive science in the era of artificial intelligence: a roadmap for reverse-engineering the infant language-learner. <em>Cognition</em>, 173:43–59, 2018. URL: <a class="reference external" href="https://doi.org/10.1016/j.cognition.2017.11.008">https://doi.org/10.1016/j.cognition.2017.11.008</a>.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">21</span></dt>
<dd><p>Luc Steels. The synthetic modeling of language origins. <em>Evolution of communication</em>, 1(1):1–34, 1997. URL: <a class="reference external" href="https://doi.org/10.1075/eoc.1.1.02ste">https://doi.org/10.1075/eoc.1.1.02ste</a>.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">22</span></dt>
<dd><p>Simon Kirby. Natural language from artificial life. <em>Artificial life</em>, 8(2):185–215, 2002. URL: <a class="reference external" href="https://doi.org/10.1162/106454602320184248">https://doi.org/10.1162/106454602320184248</a>.</p>
</dd>
<dt class="label" id="id28"><span class="brackets">23</span></dt>
<dd><p>David Marr. <em>Vision: A computational investigation into the human representation and processing of visual information</em>. W.H. Freeman and Company, 1982.</p>
</dd>
<dt class="label" id="id12"><span class="brackets">24</span></dt>
<dd><p>Andrea Weber and Odette Scharenborg. Models of spoken-word recognition. <em>Wiley Interdisciplinary Reviews: Cognitive Science</em>, 3(3):387–401, 2012. <a class="reference external" href="https://doi.org/10.1002/wcs.1178">doi:10.1002/wcs.1178</a>.</p>
</dd>
<dt class="label" id="id27"><span class="brackets">25</span></dt>
<dd><p>James S Magnuson, Heejo You, Sahil Luthra, Monica Li, Hosung Nam, Monty Escabi, Kevin Brown, Paul D Allopenna, Rachel M Theodore, Nicholas Monto, and others. Earshot: a minimal neural network model of incremental human speech recognition. <em>Cognitive science</em>, 44(4):e12823, 2020. URL: <a class="reference external" href="https://doi.org/10.1111/cogs.12823">https://doi.org/10.1111/cogs.12823</a>.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">26</span></dt>
<dd><p>Janet F Werker and Richard C Tees. Cross-language speech perception: evidence for perceptual reorganization during the first year of life. <em>Infant behavior and development</em>, 7(1):49–63, 1984. URL: <a class="reference external" href="https://doi.org/10.1016/S0163-6383(84)80022-3">https://doi.org/10.1016/S0163-6383(84)80022-3</a>.</p>
</dd>
<dt class="label" id="id16"><span class="brackets">27</span></dt>
<dd><p>Jenny R Saffran, Richard N Aslin, and Elissa L Newport. Statistical learning by 8-month-old infants. <em>Science</em>, 274(5294):1926–1928, 1996. URL: <a class="reference external" href="https://doi.org/10.1126/science.274.5294.1926">https://doi.org/10.1126/science.274.5294.1926</a>.</p>
</dd>
<dt class="label" id="id24"><span class="brackets">28</span></dt>
<dd><p>Jessica Maye, Janet F Werker, and LouAnn Gerken. Infant sensitivity to distributional information can affect phonetic discrimination. <em>Cognition</em>, 82(3):B101–B111, 2002. URL: <a class="reference external" href="https://doi.org/10.1016/S0010-0277(01)00157-3">https://doi.org/10.1016/S0010-0277(01)00157-3</a>.</p>
</dd>
<dt class="label" id="id15"><span class="brackets">29</span></dt>
<dd><p>Jenny R Saffran and Natasha Z Kirkham. Infant statistical learning. <em>Annual review of psychology</em>, 69:181–203, 2018. URL: <a class="reference external" href="https://doi.org/10.1146/annurev-psych-122216-011805">https://doi.org/10.1146/annurev-psych-122216-011805</a>.</p>
</dd>
<dt class="label" id="id23"><span class="brackets">30</span></dt>
<dd><p>Tasha Nagamine, Michael L Seltzer, and Nima Mesgarani. Exploring how deep neural networks form phonemic categories. In <em>Sixteenth Annual Conference of the International Speech Communication Association</em>. 2015. URL: <a class="reference external" href="https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_1912.pdf">https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_1912.pdf</a>.</p>
</dd>
<dt class="label" id="id19"><span class="brackets">31</span></dt>
<dd><p>Okko Räsänen and Heikki Rasilo. A joint model of word segmentation and meaning acquisition through cross-situational learning. <em>Psychological review</em>, 122(4):792, 2015. URL: <a class="reference external" href="https://psycnet.apa.org/doi/10.1037/a0039702">https://psycnet.apa.org/doi/10.1037/a0039702</a>.</p>
</dd>
<dt class="label" id="id31"><span class="brackets">32</span></dt>
<dd><p>Sofoklis Kakouros and Okko Räsänen. 3pro–an unsupervised method for the automatic detection of sentence prominence in speech. <em>Speech Communication</em>, 82:67–84, 2016. URL: <a class="reference external" href="https://doi.org/10.1016/j.specom.2016.06.004">https://doi.org/10.1016/j.specom.2016.06.004</a>.</p>
</dd>
<dt class="label" id="id30"><span class="brackets">33</span></dt>
<dd><p>Herman Kamper, Aren Jansen, and Sharon Goldwater. A segmental framework for fully-unsupervised large-vocabulary speech recognition. <em>Computer Speech &amp; Language</em>, 46:154–174, 2017. URL: <a class="reference external" href="https://doi.org/10.1016/j.csl.2017.04.008">https://doi.org/10.1016/j.csl.2017.04.008</a>.</p>
</dd>
<dt class="label" id="id17"><span class="brackets">34</span></dt>
<dd><p>Okko Räsänen, Gabriel Doyle, and Michael C Frank. Pre-linguistic segmentation of speech into syllable-like units. <em>Cognition</em>, 171:130–150, 2018. URL: <a class="reference external" href="https://doi.org/10.1016/j.cognition.2017.11.003">https://doi.org/10.1016/j.cognition.2017.11.003</a>.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">35</span></dt>
<dd><p>Dennis Norris. Shortlist: a connectionist model of continuous speech recognition. <em>Cognition</em>, 52(3):189–234, 1994. URL: <a class="reference external" href="https://doi.org/10.1016/0010-0277(94)90043-4">https://doi.org/10.1016/0010-0277(94)90043-4</a>.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">36</span></dt>
<dd><p>Shinji Maeda. Improved articulatory models. <em>The Journal of the Acoustical Society of America</em>, 84(S1):S146–S146, 1988. URL: <a class="reference external" href="https://doi.org/10.1121/1.2025845">https://doi.org/10.1121/1.2025845</a>.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">37</span></dt>
<dd><p>Peter Birkholz. <em>3D-Artikulatorische Sprachsynthese</em>. PhD thesis, der Universität Rostock, 2005. URL: <a class="reference external" href="https://www.vocaltractlab.de/publications/birkholz-2005-dissertation.pdf">https://www.vocaltractlab.de/publications/birkholz-2005-dissertation.pdf</a>.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">38</span></dt>
<dd><p>Peter Birkholz, Lucia Martin, Klaus Willmes, Bernd J Kröger, and Christiane Neuschaefer-Rube. The contribution of phonation type to the perception of vocal emotions in german: an articulatory synthesis study. <em>The Journal of the Acoustical Society of America</em>, 137(3):1503–1512, 2015. URL: <a class="reference external" href="https://doi.org/10.1121/1.4906836">https://doi.org/10.1121/1.4906836</a>.</p>
</dd>
<dt class="label" id="id13"><span class="brackets">39</span></dt>
<dd><p>Jason A Tourville and Frank H Guenther. The diva model: a neural theory of speech acquisition and production. <em>Language and cognitive processes</em>, 26(7):952–981, 2011. URL: <a class="reference external" href="https://doi.org/10.1080/01690960903498424">https://doi.org/10.1080/01690960903498424</a>.</p>
</dd>
<dt class="label" id="id32"><span class="brackets">40</span></dt>
<dd><p>Ian S Howard and Piers Messum. Learning to pronounce first words in three languages: an investigation of caregiver and infant behavior using a computational model of an infant. <em>PLoS One</em>, 9(10):e110334, 2014. URL: <a class="reference external" href="https://doi.org/10.1371/journal.pone.0110334">https://doi.org/10.1371/journal.pone.0110334</a>.</p>
</dd>
<dt class="label" id="id18"><span class="brackets">41</span></dt>
<dd><p>Heikki Rasilo and Okko Räsänen. An online model for vowel imitation learning. <em>Speech Communication</em>, 86:1–23, 2017. URL: <a class="reference external" href="https://doi.org/10.1016/j.specom.2016.10.010">https://doi.org/10.1016/j.specom.2016.10.010</a>.</p>
</dd>
<dt class="label" id="id21"><span class="brackets">42</span></dt>
<dd><p>Pierre-Yves Oudeyer, George Kachergis, and William Schueller. Computational and robotic models of early language development: a review. In J.S. Horst and J. von Koss Torkildsen, editors, <em>International handbook of language acquisition</em>. Routledge/Taylor &amp; Francis Group, 2019. URL: <a class="reference external" href="https://psycnet.apa.org/doi/10.4324/9781315110622-5">https://psycnet.apa.org/doi/10.4324/9781315110622-5</a>.</p>
</dd>
<dt class="label" id="id40"><span class="brackets">43</span></dt>
<dd><p>Xabier Lareo. Smart speakers and virtual assistants. <em>TechDispatch #1:</em>, 2019. URL: <a class="reference external" href="https://data.europa.eu/doi/10.2804/004275">https://data.europa.eu/doi/10.2804/004275</a>.</p>
</dd>
<dt class="label" id="id48"><span class="brackets">44</span></dt>
<dd><p>Andreas Nautsch, Catherine Jasserand, Els Kindt, Massimiliano Todisco, Isabel Trancoso, and Nicholas Evans. The gdpr &amp; speech data: reflections of legal and technology communities, first steps towards a common understanding. <em>arXiv preprint arXiv:1907.03458</em>, 2019. URL: <a class="reference external" href="https://doi.org/10.21437/Interspeech.2019-2647">https://doi.org/10.21437/Interspeech.2019-2647</a>.</p>
</dd>
<dt class="label" id="id41"><span class="brackets">45</span></dt>
<dd><p>Jessica Gasiorek. <em>Message processing: The science of creating understanding</em>. UH Mānoa Outreach College, 2018. URL: <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">http://pressbooks-dev.oer.hawaii.edu/messageprocessing/</a>.</p>
</dd>
<dt class="label" id="id2"><span class="brackets">46</span></dt>
<dd><p>Rachel L Finn, David Wright, and Michael Friedewald. Seven types of privacy. In <em>European data protection: coming of age</em>, pages 3–32. Springer, 2013. URL: <a class="reference external" href="https://doi.org/10.1007/978-94-007-5170-5_1">https://doi.org/10.1007/978-94-007-5170-5_1</a>.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Security_and_privacy.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">13. </span>Security and privacy in speech technology</p>
        </div>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>