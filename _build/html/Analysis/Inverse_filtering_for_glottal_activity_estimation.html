
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Inverse filtering for glottal activity estimation &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Analysis/Inverse_filtering_for_glottal_activity_estimation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#signal-model">
   Signal Model
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Inverse filtering for glottal activity estimation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#signal-model">
   Signal Model
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="inverse-filtering-for-glottal-activity-estimation">
<h1>Inverse filtering for glottal activity estimation<a class="headerlink" href="#inverse-filtering-for-glottal-activity-estimation" title="Permalink to this headline">¶</a></h1>
<div class="contentLayout2">
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<p>The main source of speech sounds, where the acoustic signal is first
created, are the vocal folds, which oscillate in the airflow generated
by the lungs (for details, see <span class="xref myst">Speech production and acoustic
properties</span>). The opening
between the vocal folds is known as the <em>glottis</em> and the movements of
vocal folds as well and the corresponding airflow are jointly known as
glottal activity.</p>
<p>Since glottal activity is thus central to speech, it is also important
to understand how it works. What makes a “good” voice? If there is a
disruption to the vocal folds, how does that affect the voice? These are
often medical questions, but they have a large social and societal
impact. If you loose your voice, you can easily become isolated since
you loose an important mode of communication. If you are in a voice
profession such as teaching, sales, singing or acting, also your ability
to work relies on your voice such that any disturbance in the voice
impedes your ability to work. Studying the glottis is thus paramount.</p>
<p>The vocal folds are located in the neck, covered and surrounded by a
cartilage. Accessing them is therefore difficult. Putting a camera in
the throat is uncomfortable to say the least and even when it is
possible, it impedes normal speech, giving measurements a bias of
unknown size. Moreover, since the vocal folds oscillate with a
fundamental frequency that can go up to 400 Hz, we need a camera whose
frame rate is at least 4000 Hz to get 10 frames per period. In other
words, we would need a high-speed camera. While such cameras are today
readily available, they need a lot of light, which generates a lot of
heat, which is not compatible within the sensitive tissues inside the
throat. Imaging with other methods, X-rays or magnetic resonance
imaging, generally have a slower frame rate and some imaging like X-rays
also generate harmful radiation (esp. at high frame rates).</p>
<p>The cartilage surrounding the vocal folds also prevents ultrasound
measurements. The only usable direct measurement is <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">electroglottography
(EGG)</a>, which measures
the impedance through the neck using electrodes. It measures
conductivity, which is highly dependent on the contact area of the vocal
folds, thus giving information about the position of the vocal folds.
However, this information is usually one-dimensional which limits the
usability of such measurements. It also sensitive to and requires
careful placement of electrodes.</p>
<p>What remains is the acoustic signal. With a microphone, we can record
the sound emitted from the mouth, and try to deduce the movements of the
vocal folds from the sound. It is minimally invasive, because we do not
need to insert any sensors inside or onto the the body. Airflow through
the glottis is closely related to the movements of the vocal folds; when
the vocal folds are open air can flow and when they are closed, airflow
is stopped. Airflow in turn is related to the acoustic signal; sound is
a variation in air pressure such that the gradient of airflow
approximately translates to the corresponding sound.</p>
<p>When sound is generated in the vocal folds, it is however acoustically
shaped by the vocal tract (for details, again, see <span class="xref myst">Speech production
and acoustic properties</span>);
some frequencies are emphasised and others attenuated. To analyse
glottal activity, we therefore need to cancel the acoustic effect of the
vocal tract. Recall that the effect of the vocal tract can be
efficiently modelled by a linear predictive filter. We can thus estimate
a filter corresponding to the effect of the vocal tract, and then invert
the effect of that filter. This process is known as <em>inverse filtering</em>.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
<div class="section" id="signal-model">
<h2>Signal Model<a class="headerlink" href="#signal-model" title="Permalink to this headline">¶</a></h2>
<p>In glottal inverse filtering, we follow the source-filter paradigm for
speech source modelling, where the acoustic speech signal is modelled as
an excitation signal filtered by the vocal tract. The assumption is that
these two components are independent. By assuming that we know the
effect of the vocal tract, we can therefore remove its effect by
inverting its effect. If we further model the vocal tract as a
tube-model, its effect corresponds to IIR filtering, such that the
inverse process is FIR filtering.</p>
<p>The main difficulty in of glottal inverse filtering is estimation of the
filter corresponding to the effect of the vocal tract. The task
resembles classical <span class="xref myst">linear predictive</span> modelling,
where the parameters of an IIR filter are uniquely estimated from the
autocorrelation of the signal. Covertly, this however assumes that the
excitation is uncorrelated white noise. However, in voiced speech, the
excitation is the glottal excitation, which resembles a half-wave
rectified sinusoid. That is, it is a fairly smooth curve with a
characteristic comb-structure in the spectrum, as well as a distinct
tilt with more energy at low frequencies. Though we know a lot about the
glottal excitation, it is hard to model. If we would have the glottal
excitation, then the vocal tract would be easy to estimate and vice
versa - a typical chicken-and-egg problem.</p>
<p>One of the most popular methods for glottal inverse filtering is
iterative adaptive inverse filtering (IAIF), where both the glottal
excitation and the vocal tract are modelled with linear predictive
filtering, and both filters are estimated in an alternating iteration.
Many improvements based on more advanced signal models as well as
machine learning have been proposed, but the question is far from
solved. A central problem in evaluation such methods is the ground
truth. We can estimate curves which look like glottal excitations, but
we would need to know the actual movements of the vocal folds to verify
the accuracy of the obtained curves. Since we do not have a satisfactory
direct method for observing glottal activity, which would not bias
results, we cannot verify our models.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Analysis"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>