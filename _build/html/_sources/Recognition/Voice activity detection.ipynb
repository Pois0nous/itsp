{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Activity Detection (VAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "![vadspp](code/vad_spp-1.png)\n",
    "\n",
    "-   *Voice activity detection* (VAD) (or speech activity detection, or\n",
    "    speech detection) refers to a class of methods which detect whether\n",
    "    a sound signal contains speech or not.\n",
    "-   A closely related and partly overlapping task is *speech presence\n",
    "    probability* (SPP) estimation.\n",
    "    -   Instead of a binary present/not-present decision, SPP gives a\n",
    "        probability level that the signal contains speech.\n",
    "    -   A VAD can be derived from SPP by setting a threshold probability\n",
    "        above which the signal is considered to contain speech.\n",
    "    -   In most cases, SPP is thus the more fundamental problem.\n",
    "-   Voice activity detection is used as a pre-processing algorithm for\n",
    "    almost all other speech processing methods.\n",
    "    -   In *speech coding*, it is used to to determine when speech\n",
    "        transmission can be switched off to reduce the amount of\n",
    "        transmitted data.\n",
    "    -   In *speech recognition*, it is used to find out what parts of\n",
    "        the signal should be fed to the recognition engine. Since\n",
    "        recognition is a computationally complex operation, ignoring\n",
    "        non-speech parts saves CPU power.\n",
    "-   VAD or SPP is thus used mostly as a resource-saving operation.\n",
    "-   In *speech enhancement*, where we want to reduce or remove noise in\n",
    "    a speech signal, we can estimate noise characteristics from\n",
    "    non-speech parts (learn/adapt) and remove noise from the speech\n",
    "    parts (apply).\n",
    "-   A closely related method in *audio* applications is *noise gateing*,\n",
    "    where typically a microphone signal is muted whenever there is no\n",
    "    signal present.\n",
    "    -   For example, when a singer is not singing in the microphone,\n",
    "        then the microphone is off. When the singer is not singing,\n",
    "        microphone signal is only noise and therefore the noise gate\n",
    "        removes (gates) noise.\n",
    "-   VADs can thus also be used in improving signal quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "#import sounddevice as sd\n",
    "import IPython.display as ipd\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "#from scipy import signal\n",
    "#from scipy import linalg\n",
    "\n",
    "\n",
    "fs = 48000\n",
    "\n",
    "# Define windowing\n",
    "window_length_ms = 30\n",
    "window_step_ms = window_length_ms/2\n",
    "window_step = int(np.round(fs*window_step_ms/1000))\n",
    "window_length = window_step*2\n",
    "spectrum_length = window_length//2 + 1\n",
    "\n",
    "# Network parameters\n",
    "past_frames_lookback = 10\n",
    "hidden_layer_size = spectrum_length*2\n",
    "bottleneck_size = 30\n",
    "\n",
    "\n",
    "def stft(data,window_length=int(1440),window_step=int(1440/2)):\n",
    "    windowing_fn = np.sin(np.pi*np.linspace(0.5,window_length-0.5,num=window_length)/window_length) # half-sine window\n",
    "    window_count = int(np.floor((data.shape[0]-window_length)/window_step)+1)\n",
    "    spectrogram_matrix = np.zeros([window_length,window_count],dtype=complex)\n",
    "    for window_ix in range(window_count):    \n",
    "        data_frame = np.multiply(windowing_fn,data[window_ix*window_step+np.arange(window_length)])\n",
    "        spectrogram_matrix[:,window_ix] = np.fft.fft(data_frame)\n",
    "    spectrum_length = window_length // 2 + 1 \n",
    "    return spectrogram_matrix[range(spectrum_length),:]\n",
    "\n",
    "def istft(spectrogram):\n",
    "    window_count = spectrogram.shape[1]\n",
    "    spectrum_length = spectrogram.shape[0]\n",
    "    window_step = spectrum_length-1\n",
    "    window_length = window_step*2\n",
    "    data_length = (window_count-1)*window_step + window_length\n",
    "    windowing_fn = np.sin(np.pi*np.linspace(0.5,window_length-0.5,num=window_length)/window_length) # half-sine window\n",
    "    data = np.zeros([data_length])\n",
    "    backward_index = range(spectrum_length-2,0,-1)\n",
    "    for window_ix in range(window_count):\n",
    "        full_spectrum = np.concatenate((spectrogram[:,window_ix],np.conjugate(spectrogram[backward_index,window_ix])))\n",
    "        data_window = np.fft.ifft(full_spectrum) * windowing_fn\n",
    "        data[window_ix*window_step + np.arange(window_length)] += np.real(data_window)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-noise VAD = Trivial case\n",
    "![vadsad](code/vad_sad-1.png)\n",
    "\n",
    "-   To introduce basic vocabulary and methodology, let us consider a\n",
    "    case where a speaker is speaking in an (otherwise) silent\n",
    "    environment. \n",
    "    -   When there is no speech, there is silence.\n",
    "    -   (Any) Signal activity indicates voice activity.\n",
    "-   Signal activity can be measured by, for example, estimating signal\n",
    "    energy per frame $\\Rightarrow$ the *energy thresholding algorithm*.\n",
    "\n",
    "![vadene](code/vad_ene-1.png)\n",
    "\n",
    "-   Clearly energy thresholding works for silent speech signals.\n",
    "    -   Low-energy frames are correctly labeled as non-speech and speech\n",
    "        parts are likewise correctly labeled.\n",
    "-   It is however not trivial to choose an appropriate threshold-level.\n",
    "    -   A low threshold level would make sure that all speech-frames are\n",
    "        correctly labeled. However, we might then also label frames with\n",
    "        other sounds, like breathing sounds or other background noises,\n",
    "        as speech frames.\n",
    "    -   A high threshold would make sure that all detected speech-frames\n",
    "        actually are truly speech frames. But then we could miss offsets\n",
    "        (sounds which are trailing off), since they often have a low\n",
    "        energy.\n",
    "-   What strategy should we use to choose a threshold?\n",
    "    -   What is the correct label for something like breathing-noises?\n",
    "    -   How do we actually measure performance of a VAD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAD objective and performance measurement\n",
    "\n",
    "-   The objective of a VAD implementation depends heavily on the\n",
    "    application.\n",
    "    -   In speech coding, our actual objective is to reduce bitrate\n",
    "        without decreasing quality. $\\Rightarrow$ We want to make sure\n",
    "        that no speech frames are classified as background noise,\n",
    "        because that would reduce quality. \n",
    "        \n",
    "        $\\Rightarrow$ We make a conservative estimate.\n",
    "        \n",
    "    -   In keyword spotting (think \"Siri\" or \"OK Google\"), we want to\n",
    "        detect the start of a particular combination of words. The VADs\n",
    "        task is to avoid running a computationally expensive keyword\n",
    "        spotting algorithm all the time. Missing one keyword is not so\n",
    "        bad (the user would then just try again), but if it is too\n",
    "        sensitive then the application would drain the battery.\n",
    "        \n",
    "        $\\Rightarrow$ We want to be sure that only keywords are spotted.\n",
    "\n",
    "-   The objective of a VAD implementation depends heavily on the\n",
    "    application.\n",
    "    -   In speech enhancement, we want to find non-speech areas such\n",
    "        that we can there estimate noise characteristics, such that we\n",
    "        can remove anything which looks like noise. We want to be sure\n",
    "        that there is no speech in the noise estimate, otherwise we\n",
    "        would end up removing some speech and not only noise.\n",
    "    -   In speech recognition, VAD is used purely for resource saving.\n",
    "        We do not want to reduce accuracy of the recognition, but want\n",
    "        to minimize CPU usage.\n",
    "-   We need a set of performance measures which reflect these different\n",
    "    objectives.\n",
    "-   The performance is then often described by looking at how often are\n",
    "    frames which do contain speech labeled as speech/non-speech, and how\n",
    "    often is non-speech labeled as speech/non-speech?\n",
    "\n",
    "| Input        | Speech           | Non-speech |\n",
    "| --- | --- | --- |\n",
    "| Speech       | True positive    | False negative |\n",
    "| Non-speech   | False positive   | True negative |\n",
    "\n",
    "-   For speech coding, we want to keep the number of false negatives low, and false positives are of only secondary importance.\n",
    "-   For keyword spotting, we want to keep the number of false positives low, and false negatives are secondary importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance in noise -- -3dB / -4dB threshold\n",
    "![vadene3](code/vad_ene_noise3-1.png)\n",
    "![vadene4](code/vad_ene_noise4-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "\n",
    "![vadhyst](code/vad_ene_hyst-1.png)\n",
    "\n",
    "-   We already saw that speech coding wants to avoid false negatives\n",
    "    (=speech frames labeled as non-speech).\n",
    "-   Can we identify typical situations where false negatives occur?\n",
    "    -   Offsets (where a phonation ends) often have low energy\\\n",
    "        $\\Rightarrow$ Easily misclassified as non-speech.\n",
    "    -   Stops have a silence in the middle of an utterance.\\\n",
    "        $\\Rightarrow$ Easily misclassified as non-speech.\n",
    "-   We should be careful at the end of phonations.\n",
    "    -   We can use a *hangover* time, such that after a speech segment\n",
    "        we keep the label as speech for a while until we are sure that\n",
    "        speech has ended.\n",
    "    -   For onsets (starts of phonemes) we usually want to be very\n",
    "        sensitive.\n",
    "-   We obtain a hysteresis rule;\n",
    "    -   If any of the last $K$ frames was identified as speech, then the\n",
    "        current frame is labelled as speech. Otherwise non-speech.\n",
    "\n",
    "\n",
    "## VAD for noisy speech\n",
    "\n",
    "-   Clean speech (absolutely no background noise) is very rare if not\n",
    "    impossible to achieve.\n",
    "-   Real-life speech recordings practically always have varying amounts\n",
    "    of background noise.\n",
    "    -   Performance of energy thresholding decreases rapidly when the\n",
    "        SNR drops.\n",
    "    -   For example, weak offsets easily disappear in noise.\n",
    "-   We need more advanced VAD methods for noisy speech.\n",
    "    -   We need to identify characteristics which differentiate between\n",
    "        speech and noise.\n",
    "    -   Measures for such characteristics are known as *features*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "![vadfeat](code/vad_features1-1.png)\n",
    "\n",
    "-   In VAD, with features we try to measure some property of the signal\n",
    "    which would give an indication to whether the signal is speech or\n",
    "    non-speech.\n",
    "    -   Signal energy is naturally a useful feature, since the energy of\n",
    "        speech varies a lot.\n",
    "    -   Voiced sounds generally have energy mainly at the low\n",
    "        frequencies, whereby estimators for spectral tilt are often\n",
    "        useful. For example,\n",
    "        -   Zero-crossings (per time unit) is high for high-frequency\n",
    "            signals (noise) and low for low-frequency signals (voiced\n",
    "            speech), whereby it can be used as a feature.\n",
    "        -   The lag-1 autocorrelation is high (close to one) for\n",
    "            low-frequency signals and low (close to -1) for\n",
    "            high-frequency signals.\n",
    "-   Speech sounds can be efficiently modelled by linear prediction.\n",
    "    -   If the prediction error is small, then it is likely that the\n",
    "        signal is speech.\n",
    "    -   If the prediction error is large, then it is probably\n",
    "        non-speech.\n",
    "-   Voiced speech has by definition a prominent pitch.\n",
    "    -   If we can identify a prominent pitch in the range then it is\n",
    "        likely voiced speech.\n",
    "-   Speech information is described effectively by their spectral\n",
    "    envelope.\n",
    "    -   MFCC can be used as a description of envelope information and it\n",
    "        is thus a useful set of features.\n",
    "    -   Linear prediction parameters (esp. prediction residual) also\n",
    "        describe envelope information and can thus also be used as a\n",
    "        feature-set.\n",
    "![vadfeat2](code/vad_features2-1.png)\n",
    "        \n",
    "-   Speech features vary rapidly and frequently.\n",
    "    -   By looking at the rate of change $\\Delta_k=f_{k+1}-f_k$ in other\n",
    "        features $f_k$, we obtain information about the rate of change\n",
    "        of the signal. (Estimate of derivative)\n",
    "    -   Likewise, we can look at the second difference\n",
    "        $\\Delta\\Delta_k=\\Delta_{k+1}-\\Delta_k$. (Estimate of second\n",
    "        derivative)\n",
    "    -   These first and second order differences can be used as features\n",
    "        and they are known as $\\Delta$- and $\\Delta\\Delta$-features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "\n",
    "-   We have collected a set of indicators for speech, the features,\n",
    "    whereby the next step is to merge the information from these\n",
    "    features to make a decision between speech and non-speech.\n",
    "-   Classification is generic problem, with plenty of solutions such as\n",
    "    -   decision trees (low-complexity, requires manual tuning)\n",
    "    -   linear classifier (relatively low-complexity, training from\n",
    "        data)\n",
    "    -   advanced methods such as neural networks, Gaussian mixture\n",
    "        models etc. (high-complexity, high-accuracy, training from data)\n",
    "![vad2](code/vad2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees (historical)\n",
    "\n",
    "-   Make a sequence of binary decisions (for example, is low or high\n",
    "    energy?) to decide whether signal is speech or non-speech.\n",
    "-   Decision trees are very simple to implement.\n",
    "-   Hard-coded $\\Rightarrow$ not very flexible.\n",
    "-   Noise in one feature can cause us to follow wrong path.\n",
    "    -   One noisy feature can break whole decision tree.\n",
    "-   Requires that each decision is manually tuned\\\n",
    "    $\\Rightarrow$ Lots of work, especially when tree is large\n",
    "-   Structure and development becomes very complex if the number of\n",
    "    features increase.\n",
    "-   Suitable for low-complexity systems and low-noise scenarios where\n",
    "    accuracy requirements are not so high.\n",
    "-   I did not prepare an illustration/figure of result.\n",
    "\n",
    "![vadtree](code/vad_decisiontree-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifier\n",
    "\n",
    "-   Instead of manually-tuned, binary decisions, can we use observed\n",
    "    data to make a statistical estimate?\n",
    "    -   Using training data would automate the tuning of the model.\\\n",
    "        Accuracy can be improved by adding more data.\n",
    "    -   By replacing binary decisions, we can let tendencies in several\n",
    "        features improve accuracy.\n",
    "-   Linear classifiers attempt to achieve a decision as a weighted sum\n",
    "    of the features.\n",
    "    -   Let $\\xi_k$ be the features.\n",
    "    -   The decision is then obtained by $\\eta = \\sum_k\\omega_k\\xi_k$,\n",
    "        where $\\omega_k$ are scalar weights.\n",
    "    -   The objective is to find weights $\\omega_k$ such that $$\\eta = \n",
    "              \\begin{cases}\n",
    "                -1 & \\text{non-speech} \\\\\n",
    "                +1 & \\text{speech}.\n",
    "              \\end{cases}$$\n",
    "\n",
    "\n",
    "-   We then need to develop a method for choosing optimal\n",
    "    weights $\\omega_k$.\n",
    "-   The first step is to define an *objective function*, which we can\n",
    "    minimize.\n",
    "    -   A good starting point is the classification error.\n",
    "    -   If $\\eta$ is the desired class for a frame and our classifier\n",
    "        gives $\\hat\\eta$, then the classification error is\n",
    "        $\\nu^2 = (\\eta -\\hat\\eta)^2$.\n",
    "    -   By minimizing the classification error, we can determine optimal\n",
    "        parameters $\\omega_k$.\n",
    "-   Let $x_k$ be the vector of all features for a frame $k$ and\n",
    "    $X=[x_0,\\,x_1\\dots]$ a matrix with all features for all frames.\n",
    "    \n",
    "    -   The classification of a single frame is then $\\eta_k=x_k^Tw$.\n",
    "    -   The classification of all frames is then a vector $y=X^Tw$,\n",
    "        where $w$ is the vector of weights $\\omega_k$.\n",
    "    -   The sum of classification errors is the norm $\\|y-\\hat y\\|^2$.\n",
    "    \n",
    "![vadlin](code/vad_linearclassifier-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit of math\n",
    "\n",
    "-   The minimum of the classification error $\\|y-\\hat y\\|^2$ can be\n",
    "    found by setting the partial derivative to zero. $$\\begin{split}\n",
    "          0 &= \\frac\\partial{\\partial w}\\|y-\\hat y\\|^2\n",
    "          = \\frac\\partial{\\partial w}\\|y-X^Tw\\|^2\n",
    "          \\\\&\n",
    "          = \\frac\\partial{\\partial w}(y^Ty+w^TXX^Tw-2w^TXy)\n",
    "          \\\\&\n",
    "          =2XX^Tw - 2Xy.\n",
    "        \\end{split}$$\n",
    "-   The solution is the Moore-Penrose pseudo-inverse\n",
    "    $$w = (XX^T)^{-1}Xy^T := X^\\dagger y.$$\n",
    "-   *Note:* This is a very common mathematical approach for solving\n",
    "    problems in speech processing, so it is much more important and\n",
    "    broadly applicable than \"only\" VAD.\n",
    "\n",
    "#### Pre-whitening (advanced topic)\n",
    "\n",
    "-   If the range of values from features are very different, we end up\n",
    "    with problems.\n",
    "    -   A very loud voice will overrun weaker ones, even if the loud one\n",
    "        is full of crap.\n",
    "    -   The range (mean and variance) of features need to be normalized.\n",
    "    -   Correlations between features are also undesirable.\n",
    "-   The first step is removal of the mean, $x' = x - E[x]$, where\n",
    "    $E[x]\\approx \\frac1N\\sum_kx_k$ and $N$ is the number of frames.\n",
    "-   The covariance of the features is then\n",
    "    $C=E[x'(x')^T]\\approx \\frac1N X^TX$, where $X$ now contains the\n",
    "    zero-mean features.\n",
    "-   The eigenvalue decomposition of $C$ is $C=V^TDV$, whereby we can\n",
    "    define the pre-whitening transform $A=D^{1/2}V$ and $x'' = Ax'$.\n",
    "    -   The covariance of the modified vector is\n",
    "        $E[x''(x'')^T] = AE[x'(x')^T]A^T = A C A^T = D^{1/2}V V^TDV V^T D^{1/2}=I$.\n",
    "    -   That is, $x''$ has uncorrelated samples with equal variance and\n",
    "        zero-mean.\n",
    "-   What you need to know is that pre-whitening is a pre-processing\n",
    "    step, applied *before* training $w$.\n",
    "-   We thus train the classifier on the modified vectors\n",
    "    $x''=A(x-E[x])$, to obtain the weights $w$.\n",
    "-   The classifier with pre-whitening is\n",
    "    $$\\nu=w^T x''=w^T A(x-E[x]) = \\hat w^T (x-E[x])$$ where $\\hat w=Aw$.\n",
    "-   In other words, the pre-whitening can be included in the weights, so\n",
    "    no additional complexity is introduced other than removal of the\n",
    "    mean (which is trivial).\n",
    "\n",
    "![vadfeat3](code/vad_features3-1.png)\n",
    "![vadfeat3](code/vad_features4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing\n",
    "\n",
    "![vadfeat5](code/vad_features5-1.png)\n",
    "\n",
    "-   Above, we trained the classifier to get values $\\pm1$, but when\n",
    "    using the classifier, we choose classes based on a threshold.\n",
    "-   In practice, it does not matter if the output value is $0.1$, $1$ or\n",
    "    $1000$, because if it is above threshold, then it belongs to the\n",
    "    same class.\n",
    "-   Trying to hit $\\pm1$ is therefore an unnecessarily difficult task!\n",
    "    -   We should be just trying to obtain a multidimensional threshold\n",
    "        which separates classes!\n",
    "\n",
    "-   We can truncate the output such that big values are reduced to\n",
    "    $\\pm1$.\n",
    "-   The sigmoid function is a map ${\\mathbb R}\\rightarrow[0,1]$ defined\n",
    "    as $$f(\\nu) = \\frac1{1+\\exp(-\\nu)}.$$   \n",
    "-   If we apply the sigmoid function on the output of the linear\n",
    "    classifier, then overshooting is not a problem anymore:\n",
    "    $$\\hat \\nu = f(w^T x) \\qquad\\in[0,1].$$\n",
    "\n",
    "![vadfeat5](code/sigmoid-1.png)\n",
    "\n",
    "-   Then there is no analytic solution, but we must use iterative\n",
    "    methods.\n",
    "-   This function is known as a *perceptron*. Combining perceptrons into\n",
    "    a interconnected network gives a *neural network.*\\\n",
    "    $\\Rightarrow$ A topic for other courses.\n",
    "-   Linear classifiers are only slightly more complex than decisions\n",
    "    trees, but much more accurate.\n",
    "    -   Main complexity of VAD lies in feature-extraction anyway, so the\n",
    "        differences in complexity of decision trees and linear\n",
    "        classifiers is negligible.\n",
    "-   The main advantages of linear classifiers in comparison to decision\n",
    "    trees are that\n",
    "    -   (unbiased) we can use real data to train the model, whereby we\n",
    "        can be certain that it corresponds to reality (no bias due to\n",
    "        manual tuning),\n",
    "    -   (robust) whereas noise in one feature can break a decision tree,\n",
    "        linear classifiers merge information from all features, thus\n",
    "        reducing effect of noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More advanced classifiers\n",
    "\n",
    "-   There exists a large range of better and more complex classifiers in\n",
    "    the general field of machine learning.\n",
    "    -   Linear discriminant analysis (LDA) -- splits the feature space\n",
    "        using hyper-planes.\n",
    "    -   Gaussian mixture models (GMM) -- the feature space is modelled\n",
    "        by a sum of Gaussians.\n",
    "    -   Deep neural networks (DNN) -- similar to linear classifiers but adds\n",
    "        non-linear mappings and several layers of sums.\n",
    "    -   K-nearest neighbors (kNN), support vector machine (SVM), random\n",
    "        forest classifiers, etc.\n",
    "-   These methods are in general more effective, but training and\n",
    "    application is more complex.\n",
    "    -   Advice: Try a simple approach first and see if its good enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Presence Probability\n",
    "\n",
    "-   The output of the classifier is a continuous number, but it is\n",
    "    thresholded to obtain a decision.\n",
    "-   The continuous output contains a lot of information about the signal\n",
    "    which is lost when applying thresholding.\n",
    "    -   With a high value we are really certain that the signal is\n",
    "        speech, while a value near the threshold is relatively\n",
    "        uncertain.\n",
    "-   We can use the classifier output as an estimate of the probability\n",
    "    that the signal is speech $\\Rightarrow$ It is an estimator for\n",
    "    *speech presence probability*.\n",
    "-   Subsequent applications can use this information as input to improve\n",
    "    performance.\n",
    "    \n",
    "![vadspp](code/vad_spp-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Output before thresholding\n",
    "![vadfeat5](code/vad_features5-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise types\n",
    "-   As noted before, VAD is trivial in noise-free scenarios.\n",
    "-   In practice, typical background noise types are for example, office\n",
    "    noise, car noise, cafeteria (babble) noise, street noise, factory\n",
    "    noise, ...\n",
    "-   Clearly the problem is easier if the noise has a very different\n",
    "    character than the speech signal.\n",
    "    -   Speech is quickly varying $\\Rightarrow$ stationary noises are\n",
    "        easy.\n",
    "    -   Speech is dominated by low frequencies $\\Rightarrow$ high\n",
    "        frequency noises are easy.\n",
    "-   The classic worst case is a competing (undesired) speaker, when\n",
    "    someone else is speaking in the background (babble noise).\n",
    "    -   However, that would be difficult also for a human listener,\n",
    "        whereby it actually is a very difficult problem.\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "-   Voice activity detection is a type of methods which attempt to\n",
    "    determine if a signal is speech or non-speech.\n",
    "    -   In a noise-free scenario the task is trivial, but it is also not\n",
    "        a realistic scenario.\n",
    "-   The basic idea of algorithms is:\n",
    "    1.  Calculate a set of features from the signal which are designed\n",
    "        to analyze properties which differentiate speech and non-speech.\n",
    "    2.  Merge the information from the features in a classifier, which\n",
    "        returns the likelihood that the signal is speech.\n",
    "    3.  Threshold the classifier output to determine whether the signal\n",
    "        is speech or not.\n",
    "-   VADs are used as a low-complexity pre-processing method, to save\n",
    "    resources (e.g. complexity or bitrate) in the main task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
