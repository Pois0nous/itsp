
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>11.1. Noise attenuation &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11.2. Echo cancellation" href="Echo_cancellation.html" />
    <link rel="prev" title="11. Speech enhancement" href="../Speech_enhancement.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Representations/Representations.html">
   3. Basic Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_analysis.html">
     3.1. Short-time analysis of speech and audio signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_processing.html">
     3.2. Short-time processing of speech signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Waveform.html">
     3.3. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Windowing.html">
     3.4. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Signal_energy_loudness_and_decibel.html">
     3.5. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Spectrogram_and_the_STFT.html">
     3.6. Spectrogram and the STFT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Autocorrelation_and_autocovariance.html">
     3.7. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Melcepstrum.html">
     3.8. The cepstrum, mel-cepstrum and mel-frequency cepstral coefficients (MFCCs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Linear_prediction.html">
     3.9. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Fundamental_frequency_F0.html">
     3.10. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Zero-crossing_rate.html">
     3.11. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Deltas_and_Delta-deltas.html">
     3.12. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.13. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Jitter_and_shimmer.html">
     3.14. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Analysis/Measurements_for_medical_applications.html">
     7.2. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
       7.2.1. Inverse filtering for glottal activity estimation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Forensic_analysis.html">
     7.3. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Voice_activity_detection.html">
     8.1. Voice Activity Detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Recognition_and_Verification.html">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Paralinguistic_speech_processing.html">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Concatenative_speech_synthesis.html">
     9.1. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Statistical_parametric_speech_synthesis.html">
     9.2. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Basic_tools.html">
     10.2. Basic tools
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.3. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Entropy_coding.html">
       10.3.4. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.3.5. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Code-excited_linear_prediction_CELP.html">
     10.4. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Frequency-domain_coding.html">
     10.5. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://landbot.io/blog/guide-to-conversational-design/">
   Chatbots / Conversational design (external link)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Security_and_privacy_in_speech_technology.html">
   13. Security and privacy in speech technology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../References.html">
   14. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Enhancement/Noise_attenuation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classical-methods">
   11.1.1. Classical methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spectral-subtraction">
     11.1.1.1. Spectral subtraction
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#scalar-wiener-filtering">
       11.1.1.1.1. Scalar Wiener filtering
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wiener-filtering-for-vectors">
     11.1.1.2. Wiener filtering for vectors
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speech-presence-estimation-in-noise-attenuation">
   11.1.2. Speech presence estimation in noise attenuation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#masks-power-spectra-and-temporal-characteristics">
   11.1.3. Masks, power spectra and temporal characteristics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-methods">
   11.1.4. Machine learning methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attachments">
   11.1.5. Attachments:
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Noise attenuation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classical-methods">
   11.1.1. Classical methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spectral-subtraction">
     11.1.1.1. Spectral subtraction
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#scalar-wiener-filtering">
       11.1.1.1.1. Scalar Wiener filtering
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wiener-filtering-for-vectors">
     11.1.1.2. Wiener filtering for vectors
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speech-presence-estimation-in-noise-attenuation">
   11.1.2. Speech presence estimation in noise attenuation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#masks-power-spectra-and-temporal-characteristics">
   11.1.3. Masks, power spectra and temporal characteristics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-methods">
   11.1.4. Machine learning methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attachments">
   11.1.5. Attachments:
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="noise-attenuation">
<h1><span class="section-number">11.1. </span>Noise attenuation<a class="headerlink" href="#noise-attenuation" title="Permalink to this headline">¶</a></h1>
<div class="contentLayout2">
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>When using speech technology in realistic environments, such as at home,
office or in a car, there will invariably be also other sounds present
and not only the speech sounds of desired speaker. There will be the
background hum of computers and air conditioning, cars honking, other
speakers, and so on. Such sounds reduces the quality of the desired
signal, making it more strenuous to listen, more difficult to understand
or at the worst case, it might render the speech signal unintelligible.
A common feature of these sounds is however that they are <em>independent</em>
of and <em>uncorrelated</em> with the desired signal.</p>
<p>That is, we can usually assume that such noises are <em>additive</em>, such
that the observed signal <em>y</em> is the sum of the desired signal <em>x</em> and
interfering noises <em>v</em>, that is, <em>y=x+v</em>. To improve the quality of the
observed signal, we would like to make an estimate \( \hat x \) * *of
the desired signal <em>x</em>. The estimate should approximate the desired
signal \( x\approx \hat x \) or conversely, we would like to
minimize the distance \( d\left(x,\hat x\right) \) with some
distance measure <em>d(,.,)</em>.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>Photo by Jezael Melgoza on Unsplash</p>
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
<div class="section" id="classical-methods">
<h2><span class="section-number">11.1.1. </span>Classical methods<a class="headerlink" href="#classical-methods" title="Permalink to this headline">¶</a></h2>
<div class="section" id="spectral-subtraction">
<h3><span class="section-number">11.1.1.1. </span>Spectral subtraction<a class="headerlink" href="#spectral-subtraction" title="Permalink to this headline">¶</a></h3>
<p>The <span class="xref myst">STFT spectrum</span> of a signal is a good
domain for noise attenuation because we can reasonably safely assume
that spectral components are uncorrelated with each other, such that we
treat each component separately. In other words, in the spectrum, we can
apply noise attenuation on every frequency bin with scalar operations,
whereas if the components would be correlated, we would have to use
vector and matrix operations. The benefit of scalar operations is that
they are computationally simple, <em>O(N)</em>, whereas matrix operations are
typically at least <em>O(N<sup>2</sup>).</em></p>
<p>The basic idea of spectral subtraction is that we assume that we have
access to an estimate of the noise energy \( E[|v|^2] = \sigma_v^2
\) , and we subtract that from the energy of the observation, such that
we define the energy of our estimate as</p>
<p>\[ |\hat x|^2 := |y|^2 - \sigma_v^2. \]</p>
<p>Unfortunately, since our estimate of noise energy is not perfect and
because we have hiddenly made an inaccurate assumption that <em>x</em> and <em>v</em>
are uncorrelated, the above formula can give negative values for the
energy estimate. Negative energy is not realizable and nobody likes
pessimists, so we have to modify the formula to threshold at zero</p>
<p>\[ |\hat x|^2 := \begin{cases} |y|^2 - \sigma_v^2 &amp; \text{if }
|y|^2 \geq \sigma_v^2 \ 0 &amp; \text{if } |y|^2 &lt; \sigma_v^2
\end{cases}. \]</p>
<p>Since STFT spectra are complex-valued, we then still have to find the
complex angle of the signal estimate. If the noise energy is small \(
|v|^2 \ll |x|^2 \) , then the complex angle of <em>x</em> is
approximately equal to the angle of <em>y</em>, \( \angle x \approx \angle
y \) , such that our final estimate is (when \( |y|^2\geq
\sigma_v^2 \) )</p>
<p>\[ \hat x := \angle y \cdot |\hat x| = \frac{y}{|y|} \sqrt{
|y|^2 - \sigma_v^2} = y \sqrt{\frac{|y|^2 -
\sigma_v^2}{|y|^2}}. \]</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>Spectrograms of the original signal (above) and its enhanced version
obtained with spectral subtraction applied on the STFT (below)</p>
<p><img src="attachments/172995194/175508704.png" class="image-center"
data-image-src="attachments/172995194/175508704.png"
data-unresolved-comment-count="0" data-linked-resource-id="175508704"
data-linked-resource-version="1" data-linked-resource-type="attachment"
data-linked-resource-default-alias="specsub_spectrogram.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="172995194"
data-linked-resource-container-version="33" height="400" /></p>
<p>Original noisy sample</p>
<p><a href="attachments/172995194/175508733.wav"
data-linked-resource-id="175508733" data-linked-resource-version="1"
data-linked-resource-type="attachment"
data-linked-resource-default-alias="sound_sample.wav"
data-nice-type="Multimedia"
data-linked-resource-content-type="audio/x-wav"
data-linked-resource-container-id="172995194"
data-linked-resource-container-version="33">sound_sample.wav</a></p>
<p>Enhanced with spectral subtraction</p>
<p><a href="attachments/172995194/175508717.wav"
data-linked-resource-id="175508717" data-linked-resource-version="1"
data-linked-resource-type="attachment"
data-linked-resource-default-alias="sound_sample_specsub.wav"
data-nice-type="Multimedia"
data-linked-resource-content-type="audio/x-wav"
data-linked-resource-container-id="172995194"
data-linked-resource-container-version="33">sound_sample_specsub.wav</a></p>
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
<div class="section" id="scalar-wiener-filtering">
<h4><span class="section-number">11.1.1.1.1. </span>Scalar Wiener filtering<a class="headerlink" href="#scalar-wiener-filtering" title="Permalink to this headline">¶</a></h4>
<p>Observe that the form of the relationship above is \( \hat x = y\cdot
g, \) where <em>g</em> is a scalar scaling coefficient. Instead of the above
heuristic, we could then derive a formula which gives the smallest
error, for example in the minimum error energy expectation sense or
minimum mean square error (MMSE). Specifically, the error energy
expectation is</p>
<p>\[ E\left[|e|^2\right] = E\left[|x-\hat x|^2\right] =
E\left[|x-gy|^2\right] = E\left[|x|^2\right] + g^2
E\left[|y|^2\right] - 2g E\left[xy\right]. \]</p>
<p>If we assume that target speech and noise are uncorrelated, \(
E\left[xv\right]=0 \) ,</p>
<p>then \(
E\left[xy\right]=E\left[x(x+v)\right]=E\left[|x|^2\right]
\) and</p>
<p>\[ E\left[|e|^2\right] = E\left[|x|^2\right] + g^2
E\left[|y|^2\right] - 2g E\left[|x|^2\right] =
(1-2g)E\left[|x|^2\right] + g^2 E\left[|y|^2\right]. \]</p>
<p>The minimum is found by setting the derivative to zero</p>
<p>\[ 0 = \frac{\partial}{\partial g}E\left[|e|^2\right] =
-2E\left[|x|^2\right] + 2 g E\left[|y|^2\right], \]</p>
<p>such that the final solution is</p>
<p>\[ g = \frac{E\left[|x|^2\right]}{E\left[|y|^2\right]} =
\frac{E\left[|y|^2\right]-\sigma_v^2}{E\left[|y|^2\right]}.
\]</p>
<p>and the Wiener estimate becomes</p>
<p>\[ \hat x := y \left(\frac{|y|^2 - \sigma_v^2}{|y|^2}\right).
\]</p>
<p>Observe that this estimate is almost equal to the above, but the square
root is omitted. With different optimization criteria, we can easily
derive further such estimates. Such estimates have different weaknesses
and strengths and it is then a matter of application specific tuning to
choose the best estimate.</p>
<p>Overall, it is however somewhat unsatisfactory that <em>additive</em> noise is
attenuated with a <em>multiplicative</em> method. However, without a better
model of source and noise characteristics, this is probably the best we
can do. Still, spectral subtraction is a powerful method when taking
into account how simple it is. With minimal assumptions we obtain a
signal estimate which can give a clear improvement in quality.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>Spectrograms of the original signal (above) and its enhanced version
obtained with (scalar) Wiener filtering applied on the STFT (below)</p>
<p><img src="attachments/172995194/175508702.png" class="image-center"
data-image-src="attachments/172995194/175508702.png"
data-unresolved-comment-count="0" data-linked-resource-id="175508702"
data-linked-resource-version="1" data-linked-resource-type="attachment"
data-linked-resource-default-alias="wiener_spectogram.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="172995194"
data-linked-resource-container-version="33" height="400" /></p>
<p>Enhanced with (scalar) Wiener filtering</p>
<p><a href="attachments/172995194/175508720.wav"
data-linked-resource-id="175508720" data-linked-resource-version="1"
data-linked-resource-type="attachment"
data-linked-resource-default-alias="sound_sample_wiener.wav"
data-nice-type="Multimedia"
data-linked-resource-content-type="audio/x-wav"
data-linked-resource-container-id="172995194"
data-linked-resource-container-version="33">sound_sample_wiener.wav</a></p>
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
<div class="section" id="wiener-filtering-for-vectors">
<h3><span class="section-number">11.1.1.2. </span>Wiener filtering for vectors<a class="headerlink" href="#wiener-filtering-for-vectors" title="Permalink to this headline">¶</a></h3>
<p>Above we considered the scalar case, or conversely, the case where we
can treat components of a vector to be independent such that they can be
equivalently treated as a collection of scalars. In some cases, however,
we might be unable to find an uncorrelated representation of the signal
or the corresponding whitening process could be unfeasibly complex or it
can incur too much algorithmic delay. We then have to take into account
the correlation between components.</p>
<p>Consider for example a desired signal \( x\in{\mathbb R}^{N \times1}
\) , a noise signal \( v\in{\mathbb R}^{N \times1} \) and their
additive sum, the observation \( y = x+v, \) from which we want to
estimate the desired signal with a linear filter \( \hat x := a^H y.
\) Following the MMSE derivation above, we set the derivative of the
error energy expectation to zero</p>
<p>\[ \begin{split} 0&amp;=\frac{\partial}{\partial
a}E\left[|e|^2\right] =\frac{\partial}{\partial
a}E\left[|x-\hat x|^2\right] =\frac{\partial}{\partial
a}E\left[|x-a^H y|^2\right] \&amp; =\frac{\partial}{\partial
a}E\left[|x-a^H (x+v)|^2\right] =2E\left[(x+v)\left(x-a^H
(x+v)\right)^H\right] \&amp; =2\left[ E[xx^H] - \left(E[xx^H] +
E[vv^H]\right)a\right] =2\left[ R_{xx} - \left(R_{xx} +
R_{vv}\right)a\right] \end{split} \]</p>
<p>Where the covariance matrices are \( R_{xx} = E[xx^H] \) and \(
R_{vv} = E[vv^H] \) , and we used the fact that <em>x</em> and <em>v</em> are
uncorrelated \( E[xv^H]=0 \) . The solution is then clearly</p>
<p>\[ a=\left(R_{xx}+R_{vv}\right)^{-1} R_{xx} = R_{yy}^{-1}
\left(R_{yy}-R_{vv}\right), \]</p>
<p>where we for now assume that the inverse exists. This solution is
clearly similar to the MMSE solution for the scalar case.</p>
<p>A central weakness of this approach is that it involves a matrix
inversion, which is computationally complex operation, such that on-line
application is challenging. It furthermore requires that the covariance
matrix <em>R<sub>yy</sub></em> is invertible (positive definite), which places
constraints on the methods for estimating such covariances.</p>
<p>In any case, Wiener filtering is a convenient method, because it
provides an analytical expression for an optimal solution in noise
attenuation. It consequently has very well documented properties and
performance guarantees.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
<div class="section" id="speech-presence-estimation-in-noise-attenuation">
<h2><span class="section-number">11.1.2. </span>Speech presence estimation in noise attenuation<a class="headerlink" href="#speech-presence-estimation-in-noise-attenuation" title="Permalink to this headline">¶</a></h2>
<p>Our objective is to improve speech quality and we want enhance speech
which is corrupted by noise. Speech however is typically highly varying
in time and especially in a conversation, usually speakers do not speak
at the same time. Thus often one speaker is silent about half the time.
If there is no speech noise attenuation becomes trivial; Noise can be
removed by zeroing the whole signal.</p>
<p>To take advantage of such non-speech periods, we can use <span class="xref myst">voice activity
detection (VAD)</span> to determine which parts
of the signal have speech and apply noise attenuation only there.
Paraphrasing, we have:</p>
<div class="code panel pdl" style="border-width: 1px;">
<div class="codeContent panelContent pdl">
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>If VAD(y) == Speech
   xhat = Enhance(y)
else
   xhat = 0
</pre></div>
</div>
</div>
</div>
<p>This however requires that our voice activity detector is reliable also
for noisy speech signals, which is difficult. An alternative is to
estimate posteriori likelihoods as follows:</p>
<div class="code panel pdl" style="border-width: 1px;">
<div class="codeContent panelContent pdl">
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>xhat_noisyspeech = Enhance(y)
vhat_noisyspeech = y - xhat_noisyspeech
vhat_noiseonly = y
SpeechLikelihood = PosterioriLikelihood(xhat_noisyspeech) * PosterioriLikelihood(vhat_noisyspeech) * PriorLikelihood(speech)
NonspeechLikelihood = PosterioriLikelihood(vhat_noiseonly) * (1 - PriorLikelihood(speech)
If SpeechLikelihood &gt; NonspeechLikelihood
   xhat = xhat_noisyspeech
else
   xhat = 0
</pre></div>
</div>
</div>
</div>
<p>This method thus always tries to attenuate noise and then looks at the
output whether “speech” or “non-speech” is more likely. Such hypothesis
testing is often useful but assumes that we have access to effective
likelihood estimators for both speech and noise.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>Enhanced with (scalar) Wiener filtering and gated with VAD</p>
<p><img src="attachments/172995194/175508726.png"
data-image-src="attachments/172995194/175508726.png"
data-unresolved-comment-count="0" data-linked-resource-id="175508726"
data-linked-resource-version="1" data-linked-resource-type="attachment"
data-linked-resource-default-alias="wiener_gated_spectrogram.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="172995194"
data-linked-resource-container-version="33" height="400" /></p>
<p><a href="attachments/172995194/175508721.wav"
data-linked-resource-id="175508721" data-linked-resource-version="1"
data-linked-resource-type="attachment"
data-linked-resource-default-alias="sound_sample_wiener_gated.wav"
data-nice-type="Multimedia"
data-linked-resource-content-type="audio/x-wav"
data-linked-resource-container-id="172995194"
data-linked-resource-container-version="33">sound_sample_wiener_gated.wav</a></p>
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
<div class="section" id="masks-power-spectra-and-temporal-characteristics">
<h2><span class="section-number">11.1.3. </span>Masks, power spectra and temporal characteristics<a class="headerlink" href="#masks-power-spectra-and-temporal-characteristics" title="Permalink to this headline">¶</a></h2>
<p>As seen above, we can attenuate noise if we have a good estimate of the
noise energy. However, actually, both the spectral subtraction and
Wiener filtering methods use models of the speech and noise energies.
The models used above were rather heuristic; noise energy was assumed to
be “known” and speech energy was defined as energy of observation minus
noise energy. It is however not very difficult to make better models
than that. Before going to improved models, note that we did not use
speech and noise energies independently, but only their ratio. Now
clearly the ratio of speech and noise is the signal-to-noise ratio (SNR)
of that particular component. We thus obtain an estimate of the SNR of
the whole spectrum. Conversely, we would need only the SNR of the
spectrum to attenuate noise with the above methods. The SNR as a
function of frequency and time is often referred to as a <em>mask</em> and in
the following we will discuss some methods for generating such masks. It
is however important to understand that mask-based methods are operating
on the power (or magnitude) spectrum and thus do not include any models
of the complex phase. Indeed, efforts have in general focused mostly on
the power spectrum and much less on the phase. On one hand, the
motivation is that characteristics of the power spectrum are much more
important to perception than the phase (though the phase is also
important), but on the other hand, the power spectrum is also much
easier to work with than the phase. Therefore there has been both much
more demand and supply of methods which treat the power spectrum.</p>
<p>To model speech signals, we can begin by looking at spectral envelopes,
the high-level structure of the power spectrum. It is well-known that
the phonetic information of speech lies primarily in the shape of the
spectral envelope, and the lowest-frequency peaks of the envelope
identify the vowels uniquely. In other words, the distribution of
spectral energy varies smoothly across the frequencies. This is
something we can model and use to estimate the spectral mask. Similarly,
we know that phonemes vary slowly over time, which means that the
envelope varies slowly over time. Thus, again, we can model envelope
behaviour over time to generate spectral masks.</p>
<p>A variation of such masks is known as <em>binary</em> masks, where we can set,
for example, that the mask is 1 if speech energy is larger than noise
energy, and 0 otherwise. Clearly this is equivalent with thresholding
the SNR at unity (which is equivalent to 0 dB), such that an SNR-mask
can always be converted to a binary mask, but the reverse is not
possible. The benefit of binary masks is that it simplifies some
computations.</p>
<p>If we then want to attenuate noise in a particular frame of speech it is
then useful to use as much of the surrounding data (context) as
possible. For best quality, we can model, say, a second of the speech
signal both before and after the target frame. Though this can improve
quality of the estimate, this has two clear negative consequences. First
of all, including more data increases computational complexity. The
level of acceptable complexity is though highly dependent on the
application. Secondly, if we look into <em>future</em> frames to process the
current frame, then we have to have access to such data. In a on-line
system, this means that we have to wait for the whole look-ahead data to
arrive before processing, such that the overall system has a delay
corresponding to the length of the look-ahead. We can extrapolate the
current frame from past frames, but interpolating between past and
future frames does give much better quality. The amount of acceptable
delay is also an application dependent question.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
<div class="section" id="machine-learning-methods">
<h2><span class="section-number">11.1.4. </span>Machine learning methods<a class="headerlink" href="#machine-learning-methods" title="Permalink to this headline">¶</a></h2>
<p>The first choice in designing machine learning methods for noise
attenuation and other speech enhancement tasks is the overall systems
architecture. The application is usually simply a neural network which
takes noisy speech as input and outputs an estimate of the clean speech.
A natural choice would then be to train the network with a large
database of noisy speech samples and minimize the distance of the output
to the clean speech signal. Since we assume that noise is additive, we
can create synthetic samples by adding noise to speech. By varying the
intensity (volume) of the noise samples, we can further choose the
signal to noise ratio of the noise samples. With reasonable size
databases of speech and noise, we thus get a practically infinite number
of unique noisy samples such that we can make even a large neural
network to converge.</p>
<p>A weakness of this model however is that even if the database is thus
large, it has only a limited number of unique noises and unique
speakers. There is no easy way of getting assurance that unseen noises
and speakers would be enhanced effectively. What if we receive a noisy
sample where a 3 year-old child talks with annoying
<a class="reference external" href="https://en.wikipedia.org/wiki/Vuvuzela">vuvuzelas</a> playing in the
background. If our database contained only adult speakers and did not
contain vuvuzela-sounds, then we cannot know whether our enhancement is
effective on the noisy sample.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>Machine learning configuration for speech enhancement with noisy and
target clean speech signal.</p>
<p><img src="attachments/172995194/175508188.png" class="image-center"
data-image-src="attachments/172995194/175508188.png"
data-unresolved-comment-count="0" data-linked-resource-id="175508188"
data-linked-resource-version="3" data-linked-resource-type="attachment"
data-linked-resource-default-alias="speechenhancement1.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="172995194"
data-linked-resource-container-version="33" height="400" /></p>
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>To overcome the problem of inadequate noise databases, we can take an
<em>adversarial</em> approach, where we have a <em>generative</em> network which
generates noises and an enhancement network which attenuates noises
which corrupt speech. This approach is known as a <em>generative
adversarial network (GAN)</em>. We then have two optimization tasks;</p>
<ol class="simple">
<li><p>To optimize the enhancement network (minimize estimation error) to
remove the noise generated by the generative network and</p></li>
<li><p>to optimize the generative network (maximize estimation error) to
generate noises which the enhancement network is unable to remove.</p></li>
</ol>
<p>These two tasks are adversial in the sense that they work against each
other. In practical application we would use only the enhancement
network, so the generative network is used only in training.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>Application and training with a generative adversarial network (GAN)
structure for speech enhancement.</p>
<p><img src="attachments/172995194/175508259.png"
data-image-src="attachments/172995194/175508259.png"
data-unresolved-comment-count="0" data-linked-resource-id="175508259"
data-linked-resource-version="4" data-linked-resource-type="attachment"
data-linked-resource-default-alias="speechenhancement2.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="172995194"
data-linked-resource-container-version="33" width="342" /></p>
</div>
</div>
</div>
</div>
<div class="pageSectionHeader">
</div>
<div class="section" id="attachments">
<h2><span class="section-number">11.1.5. </span>Attachments:<a class="headerlink" href="#attachments" title="Permalink to this headline">¶</a></h2>
</div>
<div class="greybox" align="left">
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[jezael-melgoza-KbR06h9dNQw-unsplash_scaled.png](attachments/172995194/175508067.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[speechenhancement1.png](attachments/172995194/175508243.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[speechenhancement1.png](attachments/172995194/175508257.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[speechenhancement1.png](attachments/172995194/175508188.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[speechenhancement2.png](attachments/172995194/175508261.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[speechenhancement2.png](attachments/172995194/175508263.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[speechenhancement2.png](attachments/172995194/175508743.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[wiener_spectogram.png](attachments/172995194/175508702.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[specsub_spectrogram.png](attachments/172995194/175508704.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[sound_sample_mono.wav](attachments/172995194/175508716.wav)
(audio/x-wav)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[sound_sample_specsub.wav](attachments/172995194/175508717.wav)
(audio/x-wav)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[sound_sample_specsub_gated.wav](attachments/172995194/175508718.wav)
(audio/x-wav)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[sound_sample_wiener.wav](attachments/172995194/175508720.wav)
(audio/x-wav)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[sound_sample_wiener_gated.wav](attachments/172995194/175508721.wav)
(audio/x-wav)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[specsub_gated_spectrogram.png](attachments/172995194/175508725.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[wiener_gated_spectrogram.png](attachments/172995194/175508726.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[sound_sample.wav](attachments/172995194/175508733.wav) (audio/x-wav)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[speechenhancement2.png](attachments/172995194/175508259.png)
(image/png)  
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Enhancement"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../Speech_enhancement.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">11. </span>Speech enhancement</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Echo_cancellation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11.2. </span>Echo cancellation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>