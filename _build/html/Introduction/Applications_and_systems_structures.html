
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2.6. Applications and systems structures &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Basic Representations" href="../Representations/Representations.html" />
    <link rel="prev" title="2.5. Linguistic structure of speech" href="Linguistic_structure_of_speech.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../Introduction.html">
   2. Introduction
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Representations/Representations.html">
   3. Basic Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_analysis.html">
     3.1. Short-time analysis of speech and audio signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_processing.html">
     3.2. Short-time processing of speech signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Waveform.html">
     3.3. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Windowing.html">
     3.4. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Signal_energy_loudness_and_decibel.html">
     3.5. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Spectrogram_and_the_STFT.html">
     3.6. Spectrogram and the STFT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Autocorrelation_and_autocovariance.html">
     3.7. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Melcepstrum.html">
     3.8. The cepstrum, mel-cepstrum and mel-frequency cepstral coefficients (MFCCs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Linear_prediction.html">
     3.9. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Fundamental_frequency_F0.html">
     3.10. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Zero-crossing_rate.html">
     3.11. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Deltas_and_Delta-deltas.html">
     3.12. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.13. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Jitter_and_shimmer.html">
     3.14. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Analysis/Measurements_for_medical_applications.html">
     7.2. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
       7.2.1. Inverse filtering for glottal activity estimation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Forensic_analysis.html">
     7.3. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Voice_activity_detection.html">
     8.1. Voice Activity Detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Recognition_and_Verification.html">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Paralinguistic_speech_processing.html">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Concatenative_speech_synthesis.html">
     9.1. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Statistical_parametric_speech_synthesis.html">
     9.2. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.2. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Entropy_coding.html">
       10.2.5. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.2.6. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Code-excited_linear_prediction_CELP.html">
     10.3. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Frequency-domain_coding.html">
     10.4. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Security_and_privacy_in_speech_technology.html">
   13. Security and privacy in speech technology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../References.html">
   14. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Introduction/Applications_and_systems_structures.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications">
   2.6.1. Applications
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#systems-structures">
   2.6.2. Systems structures
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transmission-and-storage">
     2.6.2.1. Transmission and storage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#information-extraction">
     2.6.2.2. Information extraction
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speech-synthesis">
   2.6.3. Speech synthesis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#user-interfaces">
   2.6.4. User-interfaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#processing-and-preprocessing">
   2.6.5. Processing and preprocessing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#voice-activity-detection">
     2.6.5.1. Voice activity detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keyword-spotting-or-wake-word-detection">
     2.6.5.2. Keyword spotting or Wake-word detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#speech-enhancement">
     2.6.5.3. Speech enhancement
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-signal-processing">
     2.6.5.4. Other signal processing
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Applications and systems structures</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications">
   2.6.1. Applications
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#systems-structures">
   2.6.2. Systems structures
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transmission-and-storage">
     2.6.2.1. Transmission and storage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#information-extraction">
     2.6.2.2. Information extraction
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speech-synthesis">
   2.6.3. Speech synthesis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#user-interfaces">
   2.6.4. User-interfaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#processing-and-preprocessing">
   2.6.5. Processing and preprocessing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#voice-activity-detection">
     2.6.5.1. Voice activity detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keyword-spotting-or-wake-word-detection">
     2.6.5.2. Keyword spotting or Wake-word detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#speech-enhancement">
     2.6.5.3. Speech enhancement
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-signal-processing">
     2.6.5.4. Other signal processing
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="applications-and-systems-structures">
<h1><span class="section-number">2.6. </span>Applications and systems structures<a class="headerlink" href="#applications-and-systems-structures" title="Permalink to this headline">¶</a></h1>
<div class="section" id="applications">
<h2><span class="section-number">2.6.1. </span>Applications<a class="headerlink" href="#applications" title="Permalink to this headline">¶</a></h2>
<p>Speech processing is used in, for example;</p>
<ul class="simple">
<li><p>Telecommunication</p>
<ul>
<li><p>Phones and mobile phones</p></li>
<li><p>Teleconferencing systems</p></li>
<li><p>Voice-over-IP, like
<a class="reference external" href="https://en.wikipedia.org/wiki/Skype">Skype</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Google_Hangouts">Google
Hangouts</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/FaceTime">Facetime</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Zoom_Video_Communications">Zoom</a></p></li>
<li><p>Podcasts, digital radio and TV</p></li>
<li><p>Virtual reality and gaming applications</p></li>
</ul>
</li>
<li><p>Speech operated virtual assistants, like
<a class="reference external" href="https://en.wikipedia.org/wiki/Siri">Siri</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Amazon_Alexa">Alexa</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Google_Assistant">Google
Assistant</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Mycroft_(software)">Mycroft</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Cortana_%28software%29">Cortana</a> etc.</p>
<ul>
<li><p>Also speech interfaces of robots</p></li>
</ul>
</li>
<li><p>Navigators with speech feedback, like
<a class="reference external" href="https://en.wikipedia.org/wiki/TomTom">TomTom</a> and
<a class="reference external" href="https://en.wikipedia.org/wiki/Garmin">Garmin</a></p></li>
<li><p>Automated telephone services (like the helpdesk of an airline)</p></li>
<li><p>Automated transcription</p>
<ul>
<li><p>Youtube subtitles</p></li>
<li><p>Recorded notes from doctors</p></li>
</ul>
</li>
<li><p>Microphones and microphone systems</p>
<ul>
<li><p>Headsets, with or without noise attenuation, with or without
active noise cancelling</p></li>
<li><p>Stage microphones (related to audio processing)</p></li>
</ul>
</li>
<li><p>Studio recording systems (related to audio processing)</p>
<ul>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Auto-Tune">Autotune</a></p></li>
<li><p>Noise attenuation/reduction</p></li>
</ul>
</li>
<li><p>Dubbing movies e.g. for translation of audio</p>
<ul>
<li><p>Also on-line translation</p></li>
</ul>
</li>
<li><p>Speech synthesis services</p>
<ul>
<li><p>Automated e-book readers</p></li>
<li><p>User-interfaces for the blind and the handicapped</p></li>
</ul>
</li>
<li><p>Less common applications</p>
<ul>
<li><p>Access control and fraud detection with speaker identification
and verification</p></li>
<li><p>Anonymization and obfuscation (e.g. witness protection) of
speech signals</p></li>
<li><p>Speech synthesizers for disabled people (e.g. Stephen Hawking)</p></li>
<li><p>Medical analysis of speech signals (e.g.
<a class="reference external" href="https://en.wikipedia.org/wiki/Alzheimer%27s_disease">Alzheimer</a>
detection)</p></li>
</ul>
</li>
</ul>
<p><img alt="app1" src="../_images/165138615.jpeg" />
<img alt="app2" src="../_images/165138616.png" />
<img alt="app4" src="../_images/165138617.png" />
<img alt="app3" src="../_images/165138618.png" />
<img alt="app3" src="../_images/165138684.png" /></p>
<p>Such applications can be categorized according to functionality, roughly
as:</p>
<ul class="simple">
<li><p>Transmission and storage of speech</p>
<ul>
<li><p>Enable communication with a far-away person</p></li>
</ul>
</li>
<li><p>Speech operated user-interfaces</p>
<ul>
<li><p>Enable spoken interaction with a machine</p></li>
</ul>
</li>
<li><p>Information extraction from a speech signal</p>
<ul>
<li><p>Like automated transcription to generate subtitles for movies</p></li>
</ul>
</li>
<li><p>Speech synthesis, i.e. automated generation of speech</p></li>
<li><p>“Improvement” of a speech signal, such as</p>
<ul>
<li><p>Such as noise reduction, translation</p></li>
</ul>
</li>
</ul>
<p>Note that these categories are in many senses overlapping; for example,
noise reduction can be a part of any speech processing system and
information extraction is practically a mandatory part of speech
operated user-interfaces.</p>
</div>
<div class="section" id="systems-structures">
<h2><span class="section-number">2.6.2. </span>Systems structures<a class="headerlink" href="#systems-structures" title="Permalink to this headline">¶</a></h2>
<div class="section" id="transmission-and-storage">
<h3><span class="section-number">2.6.2.1. </span>Transmission and storage<a class="headerlink" href="#transmission-and-storage" title="Permalink to this headline">¶</a></h3>
<p>The objective of speech transmission systems is to compress the signal
to as few bits as possible, while keeping the sound quality at the
output as good as possible. This requires that the degradations that we
introduce are chosen such that their perceptual influence is as small as
possible. In other words, we would not like the listener to notice (or
to notice as little as possible) that the signal has been degraded. In
the illustration on the right, at the encoder on the sender side, we
therefore have a model of perceptual importance, which determines how
the signal is quantized. The quantized signal is then compressed to as
few bits as possible. For such compression, we use statistical
information about speech signals.</p>
<p>The decoder at the receiving side, reverses the steps by decompression
and dequantization.</p>
<p>Pre-processing operations would typically include noise attenuation and
voice activity detection (see below).</p>
<p><img alt="struct1" src="../_images/165138696.png" /></p>
</div>
<div class="section" id="information-extraction">
<h3><span class="section-number">2.6.2.2. </span>Information extraction<a class="headerlink" href="#information-extraction" title="Permalink to this headline">¶</a></h3>
<p>We can extract many types of information from a speech signal, like
<span class="xref myst">text content</span> and <span class="xref myst">speaker
identity</span>. Many such forms of
information can be categorized by <em>labels</em>, that is, we give a label to
a particular speech signal. That label can be for example to word which
was pronounced or the speaker identity. Alternatively, such extracted
information can be continuous-valued, such as the age of the speaker or
mood (how glad/angry are you?), but we can treat both types of
information as labels.</p>
<p>Such information extraction methods are today predominantly machine
learning methods. A typical configuration is illustrated on the right,
where the systems is trained off-line with a database of speech and
corresponding labels. Once the system has been trained, it “knows” how
to derive labels from speech input, such that in the actual use
(application) of the model, it can classify input speech to give an
estimate of the label.</p>
<p>In many cases, information extraction can also be implemented as a
signal processing task, where we use prior knowledge of the signal to
device our algorithm. For example, for estimating the <span class="xref myst">fundamental
frequency</span> (pitch) of a speech signal, we can
readily use our knowledge to device efficient algorithms. Such
algorithms are usually an order of magnitude simpler than machine
learning methods, but if the task is complicated, then the accuracy the
output is reduced correspondingly.</p>
<p><img alt="struct2" src="../_images/165138741.png" /></p>
</div>
</div>
<div class="section" id="speech-synthesis">
<h2><span class="section-number">2.6.3. </span>Speech synthesis<a class="headerlink" href="#speech-synthesis" title="Permalink to this headline">¶</a></h2>
<p>When we want to make a computer speak, we need a speech synthesiser,
which takes text as input and outputs speech. It is thus the reverse of
the information extraction -task, in that the roles of speech and labels
(text) have been switched (see figure on the right). As in information
extraction, also here we can also use simpler methods when applicable.
The classical method is concatenative synthesis, where segments of
speech, from a database, are fused together to form continuous
sentences. Such methods are common for example in public announcement
systems (e.g. train stations), where the range of possible announcements
is known in advance.</p>
<p><img alt="struct3" src="../_images/165139247.png" /></p>
</div>
<div class="section" id="user-interfaces">
<h2><span class="section-number">2.6.4. </span>User-interfaces<a class="headerlink" href="#user-interfaces" title="Permalink to this headline">¶</a></h2>
<p>User interfaces can employ speech to accept speech commands and/or
respond with speech. For example,</p>
<ul class="simple">
<li><p>A car navigator can give instructions with speech, while accepting
commands only through the touch (tactile) interface.</p></li>
<li><p>An automatic door can accept speech commands (“Door, open”) but give
no audible feedback.</p></li>
<li><p>Fully speech operated interfaces, like smart speakers, both accept
speech commands and give spoken feedback.</p></li>
</ul>
<p>The unidirectional systems with only speech recognition or only speech
generation are thus subsets of “full” speech operated systems. The stack
of modules of such a complete speech operated system is illustrated on
the right. Here the acoustic front-end (can) contain such pre-processing
methods described in the following section. Natural language
understanding assigns meaning to a sequence of words, dialogue
management maps that to a specific action, implemented by the
actuator(s), and natural language generation refers to the generation of
an answer, in text from.</p>
<p><img alt="struct4" src="../_images/165139542.png" /></p>
</div>
<div class="section" id="processing-and-preprocessing">
<h2><span class="section-number">2.6.5. </span>Processing and preprocessing<a class="headerlink" href="#processing-and-preprocessing" title="Permalink to this headline">¶</a></h2>
<p>Irrespective of application, most systems which operate with speech
signals suffer from similar type so of problems;</p>
<ul class="simple">
<li><p>The main application is expensive to run, so it would be useful to
have a pre-processing unit which detects when it makes sense to
trigger the main application. For example, when nobody is speaking,
it does not make sense to run a speech recognizer.</p></li>
<li><p>Devices are used in real-world environments, where background noises
and room echo are common. It would therefore be useful to clean up
the signal prior to feeding to the main application.</p></li>
</ul>
<p>Such functions typically constitute the acoustic front-end.</p>
<div class="section" id="voice-activity-detection">
<h3><span class="section-number">2.6.5.1. </span>Voice activity detection<a class="headerlink" href="#voice-activity-detection" title="Permalink to this headline">¶</a></h3>
<p>When there is no speech, most speech processing operations are
meaningless. <span class="xref myst">Voice activity detection</span>
refers to the classification of signal segments according to whether
they contain speech or not.</p>
</div>
<div class="section" id="keyword-spotting-or-wake-word-detection">
<h3><span class="section-number">2.6.5.2. </span>Keyword spotting or Wake-word detection<a class="headerlink" href="#keyword-spotting-or-wake-word-detection" title="Permalink to this headline">¶</a></h3>
<p>In most practical situations where voice operated devices are present,
we want to be able to talk with people and not only the device. In other
words, we need to know when the user is speaking to the device and when
not. The device then doesn’t have to try make sense of speech which is
directed to someone else. Wake-word detection (or spotting) refers to a
process which is just waiting for a specific word, which triggers the
main application. For example, smart speakers of the Amazon Alexa brand
wait for the user to say the wake-word “Alexa”, and only after
identifying that word, it starts the main process.</p>
</div>
<div class="section" id="speech-enhancement">
<h3><span class="section-number">2.6.5.3. </span>Speech enhancement<a class="headerlink" href="#speech-enhancement" title="Permalink to this headline">¶</a></h3>
<p>We can try to remove the detrimental effect of background noises and
room echoes with speech enhancement methods. Imagine for example how
difficult it is to understand someone on the phone, when the remote
speaker is standing on a busy street loud cars driving by. With noise
attenuation we can reduce such noises to make speech more pleasant to
listen at the receiving end. Also any other processing becomes simpler
if the speech signal is more clean. Thus a voice user-interface can
feature noise attenuation as pre-processing. Note however that also
other processes such as voice activity detection and wake-word detection
become easier on the clean signal (see alternative 1 in the figure on
the right). However, since noise attenuation can be a computationally
expensive process, it might be better to apply when we already know that
the signal is a speech command (see alternative 2 in the figure on the
right), though this will reduce the accuracy of voice activity and
wake-word detection.</p>
</div>
<div class="section" id="other-signal-processing">
<h3><span class="section-number">2.6.5.4. </span>Other signal processing<a class="headerlink" href="#other-signal-processing" title="Permalink to this headline">¶</a></h3>
<p>Speech signals can be processed further by an array of different
algorithms as desired. For example:</p>
<ul class="simple">
<li><p>Signals can be modified for artistic purposes with tools such as
auto-tune, where the pitch of a singing voice is modified to match a
desired pitch.</p></li>
<li><p>Speech signals can be translated to some other language.</p></li>
<li><p>The identity of a speaker can be hidden (or spoofed) for security
purposes like witness-protection, or for illegal activities like
fraud.</p></li>
</ul>
<p><img alt="struct5" src="../_images/165139583.png" /></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Introduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Linguistic_structure_of_speech.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">2.5. </span>Linguistic structure of speech</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../Representations/Representations.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Basic Representations</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>