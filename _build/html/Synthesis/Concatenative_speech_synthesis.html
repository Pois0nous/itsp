
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9.1. Concatenative speech synthesis &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.2. Statistical parametric speech synthesis" href="Statistical_parametric_speech_synthesis.html" />
    <link rel="prev" title="9. Speech Synthesis" href="../Speech_Synthesis.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Representations/Representations.html">
   3. Basic Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_analysis.html">
     3.1. Short-time analysis of speech and audio signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_processing.html">
     3.2. Short-time processing of speech signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Waveform.html">
     3.3. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Windowing.html">
     3.4. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Signal_energy_loudness_and_decibel.html">
     3.5. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Spectrogram_and_the_STFT.html">
     3.6. Spectrogram and the STFT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Autocorrelation_and_autocovariance.html">
     3.7. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Melcepstrum.html">
     3.8. The cepstrum, mel-cepstrum and mel-frequency cepstral coefficients (MFCCs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Linear_prediction.html">
     3.9. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Fundamental_frequency_F0.html">
     3.10. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Zero-crossing_rate.html">
     3.11. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Deltas_and_Delta-deltas.html">
     3.12. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.13. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Jitter_and_shimmer.html">
     3.14. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Analysis/Measurements_for_medical_applications.html">
     7.2. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
       7.2.1. Inverse filtering for glottal activity estimation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Forensic_analysis.html">
     7.3. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Voice_activity_detection.html">
     8.1. Voice Activity Detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Recognition_and_Verification.html">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Paralinguistic_speech_processing.html">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.1. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Statistical_parametric_speech_synthesis.html">
     9.2. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Basic_tools.html">
     10.2. Basic tools
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.3. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Entropy_coding.html">
       10.3.4. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.3.5. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Code-excited_linear_prediction_CELP.html">
     10.4. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Frequency-domain_coding.html">
     10.5. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://landbot.io/blog/guide-to-conversational-design/">
   Chatbots / Conversational design (external link)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Security_and_privacy_in_speech_technology.html">
   13. Security and privacy in speech technology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../References.html">
   14. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Synthesis/Concatenative_speech_synthesis.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steps-of-concatenative-synthesis">
   9.1.1. Steps of concatenative synthesis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#css-training">
   9.1.2. CSS training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   9.1.3. Further reading
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Concatenative speech synthesis</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steps-of-concatenative-synthesis">
   9.1.1. Steps of concatenative synthesis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#css-training">
   9.1.2. CSS training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   9.1.3. Further reading
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="concatenative-speech-synthesis">
<h1><span class="section-number">9.1. </span>Concatenative speech synthesis<a class="headerlink" href="#concatenative-speech-synthesis" title="Permalink to this headline">¶</a></h1>
<p><em>Concantenative speech synthesis</em> (CSS), also known as <em>unit selection
speech synthesis</em>, is one of the two primary modern speech synthesis
techniques together with <a class="reference internal" href="Statistical_parametric_speech_synthesis.html"><span class="doc std std-doc">statistical parametric speech
synthesis</span></a>. As the name
suggests, CSS is based on concatenation of pre-recorded speech segments
in order to create intelligible high-quality speech. The advantage of
this approach is extremely high naturalness of the produced speech, as
long as the system is well-designed and suitable speech data are
available for its development. The drawback is limited flexibility as
all the used speech segments have to be pre-recorded, limiting the
choice of speaker voices or other modifications to the verbal
expression.</p>
<p>The most simple CSS system imaginable could be developed using
concatenation of pre-recorded word waveforms. However, as <span id="id1">[<a class="reference internal" href="#id15" title="Lawrence R Rabiner and Ronald W Schafer. Introduction to digital speech processing. Foundations and Trends in Signal Processing, 1(1):1–194, 2007. URL: https://doi.org/10.1561/2000000001.">Rabiner and Schafer, 2007</a>]</span> note, such an approach would suffer from two primary
problems. First, concatenation of word-level waveforms would sound
unnatural, as coarticulatory effects between words would be absent from
the data. In addition, the system would be limited to very restrictive
scenarios only, as there can be tens or hundreds of thousands of lexical
items and millions of proper names in any language—way more than what
can be reasonably pre-recorded by any individual speaker. The problem is
even worse for agglutinative languages such as Finnish, where word
meanings are constructed and adjusted by extending word root forms with
various suffixes. Since words can also participate to utterances in
various positions and roles, prosodic characteristics (e.g., F0) of the
same word can differ from context to another. This means that
pre-recording all possible words is not a practical option.</p>
<p>To solve the issues of scalability and coarticulation, practical modern
CSS systems are based on sub-word units. In principle, there are only
few phones per language (e.g., around 40–50 for English), but their
acoustic characteristics are also highly dependent on the surrounding
context due to coarticulation. Context-dependent phones such as diphones
(pairs of phones) or triphones (phone triplets) are therefore utilized.
In order to build a CSS system, speech dataset has to be first carefully
annotated and segmented for the units of interest. These segments can
then be stored as acoustic parameters (e.g., speech codec parameters) to
save space and to allow easy characterization and manipulation.</p>
<div class="section" id="steps-of-concatenative-synthesis">
<h2><span class="section-number">9.1.1. </span>Steps of concatenative synthesis<a class="headerlink" href="#steps-of-concatenative-synthesis" title="Permalink to this headline">¶</a></h2>
<p>Once a database of units exists, synthesis with CSS consists of the
following basic steps: <strong>1)</strong> <em>conversion of input text to a target
specification</em>, which includes the string of phones to be synthesized
together with additional prosodic specifications such as pitch,
duration, and power, <strong>2)</strong> <em>unit selection</em> for each phone segment
according to the specification, and <strong>3)</strong> <em>post-processing</em> to reduce
the impact of potential concatenation artefacts.</p>
<p>While the text processing stage is largely similar to pre-processing in
<a class="reference internal" href="Statistical_parametric_speech_synthesis.html"><span class="doc std std-doc">statistical parametric speech synthesis systems</span></a>, the main part of CSS
is to perform unit selection in such a manner that the output speech
matches the specification with high naturalness of the sound. As
described by <span id="id2">[<a class="reference internal" href="#id14" title="Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, volume 1, 373–376. IEEE, 1996. URL: https://doi.org/10.1109/ICASSP.1996.541110.">Hunt and Black, 1996</a>]</span>, unit selection is achieved by cost
minimization using two cost functions (Fig. 1): <em>target cost</em>
<span class="math notranslate nohighlight">\(C^{t}(u_{i},t_{i})\)</span> and <em>concatenation cost</em>
<span class="math notranslate nohighlight">\(C^{c}(u_{i-1},u_{i})\)</span>. Target cost
describes the mismatch between the target speech unit specification
<span class="math notranslate nohighlight">\(t_{i}\)</span> and a candidate unit <span class="math notranslate nohighlight">\(u_{i}\)</span> from the database.
Concatenation cost describes the mismatch (e.g., acoustic or perceptual)
of the join between the candidate unit <span class="math notranslate nohighlight">\(u_{i}\)</span> and the preceding
unit <span class="math notranslate nohighlight">\(u_{i-1}\)</span>. In other words, an ideal solution
would find all the target units according to the specification without
introducing acoustic mismatches at the edges of concatenated units.</p>
<p><img alt="CSS_cost_schematic" src="../_images/CSS_cost_schematic.png" />
Figure 1: An illustration of the selection cost Ct and concenation cost Cc in diphone-based unit selection for synthesis of word “cat” [k ae t]. In practice, diphones with different initial phones but similar coarticulatory effects on the target phone can be considered in the selection process to overcome the issue of data sparsity. Adapted from <span id="id3">[<a class="reference internal" href="#id14" title="Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, volume 1, 373–376. IEEE, 1996. URL: https://doi.org/10.1109/ICASSP.1996.541110.">Hunt and Black, 1996</a>]</span>.</p>
<p>Since the target specification consists of many characteristics such as
target and context phone(s) identity, pitch, duration, and power, the
target cost can be divided into multiple subcosts <span class="math notranslate nohighlight">\(j\)</span> as
<span class="math notranslate nohighlight">\( C_{j}^{t}(t_{i},u_{i}) \)</span> . For instance, the contextual
phone(s) (e.g., [ae] in the last unit [ae t] of “cat” in Fig. 1) can
be represented by a number of features describing the manner and place
of articulation, so that the specification can be compared to different
candidate units in the database <span id="id4">[<a class="reference internal" href="#id14" title="Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, volume 1, 373–376. IEEE, 1996. URL: https://doi.org/10.1109/ICASSP.1996.541110.">Hunt and Black, 1996</a>]</span>. This enables the
use of different context phones with similar coarticulatory effects on
the target phone. Similarly, costs for segment power and pitch can be
measured, e.g., in terms of the differences in mean log-power and mean
F0. Cost for the target phone ([t] in [ae t] of Fig. 1) is usually a
binary indicator that forces the phonemic identity of the chosen unit to
match with that of the target specification. The total cost can then be
written as</p>
<div class="math notranslate nohighlight">
\[
C^{t}(t_i,u_i)=\sum_{j=1}^{P}w_{j}^{t}C_{j}^{t}(t_{j},u_{i}) \]</div>
<p>(1)</p>
<p>where <span class="math notranslate nohighlight">\(w^{t} = [w^{t}_{1},
w^{t}_{2}, ..., w^{t}_{P}]\)</span> are the
relative weights of each subcost.</p>
<p>Concatenation cost <span class="math notranslate nohighlight">\(C^{c}(u_{i-1},u_{i})\)</span>
can be derived in a similar manner to Eq. (1) by decomposing the the
total cost to <span class="math notranslate nohighlight">\(Q\)</span> subcosts, and then calculating a weighted sum of the
subcosts:</p>
<div class="math notranslate nohighlight">
\[
C^{c}(u_{i-1},u_i)=\sum_{j=1}^{Q}w_{j}^{c}C_{j}^{c}(u_{i-1},u_{i})
\]</div>
<p>(2)</p>
<p>Note that the subcosts and their weights <span class="math notranslate nohighlight">\(w^{c}\)</span> for
<span class="math notranslate nohighlight">\(C^{c}\)</span> do not need match those of <span class="math notranslate nohighlight">\(C^{t}\)</span>, as the
concatenation cost specifically focuses on the acoustic compatibility of
the subsequent units. Therefore
subcosts  <span class="math notranslate nohighlight">\(C^{c}_{j}(u_{i-1},u_{i})\)</span>
associated with continuity of the spectrum (or cepstrum), segment power,
and pitch in the segment and/or at the concenation point should be
considered.</p>
<p>The total cost of the selection process is the sum of the target and
concenation costs across all <span class="math notranslate nohighlight">\(n\)</span> units:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
                  \begin{split}
C(t_{1}^{n},u_{1}^{n})&amp;=\sum_{i=1}^{n}C^t(t_i,u_i)+\sum_{i=2}^{n}C^c(u_{i-1},u_i)+C^c(\#,u_1)+C^c(u_n,\#)
\\&amp;
                   =
\sum_{i=1}^{n}\sum_{j=1}^{P}w_{j}^{t}C_{j}^{t}(t_{i},u_{i})+\sum_{i=2}^{n}\sum_{j=1}^{Q}w_{j}^{c}C_{j}^{c}(u_{i-1},u_{i})+C^c(\#,u_1)+C^c(u_n,\#)
\end{split}
\end{split}\]</div>
<p>(3)</p>
<p>where <span class="math notranslate nohighlight">\( t_{1}^{n} \)</span> are the targets,  <span class="math notranslate nohighlight">\( u_{1}^{n} \)</span> are the
selected units, and # denotes silence <span id="id5">[<a class="reference internal" href="#id14" title="Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, volume 1, 373–376. IEEE, 1996. URL: https://doi.org/10.1109/ICASSP.1996.541110.">Hunt and Black, 1996</a>]</span>. The two
extra terms stand for transition from preceding silence to the utterance
and from utterance to the trailing silence. The aim of the selection
process is then to find units <span class="math notranslate nohighlight">\( \bar{u}_{1}^{n} \)</span> that minimize
the total cost in Eq. (3). The selection process can be represented as a
fully connected trellis, as shown in Fig. 3, where each edge to a node
has a basic cost of the given node to be chosen (the target cost) and an
additional cost depending on the previous unit (the concatenation cost).
Given the trellis, the optimal selection can be carried out with
<em><a class="reference external" href="https://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi search</a></em>—a
dynamic programming algorithm that calculates the least cost path
through the trellis. To make the search computationally feasible for
large databases, less likely candidates for each target can be pruned
from the trellis. In addition, <a class="reference external" href="https://en.wikipedia.org/wiki/Beam_search"><em>beam
search</em></a> with only a fixed
number of most likely nodes for each step can be applied for further
speedup.</p>
<p><img alt="CSS_search_trellis.png" src="../_images/180303620.png" /></p>
<p><strong>Figure 3:</strong> An example of unit selection search trellis for word
“<em>cat</em>” [<em>k ae t</em>]. Each edge is associated with basic target cost of
the selected unit and concatenation cost dependent on the previous unit.
Adapted from <span id="id6">[<a class="reference internal" href="#id15" title="Lawrence R Rabiner and Ronald W Schafer. Introduction to digital speech processing. Foundations and Trends in Signal Processing, 1(1):1–194, 2007. URL: https://doi.org/10.1561/2000000001.">Rabiner and Schafer, 2007</a>]</span></p>
<p>After the speech units have been concatenated to form the intended
utterance, postprocessing techniques can be used to smooth the potential
discontinuities in F0, energy and spectrum at the unit boundaries. Note
that aggressive signal processing based modification of the segments
also often tends to decrease the naturalness of the sound.
Straightforward modification of the segments’ acoustic parameters (e.g.
with vocoding) is not therefore a recommended strategy to overcome the
issues of poor unit selection or low quality source data.</p>
</div>
<div class="section" id="css-training">
<h2><span class="section-number">9.1.2. </span>CSS training<a class="headerlink" href="#css-training" title="Permalink to this headline">¶</a></h2>
<p>The above formulation enables mathematically principled and optimal unit
selection process from a given speech database. However, the synthesis
output is highly dependent on the choice of features and functions used
for the subcosts, and also on the weights chosen for each feature. While
the cost functions and their underlying features can be largely designed
based on knowledge in signal processing and speech processing, the
weights need to be either adjusted through trial and error, or they can
be automatically optimized using some kind of quality criterion.</p>
<p>As examples of automatic weight estimation, <span id="id7">[<a class="reference internal" href="#id14" title="Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, volume 1, 373–376. IEEE, 1996. URL: https://doi.org/10.1109/ICASSP.1996.541110.">Hunt and Black, 1996</a>]</span> propose
two alternative ways to automatically acquire cost function weights:</p>
<p>1) Using a <a class="reference external" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search">grid
search</a>
across different weight values by synthesizing utterances using the
target specifications of held-out utterances from the training database,
and then comparing the synthesized waveform to the real waveform of the
held-out utterance using an objective metric. The weights that lead to
the best overall performance are then chosen.</p>
<p>2) Using regression models to predict best values for the weight. <span id="id8">[<a class="reference internal" href="#id14" title="Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, volume 1, 373–376. IEEE, 1996. URL: https://doi.org/10.1109/ICASSP.1996.541110.">Hunt and Black, 1996</a>]</span> report that cepstral distance and power difference
across the concanation point can be used as predictors for reported
perceptual quality of the concatenation in a linear regression model,
and therefore the linear regression weights can be directly used as
perceptually motivated the cost weights.  For the target weights, they
propose and approach where each unit in the database is considered as
the target specification at a time, and <span class="math notranslate nohighlight">\(K\)</span> best matching other units
are then searched for the target using an objective distance measure.
Then the sub-costs between the target and the <span class="math notranslate nohighlight">\(K\)</span> matches are calculated
and recorded. This process is repeated for all exemplars of the same
phonetic unit in the database, recording the <span class="math notranslate nohighlight">\(K\times Q\)</span> subcosts and the
related <span class="math notranslate nohighlight">\(K\)</span> distances for each exemplar. Linear regression is then
applied to predict the recorded objective perceptual distances using the
associated sub-costs, linear regression coefficients again revealing the
optimal weights for each of the subcosts. A specific advantage of the
regression approach for subcost weight estimation is that it allows
estimation of phoneme-specific weights for each subcost, as the
perceptually critical cues may differ from a phonetic context to
another.</p>
</div>
<div class="section" id="further-reading">
<h2><span class="section-number">9.1.3. </span>Further reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h2>
<div class="docutils container" id="id9">
<dl class="citation">
<dt class="label" id="id14"><span class="brackets">HB96</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id3">2</a>,<a href="#id4">3</a>,<a href="#id5">4</a>,<a href="#id7">5</a>,<a href="#id8">6</a>)</span></dt>
<dd><p>Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In <em>1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings</em>, volume 1, 373–376. IEEE, 1996. URL: <a class="reference external" href="https://doi.org/10.1109/ICASSP.1996.541110">https://doi.org/10.1109/ICASSP.1996.541110</a>.</p>
</dd>
<dt class="label" id="id15"><span class="brackets">RS07</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Lawrence R Rabiner and Ronald W Schafer. Introduction to digital speech processing. <em>Foundations and Trends in Signal Processing</em>, 1(1):1–194, 2007. URL: <a class="reference external" href="https://doi.org/10.1561/2000000001">https://doi.org/10.1561/2000000001</a>.</p>
</dd>
</dl>
</div>
<!--
![CSS_cost_schematic.png](attachments/180303393.png)
![CSS_cost_schematic.png](attachments/180303625.png)
![CSS_search_trellis.png](attachments/180303624.png)
-->
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Synthesis"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../Speech_Synthesis.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">9. </span>Speech Synthesis</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Statistical_parametric_speech_synthesis.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9.2. </span>Statistical parametric speech synthesis</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>