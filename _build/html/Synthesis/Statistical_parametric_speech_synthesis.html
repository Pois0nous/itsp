
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9.2. Statistical parametric speech synthesis &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10. Transmission, storage and telecommunication" href="../Transmission_storage_and_telecommunication.html" />
    <link rel="prev" title="9.1. Concatenative speech synthesis" href="Concatenative_speech_synthesis.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Representations/Representations.html">
   3. Basic Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_analysis.html">
     3.1. Short-time analysis of speech and audio signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_processing.html">
     3.2. Short-time processing of speech signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Waveform.html">
     3.3. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Windowing.html">
     3.4. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Signal_energy_loudness_and_decibel.html">
     3.5. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Spectrogram_and_the_STFT.html">
     3.6. Spectrogram and the STFT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Autocorrelation_and_autocovariance.html">
     3.7. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Melcepstrum.html">
     3.8. The cepstrum, mel-cepstrum and mel-frequency cepstral coefficients (MFCCs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Linear_prediction.html">
     3.9. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Fundamental_frequency_F0.html">
     3.10. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Zero-crossing_rate.html">
     3.11. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Deltas_and_Delta-deltas.html">
     3.12. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.13. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Jitter_and_shimmer.html">
     3.14. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Analysis/Measurements_for_medical_applications.html">
     7.2. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
       7.2.1. Inverse filtering for glottal activity estimation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Forensic_analysis.html">
     7.3. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Voice_activity_detection.html">
     8.1. Voice Activity Detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Recognition_and_Verification.html">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Recognition/Paralinguistic_speech_processing.html">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="Concatenative_speech_synthesis.html">
     9.1. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.2. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.2. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Entropy_coding.html">
       10.2.5. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.2.6. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Code-excited_linear_prediction_CELP.html">
     10.3. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Frequency-domain_coding.html">
     10.4. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Security_and_privacy.html">
   13. Security and privacy in speech technology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../References.html">
   14. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Synthesis/Statistical_parametric_speech_synthesis.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-generation">
   9.2.1. Feature generation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#waveform-generation-with-vocoders">
   9.2.2. Waveform generation with vocoders
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#spss-system-training">
   9.2.3. SPSS system training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#advantages-and-disadvantages-of-the-hmm-gmm-spss-compared-to-concatenative-synthesis">
   9.2.4. Advantages and disadvantages of the HMM-GMM SPSS compared to concatenative synthesis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-spss">
   9.2.5. Neural SPSS
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   9.2.6. Further reading
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Statistical parametric speech synthesis</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-generation">
   9.2.1. Feature generation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#waveform-generation-with-vocoders">
   9.2.2. Waveform generation with vocoders
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#spss-system-training">
   9.2.3. SPSS system training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#advantages-and-disadvantages-of-the-hmm-gmm-spss-compared-to-concatenative-synthesis">
   9.2.4. Advantages and disadvantages of the HMM-GMM SPSS compared to concatenative synthesis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-spss">
   9.2.5. Neural SPSS
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   9.2.6. Further reading
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="statistical-parametric-speech-synthesis">
<h1><span class="section-number">9.2. </span>Statistical parametric speech synthesis<a class="headerlink" href="#statistical-parametric-speech-synthesis" title="Permalink to this headline">¶</a></h1>
<p>While concatenative synthesis can reach highly natural synthesized
speech, the approach is inherently limited by properties of the speech
corpus used for the unit selection process. Concatenative systems can
only produce speech whose constituent segments (e.g., diphones) have
been pre-recorded. In order to make the synthesis sound natural, large
amounts of speech from a single speaker must therefore be available.
This limits the flexiblity of concatenative systems in producing
different voices, speaking styles, emotional expressions, or other
modifications to the sound that are common in everyday human
communication.</p>
<p>As an alternative to the concatenative approach, statistical parametric
speech synthesis (SPSS) is another TTS approach that has become highly
popular in the speech technology field. This is because it addresses the
main limitation of the concatenative systems — the lack of flexibility —
by generating the speech using statistical models of speech instead of
relying on pre-recorded segments. These statistical models are learned
from speech corpora using machine learning techniques, and they encode
information of how speech evolves as a function of time in the context
of a given input text.  In this respect, SPSS systems can be viewed as a
mirror image of <span class="xref myst">ASR</span> systems: while an ASR system
tries to convert speech from acoustic features to a string of words
using machine learning models, an SPSS system tries to convert a string
of words into acoustic features or directly to the acoustic waveform
using machine learning models. Both ASR and SPSS systems are typically
trained on a large amount of speech data with their transcriptions,
resulting in a set of <em>parameters</em> that describe <em>statistical
characteristics</em> of the speech data (hence “statistical parametric”
speech synthesis).</p>
<p><img alt="SPSS_basic_pipeline.png" src="../_images/175517696.png" />
<strong>Figure 1:</strong> A schematic view of an SPSS system.</p>
<p>A full SPSS system consists of text analysis, feature generation, and
waveform generation modules. The classical approach to SPSS is based on
a combination of a <em>hidden-Markov model Gaussian mixture model</em>
(HMM-GMM) architecture for feature generation and a <em>vocoder</em> for
waveform generation, and these will be discussed in more detail below.
Recent advances in neural network-based SPSS are then reviewed at the
end.</p>
<div class="section" id="feature-generation">
<h2><span class="section-number">9.2.1. </span>Feature generation<a class="headerlink" href="#feature-generation" title="Permalink to this headline">¶</a></h2>
<p>Given the linguistic description of the text-to-be-synthesized, the
purpose of the feature generation is to transform the linguistic
features into a corresponding description of the acoustic signal.
Similarly to ASR, this component mediating the two levels is called <em>an
acoustic model</em>. Technically speaking, the acoustic model converts the
linguistic input into a series of acoustic features at a fixed
frame-rate (e.g., one feature frame every 10 ms) using a probabilistic
mapping between the two. The mapping is learned from a training speech
corpus.</p>
<p>A standard approach for the probabilistic mapping has been to use a
HMM-GMM as the statistical parametric model. Similarly to an HMM-GMM ASR
system, the states <span class="math notranslate nohighlight">\(s\)</span> of the HMM correspond to parts of subword units
(e.g., parts of a phone, diphone, or triphone). Transition probabilities
P(s | st-1) between the states describe how the speech evolves through
each subword unit and from a unit to another. Acoustic characteristics
associated with each state are modeled with a GMM, where the GMM
describes a probability distribution <span class="math notranslate nohighlight">\(P(y | s)\)</span> over the possible
acoustic feature vectors in that state. Given a sequence of desired
subword units (as instructed by the linguistic features), the model can
be stochastically or deterministically sampled to produce a sequence of
acoustic features. These are then fed to <em>a waveform generation module</em>
to produce the actual speech signal. In the most basic form,
self-transitions from an HMM state to itself account for the duration
spent in that state (i.e., how many frames should the same acoustic
content be repeated). However, separate more advanced duration models
are often used to overcome the limitations of a first-order Markov chain
in modeling thetemporal dependencies and durational characteristics of
speech.</p>
<p><img alt="synthesis_HMM_GMM.png" src="../_images/175518368.png" /></p>
<p><strong>Figure 2:</strong> A visual illustration of HMM-GMM-based speech feature
generation. State sequence <span class="math notranslate nohighlight">\(s = \{s_1, s_2,...,s_{10}\}\)</span> required for word “cat” (/k ae t/) is shown on
top, where each phoneme consists of three states: initial, center and
final state (e.g., <span class="math notranslate nohighlight">\(k_{1}\)</span>, <span class="math notranslate nohighlight">\(k_{2}\)</span>, and <span class="math notranslate nohighlight">\(k_{3}\)</span>). 
Each state is associated with an <span class="math notranslate nohighlight">\(N\)</span>-dimensional Gaussian mixture model
(GMM), where <span class="math notranslate nohighlight">\(N\)</span> is the dimensionality of the speech features <span class="math notranslate nohighlight">\(y\)</span>. At
each time step, the GMM of the active state is sampled for a feature
vector <span class="math notranslate nohighlight">\(y_{t}\)</span>.  After this, a state transition can occur to
a next state or back to the current state, controlling the durational
aspects of the speech.</p>
</div>
<div class="section" id="waveform-generation-with-vocoders">
<h2><span class="section-number">9.2.2. </span>Waveform generation with vocoders<a class="headerlink" href="#waveform-generation-with-vocoders" title="Permalink to this headline">¶</a></h2>
<p>A typical high-quality speech waveform consists of “continuous” (e.g.,
16-bit quantized) amplitude values sampled at 16 kHz. In addition, the
shape of the waveform is affected by several factors that do not
directly contribute to the naturalness or intelligibility of speech,
such as signal gain or phase and amplitude characteristics of the
recording and transmission chain. This means that mere 80 milliseconds
of a raw waveform — a typical length of one vowel — would correspond to
0.08 s\16 kHz = 1280-dimensional amplitude vector, and that this
vector could take countless of shapes for perceptually highly similar
sounds. Moreover, the values encoded in this vector would be highly
correlated with each other (see <span class="xref myst">LPC</span>). Given the
high dimensionality, variability, and redundant nature of the waveform
signal representation, it is not an attractive target for statistical
parametric modeling with classical machine learning techniques (but see
also Neural SPSS below).</p>
<p>However, as we remember from speech feature extraction (see, e.g.,
<span class="xref myst">SFFT</span>), speech signal can be considered as
quasi-stationary in short windows of approx. 10–30 ms in duration.
Speech contents of the signal within these short windows can be
described using a set of spectral and source features (such as
<span class="xref myst">MFCCs</span> and <span class="xref myst">F0</span>) that are
assumed to be fixed for that window. When extracting the features in a
sliding window with short (e.g., 10 ms) window steps, the overall
structure of the signal can be captured with a much lower dimensional
and less variable representation than what the actual waveform would
be.  <strong>A vocoder, then, is an algorithm that can 1) parametrize a
speech waveform into a more compact set of descriptive features as a
function of time, but also to 2) synthesize the speech back from the
features with minimal loss in speech quality</strong>. In addition, many
vocoders use features that are interpretable in terms of speech
production or speech acoustics, enabling analysis and manipulation of
the speech signal to observe or cause certain phenomena in the speech
signal.</p>
<p>Compactness and invariance of the acoustic signal representation is also
why vocoding is used in SPSS systems:  instead of generating the speech
waveform directly, the feature generation module first generates a
lower-dimensional set of vocoder features that characterize the speech
signal with its essential properties. A vocoder then takes these
features as input and generates the corresponding waveform using a
series of signal processing operations. These operations are essentially
an inverse of the original feature extraction process, combined with
some additional mechanisms for re-introducing (or inventing) information
lost during the feature extraction process (such as signal phase that is
discarded from standard spectral features).</p>
<p>For instance, when using the popular STRAIGHT vocoder (Kawahara et al.,
1999), the HMM-GMM model first generates a sequence of feature vectors
that encode spectral envelope, F0, and periodicity characteristics of
the speech signal to-be-produced, as instructed by the text analysis
module. These features are then fed to STRAIGHT that synthesizes the
final speech waveform based on the features.</p>
<p><img alt="vocoder_basic_structure" src="../_images/175517700.png" />
<strong>Figure 3:</strong>
A schematic view of a vocoder and typical uses for vocoder features.<br />
When used as a part of an SPSS system, vocoder features are generated by
the parametric statistical model during the synthesis process.</p>
</div>
<div class="section" id="spss-system-training">
<h2><span class="section-number">9.2.3. </span>SPSS system training<a class="headerlink" href="#spss-system-training" title="Permalink to this headline">¶</a></h2>
<p>Training of an SPSS system refers to estimation of the parametric
acoustic model (e.g., a HMM-GMM) that is responsible for mapping the
linguistic features to the corresponding waveform generation (vocoder)
features. This is achieved using a corpus of speech data, where each
utterance comes with the corresponding text of what was said, and
optionally with phonetic annotation describing the phonetic units and
their temporal positions in the waveform. First, the text analysis
module is used to create linguistic features of a training utterance
while the vocoder is used to extract vocoder features from the
corresponding speech waveform. Then the statistical model is trained to
minimize prediction error of the given vocoder features when the
linguistic features are used as inputs. Access to phonetic annotation
allows more accurate temporal alignment between the linguistic features
and the speech signal. Since the widely utilized HMM architecture for
acoustic modeling is not ideal for modeling speech segment durations, a
separate <em>duration model</em> is often trained to align the linguistic
features (which are agnostic of speaking rate and rhythm in the actual
speech data) with the phonetic units realized in the acoustic speech
signal. In the figure below, both the acoustic model and the duration
model are denoted jointly by the parametric statistical model block.</p>
<p><img alt="SPSS_training_pipeline.png" src="../_images/175517698.png" />
<strong>Figure 4:</strong> A schematic view of SPSS system training.</p>
</div>
<div class="section" id="advantages-and-disadvantages-of-the-hmm-gmm-spss-compared-to-concatenative-synthesis">
<h2><span class="section-number">9.2.4. </span>Advantages and disadvantages of the HMM-GMM SPSS compared to concatenative synthesis<a class="headerlink" href="#advantages-and-disadvantages-of-the-hmm-gmm-spss-compared-to-concatenative-synthesis" title="Permalink to this headline">¶</a></h2>
<p>Since the “instructions” for speech generation are encoded by parameters
of the SPSS model, the model can easily be adapted to produce speech
with different characteristics. For instance, the vocal tract
characteristics of the training speaker are encoded by the means and
variances of the Gaussian distributions in each HMM state whereas
durational characteristics are encoded by the transition probabilty
matrix of the HMM. Therefore, the system can be adapted to other
speakers by simply adapting the pre-trained HMM-GMM using speech from a
new talkers. In this case, standard techniques such as
Maximum-a-posteriori (MAP) adaptation or Maximum-likelihood linear
regression (MLLR) can be used to update the model parameters. In
addition, since the parameters of the HMM-GMM are often interpretable in
terms of speech spectral envelope or phonation characteristics, it is
possible to either modify the models or to post-process the resulting
acoustic features in order to achieve desired effects. For example,
changing of the speech pitch can be done by simply adjusting the F0
parameter, whereas reduction of some synthesis artifacts such as muffled
sound quality due to statistical averaging can be attempted by adjusting
the GMM parameters with a chosen transformation.</p>
<p>The potential disadvantages of the statistical approach include sound
quality issues (e.g., muffledness) due to statistical smoothing taking
place in a stochastic generative model, sound quality of the used
vocoders, and potential problems in robust statistical model estimation
from finite data.</p>
</div>
<div class="section" id="neural-spss">
<h2><span class="section-number">9.2.5. </span>Neural SPSS<a class="headerlink" href="#neural-spss" title="Permalink to this headline">¶</a></h2>
<p>Recent advances in artificial neural networks (ANNs) have also led to
new developments in SPSS beyond the classical HMM-GMM framework. In
terms of vocoding, WaveNet (van den Oord et al., 2016) is a highly
influential neural network waveform generator that can produce
high-quality speech. It is based on an autoregressive convolutional
neural network (CNN) architecture and it operates directly on the speech
waveforms.  Given a history of previous waveform samples and some
“conditioning” information on what type of signal should be produced,
the model predicts the most likely next speech waveform sample at each
time step. For instance, WaveNet can be trained to produce speech from
spectral features such as log-Mel energies and F0 information. Although
WaveNet can reach near-human naturalness of the produced speech (with
certain limitations), waveform-level autoregressive processing is also
computationally extremely expensive. Development of computationally
flexible high-quality neural vocoders is therefore still an active
research area.</p>
<p>In addition to vocoding, neural networks have become commonplace
replacements for the HMM-GMM in the feature generation stage. For
instance, deep feed-forward networks or LSTMs can be utilized in the
feature generation. Since LSTMs are especially good at modeling temporal
dependencies, they can theoretically handle larger temporal ambiguity
and variability between the input linguistic specifications and the
target vocoder features.</p>
<p>Given sufficient training data, it is also possible to implement the
entire chain from written text to the synthesized waveform using a
neural network system. Tacotron 2 is an example of such a system, where
the input text is processed by a sequence-to-sequence ANN model to
directly create a log-Mel spectrogram corresponding to the input text
(i.e., without a dedicated text analysis module). The spectrogram is
then fed to the WaveNet module (see above) to produce the speech signal.
As a result, Tacotron 2 and the Wavenet vocoder can together achieve
highly impressive speech quality. The advantage of these type end-to-end
approaches is that there are fewer assumptions regarding what kind of
intermediate representations are good for the task at hand, reducing the
risk that the pre-specified operations and representations cause a loss
of relevant information in the pipeline. There is also no need for deep
understanding of the linguistic structure underlying written and spoken
language or access to pre-existing text analysis tools, making
deployment of the systems possible for any language with sufficient
training data (text and corresponding speech). Since all the components
are based on differentiable neural network operations, it is also
possible to jointly optimize the entire chain from waveform generation
to text processing. In principle, neural SPSS systems are also highly
flexible, as basically any type of side information can be injected to
the system to adjust the characteristics of the produced speech.</p>
<p>Neural systems, however, also have their drawbacks. The amount of data
and computational resources required to train these systems can be high.
Runtime computational requirements of neural vocoders may also be
problematic in some applications, although recent advances in vocoders
and in parallelization of the computations has already led to
significant advances in this respect. Another issue is the lack of
interpretability and transparency of model parameters: while parameters
of classical models such as HMMs and GMMs have relatively clear
relationship with what are the inputs and outputs of the system, the
same is not true for ANNs with multiple layers. This makes it much more
difficult to understand the behavior of the model, especially when
trying to overcome problems in model performance. Lack of transparency
and interpretability also means that manual control of characteristics
of the produced speech is more difficult. Finally, adaptation of the
models to new data (e.g., a new speaker or speaking style) cannot make
use of the well-understood mathematical solutions available to the
classical models. In contrast, the design, training, and adaptation of
ANNs are much more heuristics-driven, similarly to the use of ANNs in
any other machine learning domain.</p>
</div>
<div class="section" id="further-reading">
<h2><span class="section-number">9.2.6. </span>Further reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h2>
<p>Kawahara, K., Masuda-Katsuse, I., and de Cheveigné , A. (1999).
Restructuring speech representations using a pitch-adaptive
time–frequency smoothing and an instantaneous-frequency-based F0
extraction: Possible role of a repetitive structure in sounds, Speech
Communication, 27, 187–207. (STRAIGHT vocoder)</p>
<p>Yamagishi, J. (2006). An introduction to HMM-based speech synthesis. 
<a class="reference external" href="https://wiki.inf.ed.ac.uk/pub/CSTR/TrajectoryModelling/HTS-Introduction.pdf">https://wiki.inf.ed.ac.uk/pub/CSTR/TrajectoryModelling/HTS-Introduction.pdf</a>  
(introduction to HMM-based SPSS)</p>
<p>Shen, J. et al. (2017). Natural TTS synthesis by conditioning WaveNet on
Mel spectrogram predictions. ArXiV pre-print:
<a class="reference external" href="https://arxiv.org/abs/1712.05884">https://arxiv.org/abs/1712.05884</a>   (Tacotron 2)</p>
<p>Tokuda, K., Nankaku, Y., Toda, T., Zen, H., Yamagishi, Y., and Oura, K.
(2013). Speech synthesis based on hidden Markov models. <em>Proceedings of
the IEEE</em>, 101, 1234–1252. (introduction to SPSS)*<br />
*</p>
<p>van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O.,
Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. (2016).
<em>WaveNet: A generative model for raw audio.</em> * ArXiV pre-print:
<a class="reference external" href="https://arxiv.org/pdf/1609.03499.pdf">https://arxiv.org/pdf/1609.03499.pdf</a>  (WaveNet original paper)*<br />
*</p>
<p>Wu, Z., Watts, O., and King, S. (2016). Merlin: An Open Source Neural
Network Speech Synthesis System. <em>In Proc. 9th ISCA Speech Synthesis
Workshop (SSW9)</em>, September 2016, Sunnyvale, CA,
USA  <a class="reference external" href="https://github.com/CSTR-Edinburgh/merlin">https://github.com/CSTR-Edinburgh/merlin</a> (Merlin toolkit for
synthesis)*<br />
*</p>
<p>Zen, H., Tokuda, K., and Black, A. W. (2009). Statistical parametric
speech synthesis. <strong>Speech Communication</strong>, 51, 1039–1064.
<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0167639309000648">https://www.sciencedirect.com/science/article/pii/S0167639309000648</a>
(introduction to SPSS)*<br />
*</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Synthesis"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Concatenative_speech_synthesis.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">9.1. </span>Concatenative speech synthesis</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../Transmission_storage_and_telecommunication.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Transmission, storage and telecommunication</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>