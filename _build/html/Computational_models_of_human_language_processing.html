
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>12. Computational models of human language processing &#8212; Introduction to Speech Processing</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="13. Security and privacy in speech technology" href="Security_and_privacy.html" />
    <link rel="prev" title="11.4. Multi-channel speech enhancement and beamforming" href="Enhancement/Multi-channel_speech_enhancement_and_beamforming.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Representations/Representations.html">
   3. Basic Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Short-time_analysis.html">
     3.1. Short-time analysis of speech and audio signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Short-time_processing.html">
     3.2. Short-time processing of speech signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Waveform.html">
     3.3. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Windowing.html">
     3.4. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Signal_energy_loudness_and_decibel.html">
     3.5. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Spectrogram_and_the_STFT.html">
     3.6. Spectrogram and the STFT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Autocorrelation_and_autocovariance.html">
     3.7. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Melcepstrum.html">
     3.8. The cepstrum, mel-cepstrum and mel-frequency cepstral coefficients (MFCCs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Linear_prediction.html">
     3.9. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Fundamental_frequency_F0.html">
     3.10. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Zero-crossing_rate.html">
     3.11. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Deltas_and_Delta-deltas.html">
     3.12. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.13. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Representations/Jitter_and_shimmer.html">
     3.14. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="Analysis/Measurements_for_medical_applications.html">
     7.2. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
       7.2.1. Inverse filtering for glottal activity estimation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Analysis/Forensic_analysis.html">
     7.3. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Voice_activity_detection.html">
     8.1. Voice Activity Detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Speaker_Recognition_and_Verification.html">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Recognition/Paralinguistic_speech_processing.html">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Synthesis/Concatenative_speech_synthesis.html">
     9.1. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Synthesis/Statistical_parametric_speech_synthesis.html">
     9.2. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.2. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="Transmission/Entropy_coding.html">
       10.2.5. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.2.6. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Transmission/Code-excited_linear_prediction_CELP.html">
     10.3. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Transmission/Frequency-domain_coding.html">
     10.4. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Security_and_privacy.html">
   13. Security and privacy in speech technology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="References.html">
   14. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Computational_models_of_human_language_processing.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#human-cognition-as-a-sensorimotor-information-processing-system">
   12.1. Human cognition as a sensorimotor information processing system
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-modeling-versus-cognitive-computationalism">
     12.1.1. Computational modeling versus cognitive computationalism
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#role-of-computational-models-in-scientific-research">
   12.2. Role of computational models in scientific research
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examples-of-computational-modeling-research">
   12.3. Examples of computational modeling research
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references-and-further-reading">
   12.4. References and further reading
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Computational models of human language processing</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#human-cognition-as-a-sensorimotor-information-processing-system">
   12.1. Human cognition as a sensorimotor information processing system
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-modeling-versus-cognitive-computationalism">
     12.1.1. Computational modeling versus cognitive computationalism
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#role-of-computational-models-in-scientific-research">
   12.2. Role of computational models in scientific research
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examples-of-computational-modeling-research">
   12.3. Examples of computational modeling research
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references-and-further-reading">
   12.4. References and further reading
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="computational-models-of-human-language-processing">
<h1><span class="section-number">12. </span>Computational models of human language processing<a class="headerlink" href="#computational-models-of-human-language-processing" title="Permalink to this headline">¶</a></h1>
<p>One area of research making use of speech technology is the study of
human language learning and processing. Language is a highly complex
phenomenon with physical, biological, psychological, social and cultural
dimensions. Therefore it is also studied across several disciplines,
such as linguistics, neuroscience, psychology, and anthropology. While
many of these fields primarily focus on empirical and theoretical work
on language, computational models and simulations provide another
important aspect to the research: capability to test theoretical models
in practice. Implementation of models capable of processing real speech
data requires techniques from speech processing and machine learning.
For instance, techniques for speech
<a class="reference internal" href="Representations/Representations.html"><span class="doc std std-doc">signal representation</span></a>
and
<a class="reference internal" href="Pre-processing.html"><span class="doc std std-doc">pre-processing</span></a> are needed to interface the models with
acoustic speech recordings. Different types of <a class="reference internal" href="Modelling_tools_in_speech_processing.html"><span class="doc std std-doc">classifiers and machine
learning algorithms</span></a> are needed to
implement learning mechanisms in the models or to analyze behavior of
the developed models. In addition, model training data may be generated
with <a class="reference internal" href="Speech_Synthesis.html"><span class="doc std std-doc">speech synthesizers</span></a> (e.g., <span id="id1">[<a class="reference internal" href="References.html#id36" title="William Havard, Laurent Besacier, and Olivier Rosec. Speech-coco: 600k visually grounded spoken captions aligned to mscoco data set. arXiv preprint arXiv:1707.08435, 2017. URL: https://doi.org/10.21437/GLU.2017-9.">Havard <em>et al.</em>, 2017</a>]</span>), whereas linguistic reference data for model evaluation may
be extracted from speech recordings using <a class="reference internal" href="Recognition/Speech_Recognition.html"><span class="doc std std-doc">automatic speech
recognition</span></a>.</p>
<p>The basic idea of computational modeling is to understand how humans
learn and process language by implementing human-like learning and
speech processing capabilities as computational algorithms. The models
are then exposed to inputs similar to what humans observe, and the model
behavior is then recorded and compared to human data (Fig. 1).
Computational models can focus on questions such as how adult speech
perception operates (e.g., the highly-influential TRACE model of speech
perception; <span id="id2">McClelland and Elman [<a class="reference internal" href="References.html#id29" title="James L McClelland and Jeffrey L Elman. The trace model of speech perception. Cognitive psychology, 18(1):1–86, 1986. URL: https://doi.org/10.1016/0010-0285(86)90015-0.">1986</a>]</span>), how language learning takes place
in young children (native language aka. L1 learners; e.g., <span id="id3">[<a class="reference internal" href="References.html#id37" title="Emmanuel Dupoux. Cognitive science in the era of artificial intelligence: a roadmap for reverse-engineering the infant language-learner. Cognition, 173:43–59, 2018. URL: https://doi.org/10.1016/j.cognition.2017.11.008.">Dupoux, 2018</a>, <a class="reference internal" href="References.html#id23" title="Okko Räsänen. Computational modeling of phonetic and lexical learning in early language acquisition: existing models and future directions. Speech Communication, 54(9):975–997, 2012. URL: https://doi.org/10.1016/j.specom.2012.05.001.">Räsänen, 2012</a>]</span>) or in second-language (L2) learners, or they may
study the emergence and evolution of language through communicative
coordination between multiple agents (see, e.g.,  <span id="id4">Kirby [<a class="reference internal" href="References.html#id32" title="Simon Kirby. Natural language from artificial life. Artificial life, 8(2):185–215, 2002. URL: https://doi.org/10.1162/106454602320184248.">2002</a>], Steels [<a class="reference internal" href="References.html#id17" title="Luc Steels. The synthetic modeling of language origins. Evolution of communication, 1(1):1–34, 1997. URL: https://doi.org/10.1075/eoc.1.1.02ste.">1997</a>]</span>, for overviews).</p>
<p><img alt="basic_modeling_.schematic.png" src="_images/180302220.png" />
<strong>Figure 1:</strong> A high-level schematic view of a typical computational model development and evaluation process.</p>
<div class="section" id="human-cognition-as-a-sensorimotor-information-processing-system">
<h2><span class="section-number">12.1. </span>Human cognition as a sensorimotor information processing system<a class="headerlink" href="#human-cognition-as-a-sensorimotor-information-processing-system" title="Permalink to this headline">¶</a></h2>
<p>Computational modeling research is based on the metaphor of human brain
as a computational information processing system. From an external
observer viewpoint, this system perceives the environment using a number
of input channels (senses), processes the information using some type of
processing steps (the nervous system), and creates outputs (motor
actions) based on the available sensory information and other internal
states of the system. This input/output-relationship is affected by
developmental factors and learning from earlier sensorimotor experience,
realized as changes in the connectivity and structure of the central
nervous system. Computational research attempts to understand the
components of this perception-action loop by replacing the human
physiology and neurophysiology with computational algorithms for sensory
(or sensorimotor) information processing. Typically the aim is not to
replicate information processing of the brain at the level of individual
neurons, but to focus on the <em>computational and algorithmic principles</em>
of the process, i.e., the <em>information representation</em>, <em>flow</em> and
<em>transformation</em> within the system (see <a class="reference external" href="https://en.wikipedia.org/wiki/David_Marr_(neuroscientist)#Levels_of_analysis">Marr’s levels of
analysis</a>;
<span id="id5">Marr [<a class="reference internal" href="References.html#id31" title="David Marr. Vision: A computational investigation into the human representation and processing of visual information. W.H. Freeman and Company, 1982.">1982</a>]</span>). These processing steps could then be implemented in
infinitely many ways using different hardware (biological neurons,
silicon chips architectures, CPU instruction sets, quantum computing
etc.) or translations from computational description to
implementation-specific instructions (consider, e.g., different
programming languages with the same CPU instruction set). Despite the
implementation differences, the observed behavior of the system in terms
of inputs and the resulting outputs can still be similar.</p>
<p>To give an example, a model of adult spoken word recognition could focus
on explaining the acoustic, phonetic and/or other linguistic factors
that affect the process of word recognition. Such a model could focus on
the details of how word recognition process evolves over time when a
spoken word is heard, describing how alternative word candidates are
being considered or rejected during this process (see, e.g., <span id="id6">Magnuson <em>et al.</em> [<a class="reference internal" href="References.html#id30" title="James S Magnuson, Heejo You, Sahil Luthra, Monica Li, Hosung Nam, Monty Escabi, Kevin Brown, Paul D Allopenna, Rachel M Theodore, Nicholas Monto, and others. Earshot: a minimal neural network model of incremental human speech recognition. Cognitive science, 44(4):e12823, 2020. URL: https://doi.org/10.1111/cogs.12823.">2020</a>], Weber and Scharenborg [<a class="reference internal" href="References.html#id15" title="Andrea Weber and Odette Scharenborg. Models of spoken-word recognition. Wiley Interdisciplinary Reviews: Cognitive Science, 3(3):387–401, 2012. doi:10.1002/wcs.1178.">2012</a>]</span>, for examples). Even if the
model would not focus on modeling neurons of the human brain, it could
still explain how our minds decode linguistic information from speech
input. This explanation could include how the process is affected by
factors such as noisy environments, misprounciations, distributional
characteristics of the input, or non-native language background of the
listener—all useful information to understand both theoretical
underpinnings and practical aspects of speech communication.</p>
<p>Another central aspect of the modeling is the relationship between human
learning and computational methods trying to characterize the process.
According to the present understanding, <em>human language learning is
largely driven by</em> the interaction of <em>statistical regularities in the
sensory input</em> available to the learner (e.g., <span id="id7">Maye <em>et al.</em> [<a class="reference internal" href="References.html#id27" title="Jessica Maye, Janet F Werker, and LouAnn Gerken. Infant sensitivity to distributional information can affect phonetic discrimination. Cognition, 82(3):B101–B111, 2002. URL: https://doi.org/10.1016/S0010-0277(01)00157-3.">2002</a>], Saffran <em>et al.</em> [<a class="reference internal" href="References.html#id19" title="Jenny R Saffran, Richard N Aslin, and Elissa L Newport. Statistical learning by 8-month-old infants. Science, 274(5294):1926–1928, 1996. URL: https://doi.org/10.1126/science.274.5294.1926.">1996</a>], Saffran and Kirkham [<a class="reference internal" href="References.html#id18" title="Jenny R Saffran and Natasha Z Kirkham. Infant statistical learning. Annual review of psychology, 69:181–203, 2018. URL: https://doi.org/10.1146/annurev-psych-122216-011805.">2018</a>], Werker and Tees [<a class="reference internal" href="References.html#id14" title="Janet F Werker and Richard C Tees. Cross-language speech perception: evidence for perceptual reorganization during the first year of life. Infant behavior and development, 7(1):49–63, 1984. URL: https://doi.org/10.1016/S0163-6383(84)80022-3.">1984</a>]</span>),
<em>innate mechanisms, constraints, and biases for perception and learning</em>
from such input, and <em>other mechanisms responsible for social,
communicative and exploratory needs</em> <em>o</em>f the learner. By extracting the
statistical regularities from their sensorimotor linguistic enviroment,
children are capable of learning any of the world’s languages while
fundamentally sharing the same basic cognitive mechanisms. A central
topic in computational modeling of language acqusition is therefore to
understand how much of language structure can be learned from the input
data, and how much language-related prior knowledge needs to be built-in
to the hard-coded mechanisms of these models. Note that human
statistical learning is closely related to machine learning in
computers, as both aim to extract statistical regularities from data
using some sort of pre-specified learning principles. However, unlike
standard speech technology systems such as <a class="reference internal" href="Recognition/Speech_Recognition.html"><span class="doc std std-doc">automatic speech
recognition</span></a>, humans learners do not have access to
data labels or consistent reward signals. For instance, a computational
model of early infant word learning is essentially trying to find a
solution to unsupervised pattern discovery problem: how to learn words
from acoustic or multimodal input when there is no data labeling
available. By applying a combination of speech processing and machine
learning techniques to data representative of infant language
experiences, explanation proposals for such a process can be created.</p>
<div class="section" id="computational-modeling-versus-cognitive-computationalism">
<h3><span class="section-number">12.1.1. </span>Computational modeling versus cognitive computationalism<a class="headerlink" href="#computational-modeling-versus-cognitive-computationalism" title="Permalink to this headline">¶</a></h3>
<p>Note that computational modeling and <em>representations</em> often studied in
the models should not be confused with classical
<a class="reference external" href="https://en.wikipedia.org/wiki/Computational_theory_of_mind">computationalism</a>.
The latter is loaded with certain assumptions regarding the nature of
the entities processed by the computational system (e.g., <em>content of
the representations, symbols</em>) and what are the basic computational
operations (e.g., <em>symbol manipulation</em> using Turing machines). In
contrast, computational models are simply descriptions of the studied
process in terms of the described assumptions, inputs, outputs, and
processing mechanisms without prescribing further <em>meaning</em> to the
components (unless otherwise specified). For instance, <em>representations</em>
of typical DSP and machine-learning -based models can simply be treated
as quantifiable states, such as artificial neuron/layer activations,
posterior distributions, neural layer weights, distribution parameters.
In other words, the representations are scalars, vectors, or matrices
that are somehow causally related to the inputs of the system. Behavior
of these representations can then be correlated and compared with
theoretical concepts regarding the phenomenon of interest (e.g.,
comparing selectivity of neural layer activations towards known phoneme
categories in the acoustic input to the model; see, e.g., <span id="id8">Nagamine <em>et al.</em> [<a class="reference internal" href="References.html#id26" title="Tasha Nagamine, Michael L Seltzer, and Nima Mesgarani. Exploring how deep neural networks form phonemic categories. In Sixteenth Annual Conference of the International Speech Communication Association. 2015. URL: https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_1912.pdf.">2015</a>]</span>) or comparing the overall model behavior to human behavior
with similar input (e.g., <span id="id9">Räsänen and Rasilo [<a class="reference internal" href="References.html#id22" title="Okko Räsänen and Heikki Rasilo. A joint model of word segmentation and meaning acquisition through cross-situational learning. Psychological review, 122(4):792, 2015. URL: https://psycnet.apa.org/doi/10.1037/a0039702.">2015</a>]</span>). As long as the models
are able to explain the data or phenomena of interest, the <em>models are a
computational and hypothetical explanation</em> to the phenomenon without
loading the components with additional theoretical or philosophical
assumptions. Additional theoretical loading comes from the <em>data</em> and
<em>evaluation protocols</em> chosen to investigate the models and in terms of
how the modeling findings are interpreted.</p>
</div>
</div>
<div class="section" id="role-of-computational-models-in-scientific-research">
<h2><span class="section-number">12.2. </span>Role of computational models in scientific research<a class="headerlink" href="#role-of-computational-models-in-scientific-research" title="Permalink to this headline">¶</a></h2>
<p>Computational modeling has a role in scientific theory development and
hypothesis testing by providing the means to test high-level theories of
language processing with practical simulations (Fig. 2). This supports
the more traditional approaches to language research that include
collection of empirical data on human language processing, conducting
brain research, or running controlled behavioral experiments in the
laboratory or as real-world intervention studies. By implementing
high-level conceptual models of language processing using real
algorithms operating on real-world language data, one can test whether
the models scale up to complexity of real-world sensory data accessible
to human listeners. In addition to explaining already collected data on
human language processing, computational models can also lead to new
insights and hypotheses about the human processing to be tested in
behavioral experiments.</p>
<p><img alt="role_of_comp_mods" src="_images/180300259.png" />
<strong>Figure 2:</strong>
Different aspects of human language processing research and how they
interact.<br />
Computational modeling uses data from empirical research to test and
inform high-level theories related to the given topic.</p>
<p>One potential advantage of computational modeling is its capability to
address multiple processing mechanisms and language phenomena
simultaneously. This is since <em>computational models can, and must,
explicitly address all aspects of the information processing chain</em> from
input data to the resulting behaviour. By first formulating theories of
language processing in terms of computational goals and operations, then
implementing them as functional signal processing and machine learning
algorithms, and finally exposing them to realistic sensory data
comparable to what real humans experience, ecological plausibility and
validity of the underlying theories can be explicitly tested (cf. <span id="id10">Marr [<a class="reference internal" href="References.html#id31" title="David Marr. Vision: A computational investigation into the human representation and processing of visual information. W.H. Freeman and Company, 1982.">1982</a>]</span>). In contrast, behavioral experiments with real humans—although
necessary for the advancement of our scientific understanding and for
general data collection—can usually focus on only one phenomenon at a
time due to the need for highly-controlled experimental setups. The
fragmentation of focus also makes it difficult to combine knowledge from
individual studies into holistic theoretical frameworks (e.g.,
understanding how phonemic, lexical, and syntactic learning are
dependent on each other in early language development).</p>
</div>
<div class="section" id="examples-of-computational-modeling-research">
<h2><span class="section-number">12.3. </span>Examples of computational modeling research<a class="headerlink" href="#examples-of-computational-modeling-research" title="Permalink to this headline">¶</a></h2>
<p><em><strong>Computational models of child language development</strong></em>: Computational
models of language learning aim at understanding how human children
learn to perceive and produce their native language. The basic idea is
to simulate the learning of a human child, either starting from “birth”
or from a specific stage of language development. Individual models
typically aim to answer questions such as: how phonemic categories are
learned, how word segmentation is achieved, how spoken words are
associated with their referential meanings, or how syntax can be
acquired? The grand challenge is to understand how the adult-like
understanding of language as a discrete, symbolic, and compositional
system can emerge from the exposure to noisy and inherently continuous
sensorimotor environment. Typical computational modeling research
questions include: 1) <em>to what extent are languages learnable from the
statistics of sensory experiences</em>, 2) <em>what type of learning mechanisms
or constraints are needed for the process</em>, and 3) <em>what kind of and how
much data</em> (“experiences”) are required in the process (quality and
quantity of speech, uni- vs. multimodal input etc.). A broader view
takes into account the fact that the children are not just passive
receivers of sensory information but can interact with their caregivers
and their environment as active explorers and learners. Therefore it is
also of interest 4) <em>what type of additional interaction-related
mechanisms and dynamically created experiences are critica</em>l. The big
and yet unaswered question is what are the critical ingredients for
successful language learning, as all normally developing children with
very different language experiences, environments, and also somewhat
differing cognitive skills still manage to converge to a shared
communicative system of their native language.<br />
      As the short-term outcomes, models of language learning can test
and propose different hypotheses for different aspects of language
learning. They also produce functional algorithms for processing
acoustic or multimodal language data in low-resource settings, where
access to data labels is limited (e.g., <span id="id11">Kakouros and Räsänen [<a class="reference internal" href="References.html#id34" title="Sofoklis Kakouros and Okko Räsänen. 3pro–an unsupervised method for the automatic detection of sentence prominence in speech. Speech Communication, 82:67–84, 2016. URL: https://doi.org/10.1016/j.specom.2016.06.004.">2016</a>], Kamper <em>et al.</em> [<a class="reference internal" href="References.html#id33" title="Herman Kamper, Aren Jansen, and Sharon Goldwater. A segmental framework for fully-unsupervised large-vocabulary speech recognition. Computer Speech &amp; Language, 46:154–174, 2017. URL: https://doi.org/10.1016/j.csl.2017.04.008.">2017</a>], Räsänen <em>et al.</em> [<a class="reference internal" href="References.html#id20" title="Okko Räsänen, Gabriel Doyle, and Michael C Frank. Pre-linguistic segmentation of speech into syllable-like units. Cognition, 171:130–150, 2018. URL: https://doi.org/10.1016/j.cognition.2017.11.003.">2018</a>]</span>). Long-term outcomes from language
acquisition modeling contribute to both basic science and practice. In
terms of basic science, the research tries to answer the question of how
one of the most advanced aspects of human cognition, i.e., language,
operates. Long-term practical goals include understanding the impact of
external factors in language development and how to ensure equally
supportive environments for children in different social settings,
understanding different types of language-related disorders and how to
best respond to them, but also how to develop autonomous AI systems
capable of human-like language learning and understanding without
supervised training, i.e.., development of systems ultimately capable of
<em>understanding the intentions and meaning in communication</em>. <br />
     Computational modeling of early language acquisition is closely
related to zero-resource speech processing (see
<a class="reference external" href="http://www.zerospeech.com/">http://www.zerospeech.com/</a>) that aims at algorithms capable of
unsupervised language learning from speech data.</p>
<p><em><strong>Models of spoken word recognition</strong>:</em> Another widely studied topic is
speech perception in adults. Computational models developed for this
purpose attempt to explain how the brain processes incoming speech in
order to recognize words in the input. Models in this area may focus on
explaining the interaction between sub-word and word-level units in
perception, on how words compete with each other during the recognition
process, or, e.g., on how the speech perception is affected by noise in
native and non-native listeners. Since word recognition is essentially a
temporal process, particular attention is typically paid to the
evolution of the recognition process as a function of time (or
proportion of input word or utterance perceived).<br />
    For an overview, see <span id="id12">Weber and Scharenborg [<a class="reference internal" href="References.html#id15" title="Andrea Weber and Odette Scharenborg. Models of spoken-word recognition. Wiley Interdisciplinary Reviews: Cognitive Science, 3(3):387–401, 2012. doi:10.1002/wcs.1178.">2012</a>]</span>. For some examples
of models, see <span id="id13">Magnuson <em>et al.</em> [<a class="reference internal" href="References.html#id30" title="James S Magnuson, Heejo You, Sahil Luthra, Monica Li, Hosung Nam, Monty Escabi, Kevin Brown, Paul D Allopenna, Rachel M Theodore, Nicholas Monto, and others. Earshot: a minimal neural network model of incremental human speech recognition. Cognitive science, 44(4):e12823, 2020. URL: https://doi.org/10.1111/cogs.12823.">2020</a>], McClelland and Elman [<a class="reference internal" href="References.html#id29" title="James L McClelland and Jeffrey L Elman. The trace model of speech perception. Cognitive psychology, 18(1):1–86, 1986. URL: https://doi.org/10.1016/0010-0285(86)90015-0.">1986</a>], Norris [<a class="reference internal" href="References.html#id25" title="Dennis Norris. Shortlist: a connectionist model of continuous speech recognition. Cognition, 52(3):189–234, 1994. URL: https://doi.org/10.1016/0010-0277(94)90043-4.">1994</a>]</span>.</p>
<p><em><strong>Models of speech production</strong></em>: This line of research attempts to
explain how human speech production works in terms of articulators and
their motor control. Some studies also focus on the acquisition of
speech production skills. Typical speech production models involve an
articulatory <a class="reference internal" href="Speech_Synthesis.html"><span class="doc std std-doc">speech synthesizer</span></a>—an algorithm capable
of producing audible speech signals by modeling the physical
characteristics of the vocal apparatus—and some type of motor control
algorithms that are responsible for phonation and articulator movements.
Sometimes hearing system is simulated as well. These models have various
uses from general understanding of the articulatory basis of speech to
understanding speech pathologies, articulatory learning in childhood or
adulthood, or special types of sound production such as singing.<br />
    For classical and more recent examples of articulatory models of
speech production, see, e.g., <span id="id14">Birkholz [<a class="reference internal" href="References.html#id39" title="Peter Birkholz. 3D-Artikulatorische Sprachsynthese. PhD thesis, der Universität Rostock, 2005. URL: https://www.vocaltractlab.de/publications/birkholz-2005-dissertation.pdf.">2005</a>], Birkholz <em>et al.</em> [<a class="reference internal" href="References.html#id38" title="Peter Birkholz, Lucia Martin, Klaus Willmes, Bernd J Kröger, and Christiane Neuschaefer-Rube. The contribution of phonation type to the perception of vocal emotions in german: an articulatory synthesis study. The Journal of the Acoustical Society of America, 137(3):1503–1512, 2015. URL: https://doi.org/10.1121/1.4906836.">2015</a>], Maeda [<a class="reference internal" href="References.html#id28" title="Shinji Maeda. Improved articulatory models. The Journal of the Acoustical Society of America, 84(S1):S146–S146, 1988. URL: https://doi.org/10.1121/1.2025845.">1988</a>]</span>. For models of infant learning of speech
production, see, e.g.,<span id="id15">Howard and Messum [<a class="reference internal" href="References.html#id35" title="Ian S Howard and Piers Messum. Learning to pronounce first words in three languages: an investigation of caregiver and infant behavior using a computational model of an infant. PLoS One, 9(10):e110334, 2014. URL: https://doi.org/10.1371/journal.pone.0110334.">2014</a>], Rasilo and Räsänen [<a class="reference internal" href="References.html#id21" title="Heikki Rasilo and Okko Räsänen. An online model for vowel imitation learning. Speech Communication, 86:1–23, 2017. URL: https://doi.org/10.1016/j.specom.2016.10.010.">2017</a>], Tourville and Guenther [<a class="reference internal" href="References.html#id16" title="Jason A Tourville and Frank H Guenther. The diva model: a neural theory of speech acquisition and production. Language and cognitive processes, 26(7):952–981, 2011. URL: https://doi.org/10.1080/01690960903498424.">2011</a>]</span>.</p>
<p><em><strong>Multi-agent models of language learning, evolution and
communication:</strong></em> Languages are essentially cultural conventions based
on social activity, enabled by genetically coded cognitive and
physiological mechanisms, and learned through interactions between
people. One branch of computational modeling focuses on understanding
how languages emerge, evolve, and are learned through multi-agent
communication and interaction. These simulations, sometimes referred to
as <em>language games</em> or <em>iterated learning</em> (see <span id="id16">Kirby [<a class="reference internal" href="References.html#id32" title="Simon Kirby. Natural language from artificial life. Artificial life, 8(2):185–215, 2002. URL: https://doi.org/10.1162/106454602320184248.">2002</a>]</span>), focus on
non-linear dynamical systems that result from the interaction of
multiple communicative computational agents. These agents can be purely
based on simulation, or they can be based on physical robots interacting
in a shared  physical environment. By providing the agents with
different types of innate goals, mechanisms, learning skills and
environmental conditions, one can study the extent that language-like
signaling systems (as a social system) or language skills (as subjective
capabilities) can emerge from such conditions.<br />
    For overviews, see <span id="id17">Kirby [<a class="reference internal" href="References.html#id32" title="Simon Kirby. Natural language from artificial life. Artificial life, 8(2):185–215, 2002. URL: https://doi.org/10.1162/106454602320184248.">2002</a>], Steels [<a class="reference internal" href="References.html#id17" title="Luc Steels. The synthetic modeling of language origins. Evolution of communication, 1(1):1–34, 1997. URL: https://doi.org/10.1075/eoc.1.1.02ste.">1997</a>]</span>.</p>
</div>
<div class="section" id="references-and-further-reading">
<h2><span class="section-number">12.4. </span>References and further reading<a class="headerlink" href="#references-and-further-reading" title="Permalink to this headline">¶</a></h2>
<p><span id="id18"></span></p>
<p>Birkholz, P.: VocalTractLab: <a class="reference external" href="http://www.vocaltractlab.de/">http://www.vocaltractlab.de/</a> [for work
on articulatory synthesis]</p>
<p>Dupoux, E. et al.: Zero Resource Speech Challenge:
<a class="reference external" href="http://www.zerospeech.com/">http://www.zerospeech.com/</a> [a challenge on unsupervised speech
pattern learning]</p>
<div class="docutils container" id="id19">
<dl class="citation">
<dt class="label" id="id57"><span class="brackets"><a class="fn-backref" href="#id14">Bir05</a></span></dt>
<dd><p>Peter Birkholz. <em>3D-Artikulatorische Sprachsynthese</em>. PhD thesis, der Universität Rostock, 2005. URL: <a class="reference external" href="https://www.vocaltractlab.de/publications/birkholz-2005-dissertation.pdf">https://www.vocaltractlab.de/publications/birkholz-2005-dissertation.pdf</a>.</p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id14">BMW+15</a></span></dt>
<dd><p>Peter Birkholz, Lucia Martin, Klaus Willmes, Bernd J Kröger, and Christiane Neuschaefer-Rube. The contribution of phonation type to the perception of vocal emotions in german: an articulatory synthesis study. <em>The Journal of the Acoustical Society of America</em>, 137(3):1503–1512, 2015. URL: <a class="reference external" href="https://doi.org/10.1121/1.4906836">https://doi.org/10.1121/1.4906836</a>.</p>
</dd>
<dt class="label" id="id55"><span class="brackets"><a class="fn-backref" href="#id3">Dup18</a></span></dt>
<dd><p>Emmanuel Dupoux. Cognitive science in the era of artificial intelligence: a roadmap for reverse-engineering the infant language-learner. <em>Cognition</em>, 173:43–59, 2018. URL: <a class="reference external" href="https://doi.org/10.1016/j.cognition.2017.11.008">https://doi.org/10.1016/j.cognition.2017.11.008</a>.</p>
</dd>
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id1">HBR17</a></span></dt>
<dd><p>William Havard, Laurent Besacier, and Olivier Rosec. Speech-coco: 600k visually grounded spoken captions aligned to mscoco data set. <em>arXiv preprint arXiv:1707.08435</em>, 2017. URL: <a class="reference external" href="https://doi.org/10.21437/GLU.2017-9">https://doi.org/10.21437/GLU.2017-9</a>.</p>
</dd>
<dt class="label" id="id53"><span class="brackets"><a class="fn-backref" href="#id15">HM14</a></span></dt>
<dd><p>Ian S Howard and Piers Messum. Learning to pronounce first words in three languages: an investigation of caregiver and infant behavior using a computational model of an infant. <em>PLoS One</em>, 9(10):e110334, 2014. URL: <a class="reference external" href="https://doi.org/10.1371/journal.pone.0110334">https://doi.org/10.1371/journal.pone.0110334</a>.</p>
</dd>
<dt class="label" id="id52"><span class="brackets"><a class="fn-backref" href="#id11">KRasanen16</a></span></dt>
<dd><p>Sofoklis Kakouros and Okko Räsänen. 3pro–an unsupervised method for the automatic detection of sentence prominence in speech. <em>Speech Communication</em>, 82:67–84, 2016. URL: <a class="reference external" href="https://doi.org/10.1016/j.specom.2016.06.004">https://doi.org/10.1016/j.specom.2016.06.004</a>.</p>
</dd>
<dt class="label" id="id51"><span class="brackets"><a class="fn-backref" href="#id11">KJG17</a></span></dt>
<dd><p>Herman Kamper, Aren Jansen, and Sharon Goldwater. A segmental framework for fully-unsupervised large-vocabulary speech recognition. <em>Computer Speech &amp; Language</em>, 46:154–174, 2017. URL: <a class="reference external" href="https://doi.org/10.1016/j.csl.2017.04.008">https://doi.org/10.1016/j.csl.2017.04.008</a>.</p>
</dd>
<dt class="label" id="id50"><span class="brackets">Kir02</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id16">2</a>,<a href="#id17">3</a>)</span></dt>
<dd><p>Simon Kirby. Natural language from artificial life. <em>Artificial life</em>, 8(2):185–215, 2002. URL: <a class="reference external" href="https://doi.org/10.1162/106454602320184248">https://doi.org/10.1162/106454602320184248</a>.</p>
</dd>
<dt class="label" id="id46"><span class="brackets"><a class="fn-backref" href="#id14">Mae88</a></span></dt>
<dd><p>Shinji Maeda. Improved articulatory models. <em>The Journal of the Acoustical Society of America</em>, 84(S1):S146–S146, 1988. URL: <a class="reference external" href="https://doi.org/10.1121/1.2025845">https://doi.org/10.1121/1.2025845</a>.</p>
</dd>
<dt class="label" id="id48"><span class="brackets">MYL+20</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id13">2</a>)</span></dt>
<dd><p>James S Magnuson, Heejo You, Sahil Luthra, Monica Li, Hosung Nam, Monty Escabi, Kevin Brown, Paul D Allopenna, Rachel M Theodore, Nicholas Monto, and others. Earshot: a minimal neural network model of incremental human speech recognition. <em>Cognitive science</em>, 44(4):e12823, 2020. URL: <a class="reference external" href="https://doi.org/10.1111/cogs.12823">https://doi.org/10.1111/cogs.12823</a>.</p>
</dd>
<dt class="label" id="id49"><span class="brackets">Mar82</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p>David Marr. <em>Vision: A computational investigation into the human representation and processing of visual information</em>. W.H. Freeman and Company, 1982.</p>
</dd>
<dt class="label" id="id45"><span class="brackets"><a class="fn-backref" href="#id7">MWG02</a></span></dt>
<dd><p>Jessica Maye, Janet F Werker, and LouAnn Gerken. Infant sensitivity to distributional information can affect phonetic discrimination. <em>Cognition</em>, 82(3):B101–B111, 2002. URL: <a class="reference external" href="https://doi.org/10.1016/S0010-0277(01)00157-3">https://doi.org/10.1016/S0010-0277(01)00157-3</a>.</p>
</dd>
<dt class="label" id="id47"><span class="brackets">ME86</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id13">2</a>)</span></dt>
<dd><p>James L McClelland and Jeffrey L Elman. The trace model of speech perception. <em>Cognitive psychology</em>, 18(1):1–86, 1986. URL: <a class="reference external" href="https://doi.org/10.1016/0010-0285(86)90015-0">https://doi.org/10.1016/0010-0285(86)90015-0</a>.</p>
</dd>
<dt class="label" id="id44"><span class="brackets"><a class="fn-backref" href="#id8">NSM15</a></span></dt>
<dd><p>Tasha Nagamine, Michael L Seltzer, and Nima Mesgarani. Exploring how deep neural networks form phonemic categories. In <em>Sixteenth Annual Conference of the International Speech Communication Association</em>. 2015. URL: <a class="reference external" href="https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_1912.pdf">https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_1912.pdf</a>.</p>
</dd>
<dt class="label" id="id43"><span class="brackets"><a class="fn-backref" href="#id13">Nor94</a></span></dt>
<dd><p>Dennis Norris. Shortlist: a connectionist model of continuous speech recognition. <em>Cognition</em>, 52(3):189–234, 1994. URL: <a class="reference external" href="https://doi.org/10.1016/0010-0277(94)90043-4">https://doi.org/10.1016/0010-0277(94)90043-4</a>.</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id18">OKS19</a></span></dt>
<dd><p>Pierre-Yves Oudeyer, George Kachergis, and William Schueller. Computational and robotic models of early language development: a review. In J.S. Horst and J. von Koss Torkildsen, editors, <em>International handbook of language acquisition</em>. Routledge/Taylor &amp; Francis Group, 2019. URL: <a class="reference external" href="https://psycnet.apa.org/doi/10.4324/9781315110622-5">https://psycnet.apa.org/doi/10.4324/9781315110622-5</a>.</p>
</dd>
<dt class="label" id="id39"><span class="brackets"><a class="fn-backref" href="#id15">RRasanen17</a></span></dt>
<dd><p>Heikki Rasilo and Okko Räsänen. An online model for vowel imitation learning. <em>Speech Communication</em>, 86:1–23, 2017. URL: <a class="reference external" href="https://doi.org/10.1016/j.specom.2016.10.010">https://doi.org/10.1016/j.specom.2016.10.010</a>.</p>
</dd>
<dt class="label" id="id41"><span class="brackets"><a class="fn-backref" href="#id3">Rasanen12</a></span></dt>
<dd><p>Okko Räsänen. Computational modeling of phonetic and lexical learning in early language acquisition: existing models and future directions. <em>Speech Communication</em>, 54(9):975–997, 2012. URL: <a class="reference external" href="https://doi.org/10.1016/j.specom.2012.05.001">https://doi.org/10.1016/j.specom.2012.05.001</a>.</p>
</dd>
<dt class="label" id="id38"><span class="brackets"><a class="fn-backref" href="#id11">RasanenDF18</a></span></dt>
<dd><p>Okko Räsänen, Gabriel Doyle, and Michael C Frank. Pre-linguistic segmentation of speech into syllable-like units. <em>Cognition</em>, 171:130–150, 2018. URL: <a class="reference external" href="https://doi.org/10.1016/j.cognition.2017.11.003">https://doi.org/10.1016/j.cognition.2017.11.003</a>.</p>
</dd>
<dt class="label" id="id40"><span class="brackets"><a class="fn-backref" href="#id9">RasanenR15</a></span></dt>
<dd><p>Okko Räsänen and Heikki Rasilo. A joint model of word segmentation and meaning acquisition through cross-situational learning. <em>Psychological review</em>, 122(4):792, 2015. URL: <a class="reference external" href="https://psycnet.apa.org/doi/10.1037/a0039702">https://psycnet.apa.org/doi/10.1037/a0039702</a>.</p>
</dd>
<dt class="label" id="id37"><span class="brackets"><a class="fn-backref" href="#id7">SAN96</a></span></dt>
<dd><p>Jenny R Saffran, Richard N Aslin, and Elissa L Newport. Statistical learning by 8-month-old infants. <em>Science</em>, 274(5294):1926–1928, 1996. URL: <a class="reference external" href="https://doi.org/10.1126/science.274.5294.1926">https://doi.org/10.1126/science.274.5294.1926</a>.</p>
</dd>
<dt class="label" id="id36"><span class="brackets"><a class="fn-backref" href="#id7">SK18</a></span></dt>
<dd><p>Jenny R Saffran and Natasha Z Kirkham. Infant statistical learning. <em>Annual review of psychology</em>, 69:181–203, 2018. URL: <a class="reference external" href="https://doi.org/10.1146/annurev-psych-122216-011805">https://doi.org/10.1146/annurev-psych-122216-011805</a>.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">Ste97</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id17">2</a>)</span></dt>
<dd><p>Luc Steels. The synthetic modeling of language origins. <em>Evolution of communication</em>, 1(1):1–34, 1997. URL: <a class="reference external" href="https://doi.org/10.1075/eoc.1.1.02ste">https://doi.org/10.1075/eoc.1.1.02ste</a>.</p>
</dd>
<dt class="label" id="id34"><span class="brackets"><a class="fn-backref" href="#id15">TG11</a></span></dt>
<dd><p>Jason A Tourville and Frank H Guenther. The diva model: a neural theory of speech acquisition and production. <em>Language and cognitive processes</em>, 26(7):952–981, 2011. URL: <a class="reference external" href="https://doi.org/10.1080/01690960903498424">https://doi.org/10.1080/01690960903498424</a>.</p>
</dd>
<dt class="label" id="id33"><span class="brackets">WS12</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id12">2</a>)</span></dt>
<dd><p>Andrea Weber and Odette Scharenborg. Models of spoken-word recognition. <em>Wiley Interdisciplinary Reviews: Cognitive Science</em>, 3(3):387–401, 2012. <a class="reference external" href="https://doi.org/10.1002/wcs.1178">doi:10.1002/wcs.1178</a>.</p>
</dd>
<dt class="label" id="id32"><span class="brackets"><a class="fn-backref" href="#id7">WT84</a></span></dt>
<dd><p>Janet F Werker and Richard C Tees. Cross-language speech perception: evidence for perceptual reorganization during the first year of life. <em>Infant behavior and development</em>, 7(1):49–63, 1984. URL: <a class="reference external" href="https://doi.org/10.1016/S0163-6383(84)80022-3">https://doi.org/10.1016/S0163-6383(84)80022-3</a>.</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Enhancement/Multi-channel_speech_enhancement_and_beamforming.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">11.4. </span>Multi-channel speech enhancement and beamforming</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Security_and_privacy.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Security and privacy in speech technology</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>