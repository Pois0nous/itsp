
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8.6. Paralinguistic speech processing &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9. Speech Synthesis" href="../Speech_Synthesis.html" />
    <link rel="prev" title="8.5. Speaker Diarization" href="Speaker_Diarization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Representations/Representations.html">
   3. Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Waveform.html">
     3.1. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Windowing.html">
     3.2. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Signal_energy_loudness_and_decibel.html">
     3.3. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Autocorrelation_and_autocovariance.html">
     3.4. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Cepstrum_and_MFCC.html">
     3.5. Cepstrum and MFCC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Linear_prediction.html">
     3.6. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Fundamental_frequency_F0.html">
     3.7. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Zero-crossing_rate.html">
     3.8. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Deltas_and_Delta-deltas.html">
     3.9. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.10. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Jitter_and_shimmer.html">
     3.11. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
     7.2. Inverse filtering for glottal activity estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Analysis/Measurements_for_medical_applications.html">
     7.3. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Analysis/Glottal_inverse_filtering.html">
       7.3.1. Glottal inverse filtering
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Forensic_analysis.html">
     7.4. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="Voice_activity_detection_VAD.html">
     8.1. Voice activity detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speaker_Recognition_and_Verification.html">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Concatenative_speech_synthesis.html">
     9.2. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Statistical_parametric_speech_synthesis.html">
     9.3. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Basic_tools.html">
     10.2. Basic tools
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.3. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Entropy_coding.html">
       10.3.4. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.3.5. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Code-excited_linear_prediction_CELP.html">
     10.4. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Frequency-domain_coding.html">
     10.5. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://landbot.io/blog/guide-to-conversational-design/">
   Chatbots / Conversational design (external link)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Security_and_privacy_in_speech_technology.html">
   13. Security and privacy in speech technology
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Recognition/Paralinguistic_speech_processing.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#coupling-between-speaker-states-and-the-speech-signal">
   8.6.1.
   <strong>
    Coupling between speaker states and the speech signal
   </strong>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speaker-traits-and-states">
   8.6.2. Speaker traits and states
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#typical-applications-of-psp">
   8.6.3. **Typical applications of PSP **
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-problem-formulation-and-standard-solutions">
   8.6.4. **Basic problem formulation and standard solutions **
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-collection-and-data-sparsity">
   8.6.5.
   <strong>
    Data collection and data sparsity
   </strong>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computational-paralinguistic-challenge">
   8.6.6. **Computational Paralinguistic Challenge **
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading-and-materials-on-psp">
   8.6.7. **Further reading and materials on PSP **
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-references">
   8.6.8.
   <strong>
    Other references
   </strong>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attachments">
   8.6.9. Attachments:
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Paralinguistic speech processing</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#coupling-between-speaker-states-and-the-speech-signal">
   8.6.1.
   <strong>
    Coupling between speaker states and the speech signal
   </strong>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speaker-traits-and-states">
   8.6.2. Speaker traits and states
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#typical-applications-of-psp">
   8.6.3. **Typical applications of PSP **
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-problem-formulation-and-standard-solutions">
   8.6.4. **Basic problem formulation and standard solutions **
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-collection-and-data-sparsity">
   8.6.5.
   <strong>
    Data collection and data sparsity
   </strong>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computational-paralinguistic-challenge">
   8.6.6. **Computational Paralinguistic Challenge **
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading-and-materials-on-psp">
   8.6.7. **Further reading and materials on PSP **
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-references">
   8.6.8.
   <strong>
    Other references
   </strong>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attachments">
   8.6.9. Attachments:
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="paralinguistic-speech-processing">
<h1><span class="section-number">8.6. </span>Paralinguistic speech processing<a class="headerlink" href="#paralinguistic-speech-processing" title="Permalink to this headline">¶</a></h1>
<p>Paralinguistic speech processing (PSP) refers to analysis of speech
signals with the aim of extracting information beyond the linguistic
content of speech (hence <em>para</em>linguistic = alongside linguistic
content; see also Schuller &amp; Batliner, 2014). In other words, PSP does
not focus on what is the literal transmitted message but on what
additional information is conveyed by the signal. Speaker
<a class="reference internal" href="Speaker_Diarization.html"><span class="doc std std-doc">diarization</span></a>,
<a class="reference internal" href="Speaker_Recognition_and_Verification.html"><span class="doc std std-doc">recognition</span></a>, and
<a class="reference internal" href="Speaker_Recognition_and_Verification.html"><span class="doc std std-doc">verification</span></a>, even though
focusing on non-linguistic aspects, are also traditionally considered as
separate problems that do not fall within the scope of PSP. A classical
example of PSP is speech emotion recognition, where the aim is to infer
the emotional state of a speaker based on a sample of his or her speech.
In a similar manner, information related to the health or age of a
speaker could be inferred from the speech signal.</p>
<div class="section" id="coupling-between-speaker-states-and-the-speech-signal">
<h2><span class="section-number">8.6.1. </span><strong>Coupling between speaker states and the speech signal</strong><a class="headerlink" href="#coupling-between-speaker-states-and-the-speech-signal" title="Permalink to this headline">¶</a></h2>
<p>The basic starting point for PSP systems is that the speech signal also
reflects the underlying cognitive and neurophysiological state of a
speaker. This is since speaking involves highly complicated cognitive
processing in terms of real-time communicative, linguistic, and
articulatory planning. In addition, execution of these plans requires
highly-precise motor control of articulators paired with real-time
monitoring of the resulting acoustic signal, and both of these tasks
take place in parallel with further speech planning. The mental state of
the speaker may also affect the manner that the speaker wishes to
express himself or herself.  Finally, the overall physiological
characteristics of the speech production apparatus also shape the
resulting signal, and details of these characteristics may also change
due to illnesses or habits. This means that many temporary or permanent
perturbations in the cognitive and physical machinery of a talker may
show up in the resulting speech.</p>
<p>To give some examples, substantial cognitive load (e.g., a concurrent
attention-requiring task) or neurodegenerative diseases affecting memory
(e.g., Alzheimer’s disease) may impact speech planning due to
compromised cognitive resources, resulting in speech output that differs
from the typical speech from the same person in non-stressful or healthy
conditons. In the same way, neurodegenerative diseases affecting the
brain’s motor system (e.g., Parkinson’s disease) may impact fluidity and
clarity of speech production, and the symptoms will become more
pronounced as the disease progresses. As for articulatory changes,
stress and emotional distress can cause increased tension in the muscles
of the larynx, which can result in tightening of the vocal folds and
therefore also causes increases in the fundamental frequency of speech.
Changes in the physical characteristics of the vocal tract may result
from, e.g., having a cold. In this case, mucus on tract surfaces may
affect resonance and damping characteristics of the vocal tract. In
addition, the mucus may prevent full closing of the velum, causing
nasalized speech often associated with a severe cold. The speaker may
also speak differently due to cognitive fatigue and throat soreness due
to the cold. Aging will also change the characteristics of the speech
production apparatus, not only in childhood but also in later years of
life. These changes are driven by physiological changes in the glottis
and in the vocal tract, where growth of the vocal tract length in early
childhood has an especially pronounced effect. The voice change in
puberty is also an example of quick growth of the larynx and vocal
folds, but small changes in the vocal folds and their control may also
take with later aging.</p>
<p>In addition to information that is not directly related to intended
communicative goals, speech also contains paralinguistic characteristics
related to communication. This is because speech has co-evolved with the
development of other social skills in humans over thousands of years.
Speech (and gestures) can thefore play different types of social
coordinative roles beyond the literal linguistic message transmitted.
For instance, prosody, and speaking style in general, can reflect
different social roles such as submissiveness, arrogance, or authority
in different interactions. Attitudes and emotions showing up in speech
can also be considered as communicative signals facilitating social
interaction and cohesion, not just being speaker-internal states that
inadvertently “leak out” for others to perceive. Demonstration of anger
or happiness through voice can transmit important information regarding
social dynamics even when visual contact between the interlocutors is
not possible. As a concrete example, consider having a telephone
conversation with someone close to you without access to anything else
than the literal message (e.g., substituting the original speech with
monotone but perfectly intelligible speech synthesis) while trying to
communicate highly sensitive and important personal information.</p>
<p>The basic aim of PSP is to use computational means to understand and
characterize the ways that different paralinguistic factors shape the
speech signal, and to build automatic systems for analyzing and
detecting the paralinguistic factors from real speech captured in
various settings.</p>
<p>Also note that the distinction between PSP systems and other established
areas of speech processing is not always clear-cut. For instance,
automated methods for speech <span class="xref myst">intelligibility
assessment</span> are also focusing on
extralinguistic factors, and the task of speaker recognition was already
mentioned at the beginning of this section. In addition, more flexible
control of speaking style is an ongoing topic of research in <span class="xref myst">speech
synthesis</span>, where the research focus is gradually
changing from the production of high-quality to speech to creation of
systems capable of richer vocalic expression. However, there is no need
for a strict distinction of PSP from other types of processing tasks,
but PSP can be viewed as an umbrella term for the increasingly many
analysis tasks focused on the various non-literal aspects of spoken
language, and where similar data-driven methodology is usually
applicable across a broad range of PSP phenomena (see also Schuller &amp;
Batliner, 2014, for a discussion).</p>
</div>
<div class="section" id="speaker-traits-and-states">
<h2><span class="section-number">8.6.2. </span>Speaker traits and states<a class="headerlink" href="#speaker-traits-and-states" title="Permalink to this headline">¶</a></h2>
<p>Schuller &amp; Batliner (2014) use a distinction into two types of speaker
characteristics: traits and states. These are related to the temporal
properties of the analyzed phenomena. Speaker traits are long-term and
slowly-changing characteristics of the speaker, such as personality
traits (e.g., Big Five classification), gender, age, or dialect. On the
other hand, speaker states are short- to medium-term phenomena, such as
speaker’s emotional state, attitude in a conversation, (temporary)
health conditions, fatigue, or stress level. When collecting data for
PSP research and system development, it is important to consider the
time-scale of the phenomenon to be analyzed and how this relates to
practical needs of the analysis task (e.g., how much speech can be
collected and analyzed before classification decision; does the system
have to be real-time). For instance, quickly changing characteristics
such as emotional state should be analyzed from relatively short speech
recordings where the factor of interest can be assumed to be stable. For
example, recognition of speaker’s emotional state during a single
utterance is a widely adopted approach in speech emotion recognition. In
contrast, analysis of speaker health (e.g., COVID-19 symptoms) from just
one utterance is likely to be inaccurate, but speaker-dependent data
collection and analysis across longer stretches of speech will likely
produce more reliable analysis outcomes. Moreover, longitudinal
monitoring of a subject across longer periods of time is likely to be
more accurate in detecting changes in the speaker’s voice, as the system
can be adapted to the acoustic and linguistic characteristics of that
specific speaker. For instance, subject-specific monitoring of the
progression of a neurodegenerative disease based on speech is likely to
be more accurate than automatic classification of disease severity from
a bag of utterances from a random collection of speakers.</p>
</div>
<div class="section" id="typical-applications-of-psp">
<h2><span class="section-number">8.6.3. </span>**Typical applications of PSP **<a class="headerlink" href="#typical-applications-of-psp" title="Permalink to this headline">¶</a></h2>
<p>Some possible applications of paralinguistic tasks include, but are not
limited to:</p>
<ul class="simple">
<li><p>Emotion classification</p></li>
<li><p>Personality classification (e.g., Big Five traits)</p></li>
<li><p>Sleepiness or intoxication detection</p></li>
<li><p>Analysis of cognitive or physical load</p></li>
<li><p>Health-related analyses (cold, snoring, neurodegenerative diseases
etc.)</p></li>
<li><p>Speech addressee analysis (e.g., adult- vs. infant-directed speech)</p></li>
<li><p>Age and gender recognition</p></li>
<li><p>Sincerity analysis</p></li>
<li><p>Attitude analysis</p></li>
</ul>
</div>
<div class="section" id="basic-problem-formulation-and-standard-solutions">
<h2><span class="section-number">8.6.4. </span>**Basic problem formulation and standard solutions **<a class="headerlink" href="#basic-problem-formulation-and-standard-solutions" title="Permalink to this headline">¶</a></h2>
<p>The basic goal of paralinguistic analysis is to extract paralinguistic
information of interest while ignoring the signal variability introduced
by other factors, such as linguistic content, speaker identity,
background noise or transmission channel characteristics (aka. <em>nuisance
factors</em>). However, for some tasks, it may also be useful to analyse the
lexical and grammatical content of speech in order to infer information
regarding the phenomena of interest.</p>
<p>Typical PSP systems follow the two classical tasks of machine learning:
classification and regression. In classification, the goal is to build a
system that can assign a speech sample (e.g., an utterance) into one of
two or more categories (e.g., intoxicated or not intoxicated). In
regression, the target is a continuous (or at least ordinal) measure
(e.g., blood alcohol concentration percentage).</p>
<p>Fig. 1 illustrates a standard PSP system pipeline, which follows a
typical supervised machine learning scenario. First, a number of
features are extracted from the speech signal. These features, together
with the corresponding class labels, are then used to train a classifier
model for the training dataset. During testing and actual use of the
system, the classifier is used to determine the most likely class of an
input waveform. Depending on the classifier architecture, the
classification result may or may not be associated with a confidence
measure of the classification decision. In regression tasks, the process
is otherwise the same, but the categorical class labels are replaced by
continous-valued measures and the classifier is replaced by a regression
model.</p>
<p>A central characteristic of many PSP tasks is that the analyzed
phenomenon (e.g., speaker emotion) is assumed to be fixed at a certain
time-scale, such as across one utterance, and therefore classification
decisions should also be made at time-scales longer than typical
frame-level signal features. In this case, it would desirable to obtain
a fixed-dimensional feature representation of the signal even if the
duration of the input waveform varies from case to case. This can be
achieved by a two-step process: <strong>1)</strong> first extracting regular
frame-level features (e.g., spectral features such as FFT) with high
temporal resolution (e.g., one frame every 10-ms), sometimes referred to
as low-level descriptors (LLDs), and then <strong>2)</strong> calculating statistical
parameters (“functionals”) of each of the features across all the
time-frames (illustrated in Fig. 2). Typical functionals of LLDs
include, e.g., <em>min, max, mean, variance, skewness,</em> and <em>kurtosis</em>, but
they can also be measures, such as centroids, percentiles, or different
types of means. In order to acquire a more complete picture of the
signal dynamics, first- and second-order time-derivatives of the
frame-level features (“deltas” and “delta-deltas”) are often included in
the feature set before calculating the functionals. In neural networks,
a varying-length signal can also be represented by a fixed-dimensional
embedding extracted from the input waveform, such as taking the output
of an LSTM-layer. Once the input signals are represented by
fixed-dimensional feature vectors, standard machine learning classifiers
can be applied to the data.</p>
<p><img src="attachments/159748623/180298884.png" class="image-center"
data-image-src="attachments/159748623/180298884.png"
data-unresolved-comment-count="0" data-linked-resource-id="180298884"
data-linked-resource-version="1" data-linked-resource-type="attachment"
data-linked-resource-default-alias="basic_PSP_pipeline.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="159748623"
data-linked-resource-container-version="55" width="800" /></p>
<p><strong>Figure 1:</strong> An example of a classical PSP processing pipeline with
training (top) and usage (bottom). Speech features are first extracted
from the original speech waveform, followed by a classifier that has
been trained using supervised learning with labeled training samples.</p>
<p><img src="attachments/159748623/180298908.png"
data-image-src="attachments/159748623/180298908.png"
data-unresolved-comment-count="0" data-linked-resource-id="180298908"
data-linked-resource-version="4" data-linked-resource-type="attachment"
data-linked-resource-default-alias="PSP_feature_extraction.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="159748623"
data-linked-resource-container-version="55" width="750" /></p>
<p><strong>Figure 2:</strong> An example signal-level feature extraction process where
variable-duration utterances become represented by fixed-dimensional
feature vectors, consisting of statistical parameters (“functionals”)
calculated across frame-by-frame speech features (sometimes also
referred to as low-level descriptors, or “LLDs”).</p>
<p>In the both cases of classification and regression, the challenge is to
build a system that can discriminate the key features of the phenomenon
without being affected by the other sources of signal variability.
High-performance well-generalizing systems can be approached with three
basic strategies: <em>well-designed features</em>, <em>robust classifiers</em>, or
<em>end-to-end learning</em>, where signal representations (“features”) and
classifiers are jointly learned from the data. Selection of the approach
depends on two primary factors:  domain knowledge and availability of
representative large-scale training data (Fig. 3).</p>
<p>The basic principle is that well-designed or otherwise properly chosen
features require less training data for inference of the machine
learning model parameters, but this necessitates that the
acoustic/linguistic characteristics of the analyzed phenomenon are known
in order to design features that capture them. If there are only limited
data and limited domain knowledge, it is still possible to calculate a
large number of potentially relevant features and then use a classifier
such as Support Vector Machines (SVMs; Boser et al., 1992) that can
robustly handle high-dimensional features, including potentially
task-irrelevant or otherwise noisy features. For instance, the baseline
systems for annual Computational Paralinguistic Challenges (see also
below) have traditionally used a combination of more than 6000
signal-level features together with an SVM classifier, and often these
systems have been highly competitive with other solutions.
Alternatively, feature selection techniques may be applied in
conjunction with data labeling to find a more compact feature set for
the problem at hand (see, e.g., Pohjalainen et al., 2015).</p>
<p>If there are plenty of training data available, multilayer neural
networks can be used to simultaneously learn useful signal
representations and a classifier for the given problem. This type of
representation learning can operate directly on the acoustic waveform or
using some standard spectral representation with limited additional
assumptions, such as FFT or log-Mel spectrum. The obvious advantage is
that feature design is no longer needed, and the features and the
classifier are seamlessly integrated and jointly optimized. In addition,
even if domain knowledge would be available, the assumptions built into
manually tailored features may lose some details of the modeled
phenomenon, whereas end-to-end neural networks can potentially use all
the information available in the input signal. Similarly to many other
applications of machine learning, neural networks can therefore be
expected to outperform the “more classical” approaches if sufficient
training data are available for the task (see also sub-section below for
data in PSP). However, besides the data requirement drawback, the
standard problems and principles of neural network design and training
apply, including the lack of access to the global optimum during the
parameter optimization process.</p>
<p>If sufficient training data and domain knowledge are both available, one
may also combine representation learning with good initial features or
some type of model priors or constraints. This may speed-up the learning
process or improve model convergence to a more effective solution due to
a more favorable starting point for the optimization process. One
particularly interesting new research direction is the use of
differentiable computational graphs in feature extraction. In this case,
prior task-related knowledge could be incorporated into digital signal
processing (DSP) steps used to extract some initial features, but the
feature extraction algorithm would be implemented together with the rest
of the network as one differentiable computational graph (e.g., using a
framework such as TensorFlow). This would allow error backpropagation
through the entire pipeline from classification decisions to the feature
extractor, thereby enabling task-optimized adaptation of the feature
extraction or pre-processing steps. As an example, Discrete Fourier
Transform is a differentiable function, allowing error gradients to pass
through it, although by default it does not have any free parameters to
optimize. However, similar signal transformations but with parametrized
basis functions could be utilized and optimized jointly with the rest of
the model.</p>
<p><img src="attachments/159748623/180298840.png" class="image-center"
data-image-src="attachments/159748623/180298840.png"
data-unresolved-comment-count="0" data-linked-resource-id="180298840"
data-linked-resource-version="2" data-linked-resource-type="attachment"
data-linked-resource-default-alias="feature_design_quadratic.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="159748623"
data-linked-resource-container-version="55" width="650" /></p>
<p><strong>Figure 3:</strong> Basic strategies for PSP system development given the two
main considerations: availability of training data (x-axis) and domain
knowledge (y-axis).</p>
<p>Naturally, the division into the four categories described in Fig. 3 is
by no means definite. For instance, recent developments in
self-supervised representation learning (van den Oord et al., 2018;
Chung et al., 2019; Baevski et al., 2020) or multitask learning (e.g.,
Wu et al., 2015) are still largely unexplored in PSP. Such approaches
could enable effective utilization of large amounts of unlabeled speech
data with fewer labeled examples in the target domain of interest.</p>
</div>
<div class="section" id="data-collection-and-data-sparsity">
<h2><span class="section-number">8.6.5. </span><strong>Data collection and data sparsity</strong><a class="headerlink" href="#data-collection-and-data-sparsity" title="Permalink to this headline">¶</a></h2>
<p>Since modern PSP largely relies on machine learning, representative
training data will be required for the phenomenon of interest. However,
access to high-quality labeled data is often be limited for many PSP
tasks. First of all, the data collection itself is often challenging and
may include important ethical considerations, such as collecting data
from intoxicated speakers or subjects with rare diseases, or data where
some type of objective (physiological) measurements of emotional state
are captured simultaneously with the speech audio. Availability of
reliable ground-truth labels for the speech data can also be difficult.
For instance, there is no direct way to measure the underlying emotional
states of speakers, whereas induced emotional speech by professional
actors may not properly reflect the variability of real-world emotional
expression. In a similar manner, assessment of severity of many diseases
is based on various indirect measurements and clinical diagnostic
practices, not on some type of oracle knowledge on a universally
standardized scale. Every time humans are used for data labelling (e.g.,
assessing emotions), there is a certain degree of inter-annotator
inconsistency due to differing opinions and general variability in human
performance. This is the case even when domain experts are used for the
task. Naturally, the more difficult the task or more ambiguous the
phenomenon, the more there will be noise and ambiguity in the labeling.</p>
<p>Another limitation hindering PSP progress is that many PSP datasets
cannot be freely distributed to the research community due to data
ownership and human participant privacy protection considerations. As a
clear example, speech with metadata related to factors such as health or
IQ of the speakers is highly sensitive in nature, and not all speakers
consent to open distribution of their identifiable voice together with
such private data of themselves. The data ownership considerations
inherently limit the pooling of different speech corpora in order to
build more comprehensive databases of speech related to different
phenomena of interest, and generally slow down replicable open science.
On the other hand, it is of utmost importance to respect the privacy of
human participants in PSP (or any other) research—not only due to
ethical considerations, but also since the entire field depends on
access to data from voluntary human participants. Commercial interests
to data ownership are also often unavoidable.</p>
<p>In total, this means that data is often a limiting factor in system
performance, and therefore deep neural networks have still not become
the only off-the-shelf-solution for many PSP problems. Efforts for more
flexible (but ethical) data sharing and pooling is therefore an
important challenge for future research. More powerful technical
solutions and deeper understanding of the PSP phenomena could be
acquired by combining rich data (and metadata) from various sources,
including different languages, cultural environments, and recording
conditions.</p>
</div>
<div class="section" id="computational-paralinguistic-challenge">
<h2><span class="section-number">8.6.6. </span>**Computational Paralinguistic Challenge **<a class="headerlink" href="#computational-paralinguistic-challenge" title="Permalink to this headline">¶</a></h2>
<p>Research in PSP has been strongly advanced by an annual Computational
Paralinguistic Challenge (ComParE) held in the context of ISCA
Interspeech conferences (see <a class="reference external" href="http://www.compare.openaudio.eu/">http://www.compare.openaudio.eu/</a>). Every
year since 2009, ComParE has included a number of paralinguistic
analysis tasks with pre-defined datasets in which participants can
compete with each other. Competitive baseline systems, evaluation
protocols and results are always provided to the participants as a
starting point, enabling low-barrier access to the world of PSP for
researchers with various backgrounds. In addition, new tasks and
datasets can be proposed to challenge organizers, providing a useful
channel for data owners and researchers to obtain competitive solutions
to their analysis problems.</p>
</div>
<div class="section" id="further-reading-and-materials-on-psp">
<h2><span class="section-number">8.6.7. </span>**Further reading and materials on PSP **<a class="headerlink" href="#further-reading-and-materials-on-psp" title="Permalink to this headline">¶</a></h2>
<p>Schuller, B. et al.: Computational Paralinguistic Challenge. WWW-site:
<a class="reference external" href="http://www.compare.openaudio.eu/">http://www.compare.openaudio.eu/</a>, last accessed 11th October 2020.</p>
<p>Eyben, F., Wöllmer, M., &amp; Schuller, B. (2010). openSMILE – The Munich
Versatile and Fast Open-Source Audio Feature Extractor. Proc. 18th ACM
International Conference on Multimedia, Florence, Italy, pp. 1459–1464,
<a class="reference external" href="https://www.audeering.com/opensmile/">https://www.audeering.com/opensmile/</a></p>
<p>Pohjalainen J., Räsänen O., &amp; Kadioglu S. (2015). Feature selection
methods and their combinations in high-dimensional classification of
speaker likability, intelligibility and personality traits. <em>Computer
Speech and Language</em>, 29, 145–171.</p>
<p>Schuller, B. &amp; Batliner, A. (2014). <em>Computational paralinguistics:
Emotion, affect and personality in speech and language processing.</em> John
Wiley &amp; Sons Ltd, UK.</p>
</div>
<div class="section" id="other-references">
<h2><span class="section-number">8.6.8. </span><strong>Other references</strong><a class="headerlink" href="#other-references" title="Permalink to this headline">¶</a></h2>
<p>Baevski, A., Zhou, H., Abdelrahman, M., &amp; Auli, M. (2020). wav2vec 2.0:
A framework for self-supervised learning of speech representations.
arXiv pre-print, <a class="reference external" href="https://arxiv.org/abs/2006.11477">https://arxiv.org/abs/2006.11477</a></p>
<p>Chung, Y-A., Hsu W-N., Tang. H., &amp; Glass, J. (2019). An unsupervised
autoregressive model for speech representation learning. <em>Proc.
Interspeech-2019,</em> Graz, Austria, pp. 146—150.</p>
<p>Boser, B., Guyon, I., &amp; Vapnik, V. (1992). A training algorithm for
optimal margin classifiers. <em>Proc. 5th Annual Workshop on Computational
Learning Theory</em>, pp. 144–152.</p>
<p>van den Oord, A., Li, Y., &amp; Vinyals, O. (2018). Representation learning
with contrastive predictive coding. arXiv pre-print,
<a class="reference external" href="https://arxiv.org/abs/1807.03748">https://arxiv.org/abs/1807.03748</a>.</p>
<p>Wu, Z., Valentini-Botinhao, C., Watts, O., &amp; King, S. (2015). Deep
neural networks employing multi-task learning and stacked bottleneck
features for speech synthesis. <em>Proc. ICASSP-2015,</em> Brisbane, Australia,
pp. 4460–4464.</p>
<div class="pageSectionHeader">
</div>
<div class="section" id="attachments">
<h2><span class="section-number">8.6.9. </span>Attachments:<a class="headerlink" href="#attachments" title="Permalink to this headline">¶</a></h2>
</div>
<div class="greybox" align="left">
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[feature_design_quadratic.pdf](attachments/159748623/180298839.pdf)
(application/pdf)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[feature_design_quadratic.png](attachments/159748623/180298844.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[feature_design_quadratic.png](attachments/159748623/180298840.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[basic_PSP_pipeline.png](attachments/159748623/180298884.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[PSP_feature_extraction.png](attachments/159748623/180298910.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[PSP_feature_extraction.png](attachments/159748623/180299544.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[PSP_feature_extraction.png](attachments/159748623/180299549.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[PSP_feature_extraction.png](attachments/159748623/180298908.png)
(image/png)  
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Recognition"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Speaker_Diarization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">8.5. </span>Speaker Diarization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../Speech_Synthesis.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Speech Synthesis</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>