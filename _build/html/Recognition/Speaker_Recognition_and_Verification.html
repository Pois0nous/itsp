
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8.4. Speaker Recognition and Verification &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8.5. Speaker Diarization" href="Speaker_Diarization.html" />
    <link rel="prev" title="8.3. Speech Recognition" href="Speech_Recognition.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Representations/Representations.html">
   3. Basic Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Waveform.html">
     3.1. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Windowing.html">
     3.2. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Signal_energy_loudness_and_decibel.html">
     3.3. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Spectrogram_and_the_STFT.html">
     3.4. Spectrogram and the STFT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Autocorrelation_and_autocovariance.html">
     3.5. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Cepstrum_and_MFCC.html">
     3.6. Cepstrum and MFCC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Linear_prediction.html">
     3.7. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Fundamental_frequency_F0.html">
     3.8. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Zero-crossing_rate.html">
     3.9. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Deltas_and_Delta-deltas.html">
     3.10. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.11. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Jitter_and_shimmer.html">
     3.12. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
     7.2. Inverse filtering for glottal activity estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Analysis/Measurements_for_medical_applications.html">
     7.3. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Analysis/Glottal_inverse_filtering.html">
       7.3.1. Glottal inverse filtering
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Forensic_analysis.html">
     7.4. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="Voice_activity_detection_VAD.html">
     8.1. Voice activity detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Paralinguistic_speech_processing.html">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Concatenative_speech_synthesis.html">
     9.2. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Statistical_parametric_speech_synthesis.html">
     9.3. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Basic_tools.html">
     10.2. Basic tools
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.3. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Entropy_coding.html">
       10.3.4. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.3.5. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Code-excited_linear_prediction_CELP.html">
     10.4. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Frequency-domain_coding.html">
     10.5. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://landbot.io/blog/guide-to-conversational-design/">
   Chatbots / Conversational design (external link)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Security_and_privacy_in_speech_technology.html">
   13. Security and privacy in speech technology
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Recognition/Speaker_Recognition_and_Verification.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-speaker-recognition">
   8.4.1.
   <strong>
    1. Introduction to Speaker Recognition
   </strong>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#front-end-processing">
   8.4.2. 2. Front-end Processing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speaker-modeling-techniques">
   8.4.3. 3. Speaker Modeling Techniques
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-mixture-modeling-gmm-universal-background-model-ubm-approach">
   8.4.4. 3.1. Gaussian Mixture Modeling (GMM) - Universal Background Model (UBM) Approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-vectors">
   8.4.5. 3.2. i-Vectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-learning-dl">
   8.4.6. 3.3.  Deep Learning (DL)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications-of-speaker-recognition">
   8.4.7. 4. Applications of Speaker Recognition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-evaluations">
   8.4.8. 5. Performance Evaluations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#types-of-errors">
   8.4.9. 5.1 Types of errors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-metrics">
   8.4.10. 5.2 Performance metrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attachments">
   8.4.11. Attachments:
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Speaker Recognition and Verification</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-speaker-recognition">
   8.4.1.
   <strong>
    1. Introduction to Speaker Recognition
   </strong>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#front-end-processing">
   8.4.2. 2. Front-end Processing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speaker-modeling-techniques">
   8.4.3. 3. Speaker Modeling Techniques
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-mixture-modeling-gmm-universal-background-model-ubm-approach">
   8.4.4. 3.1. Gaussian Mixture Modeling (GMM) - Universal Background Model (UBM) Approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-vectors">
   8.4.5. 3.2. i-Vectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-learning-dl">
   8.4.6. 3.3.  Deep Learning (DL)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications-of-speaker-recognition">
   8.4.7. 4. Applications of Speaker Recognition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-evaluations">
   8.4.8. 5. Performance Evaluations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#types-of-errors">
   8.4.9. 5.1 Types of errors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-metrics">
   8.4.10. 5.2 Performance metrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attachments">
   8.4.11. Attachments:
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="speaker-recognition-and-verification">
<h1><span class="section-number">8.4. </span>Speaker Recognition and Verification<a class="headerlink" href="#speaker-recognition-and-verification" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction-to-speaker-recognition">
<h2><span class="section-number">8.4.1. </span><strong>1. Introduction to Speaker Recognition</strong><a class="headerlink" href="#introduction-to-speaker-recognition" title="Permalink to this headline">¶</a></h2>
<p>Speaker recognition is the task of identifying a speaker using their
voice. Speaker recognition is classified into two parts: speaker
identification and speaker verification. While speaker identification is
the process of determining which voice in a group of known voices best
matches the speaker’, speaker verification is the task of accepting or
rejecting the identity claim of a speaker by analyzing their acoustic
samples. Speaker verification systems are computationally less complex
than speaker identification systems since they require a comparison
between only one or two models, whereas speaker identification requires
comparison of one model to N speaker models.</p>
<p>Speaker verification methods are divided into text-dependent and
text-independent methods. In text-dependent methods, the speaker
verification system has prior knowledge about the text to be spoken and
the user is expected to speak this text. However, in a text-independent
system, the system has no prior knowledge about the text to be spoken
and the user is not expected to be cooperative. Text-dependent systems
achieve high speaker verification performance from relatively short
utterances, while text-independent systems require long utterances to
train reliable models and achieve good performance.</p>
<p>Block diagram of a basic speaker verification system</p>
<p>As it is shown in the above block diagram of a basic speaker
verification system, a speaker verification system involves two main
phases: the training phase in which the target speakers are enrolled and
the testing phase in which a decision about the identity of the speaker
is taken. From a training point of view, speaker models can be
classified into generative and discriminative. Generative models such as
Gaussian Mixture Model (GMM) estimate the feature distribution within
each speaker. Discriminative models such as Support Vector Machine and
Deep Neural Network (DNN), in contrast, model the boundary between
speakers.</p>
<p>The performance of speaker verification systems is degraded by the
variability in channels and sessions between enrolment and verification
speech signals. Factors which affect channel/session variability
include:</p>
<ol class="simple">
<li><p>Channel mismatch between enrolment and verification speech signals
such as using different microphones in enrolment and verification
speech signals.</p></li>
<li><p>Environmental noise and reverberation conditions.</p></li>
<li><p>The differences in speaker voice such as ageing, health, speaking
style and emotional state.</p></li>
<li><p>Transmission channel such as landline, mobile phone, microphone and
voice over Internet protocol (VoIP).</p></li>
</ol>
</div>
<div class="section" id="front-end-processing">
<h2><span class="section-number">8.4.2. </span>2. Front-end Processing<a class="headerlink" href="#front-end-processing" title="Permalink to this headline">¶</a></h2>
<p>Many front-end processing are often used to process the speech signals
and to extract the features which are used in the speaker verification
system. The front-end processing consists of mainly voice activity
detection (VAD), feature extraction and channel compensation techniques;</p>
<ol class="simple">
<li><p><em><span class="xref myst">Voice activity detection (VAD)</span></em>;
The main goal of voice activity detection is to determine which
segments of a signal are speech and non-speech. A robust VAD
algorithm can improve the performance of a speaker verification
system by making sure that speaker identity is calculated only from
speech regions. Therefore, it is necessary to review the VAD
algorithm to overcome the problems in designing a robust speaker
verification system. The three widely used techniques for VAD are
the following: energy based, model based and hybrid approaches.</p></li>
<li><p>Feature extraction techniques are used to transform the speech
signals into acoustic feature vectors. Thus, the extracted acoustic
features should carry the essential characteristics of the speech
signal which recognizes the identity of the speaker by their voice.
The aim of feature extraction is to reduce the dimension of acoustic
feature vectors by removing unwanted information and emphasizing the
speaker-specific information. The MFCCs are commonly used as the
feature extraction technique for the modern speaker verification.</p></li>
<li><p>Channel compensation techniques are used to reduce the effect of
channel mismatch and environmental noise. Channel compensation can
be used in different stages of speaker verification such as feature
and model domains. Various channel compensation techniques such as
cepstral mean subtraction (CMS) , feature warping, cepstral mean
variance normalization (CMVN)  and relative spectral (RASTA)
processing have been used to reduce the effect of channel mismatch
during the feature extraction phase. In the model domain, Joint
Factor Analysis (JFA) and i-vectors are used to combat enrolment and
verification mismatch.</p></li>
</ol>
</div>
<div class="section" id="speaker-modeling-techniques">
<h2><span class="section-number">8.4.3. </span>3. Speaker Modeling Techniques<a class="headerlink" href="#speaker-modeling-techniques" title="Permalink to this headline">¶</a></h2>
<p>One of the crucial issues in speaker diarization is the techniques
employed for speaker modeling. Several modeling techniques have been
used in speaker recognition and speaker diarization tasks. The
state-of-the-art speaker modeling techniques in speaker diarization are
the following:</p>
</div>
<div class="section" id="gaussian-mixture-modeling-gmm-universal-background-model-ubm-approach">
<h2><span class="section-number">8.4.4. </span>3.1. Gaussian Mixture Modeling (GMM) - Universal Background Model (UBM) Approach<a class="headerlink" href="#gaussian-mixture-modeling-gmm-universal-background-model-ubm-approach" title="Permalink to this headline">¶</a></h2>
<p>A Gaussian Mixture Model (GMM) is a parametric probability density
function represented as a weighted sum of Gaussian component densities.
GMMs have been successfully used to model the speech features in
different speech processing applications. A Gaussian mixture model is a
weighted sum of M component Gaussian densities. Each of the components
is a multi-variant Gaussian function. A GMM is represented by mean
vectors, covariance matrices and mixture weights.</p>
<p>\[ \lambda = \{w_i, \mu_i, \Sigma_i\}, \quad i=
1,…….,\textit{C} \]</p>
<p>The covariance matrices of a GMM,  \( \Sigma_i \) , can be full rank
or constrained to be diagonal. The parameters of a GMM can also be
shared, or tied, among the Gaussian components. The number of GMM
components and type of covariance matrices are often determined based on
the amount of data available for estimating GMM parameters.</p>
<p>In speaker recognition, a speaker can be modeled by a GMM from training
data or using Maximum A Posteriori (MAP) adaptation. While the speaker
model is built using the training utterances of a specific speaker in
the GMM training, the model is also usually adapted from a large number
of speakers called Universal Background Model in MAP adaptation.</p>
<p>Given a set of training vectors and a GMM configuration, there are
several techniques available for estimating the parameters of a GMM .
The most popular and used method is the maximum likelihood (ML)
estimation.</p>
<p>The ML estimation finds the model parameters that maximize the
likelihood of the GMM given a set of data. Assuming an independence
between the training vectors \( X = \{x_i,\dots,x_N\} \) , the GMM
likelihood is typically described as:</p>
<p>\[ p(X|\lambda) = \prod_{t=1}^{N} p(x_t|\lambda) (1) \]</p>
<p>Since direct maximization is not possible on equation on equation 1, the
ML parameters are obtained iteratively using expectation-maximization
(EM) algorithm.  The EM iteratively estimate new model parameters  \(
\bar{\lambda} \)  based on a given model <span class="math notranslate nohighlight">\(\\lambda\)</span> such that \(
p(X|\bar{\lambda}) \ge p(X|\lambda) \) .</p>
<p><img src="attachments/165125114/165126354.png" class="image-center"
data-image-src="attachments/165125114/165126354.png"
data-unresolved-comment-count="0" data-linked-resource-id="165126354"
data-linked-resource-version="1" data-linked-resource-type="attachment"
data-linked-resource-default-alias="image2020-1-20_15-34-38.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="165125114"
data-linked-resource-container-version="99" height="250" />Example of
speaker model adaptation</p>
<p>The parameters of a GMM can also be estimated using Maximum A Posteriori
(MAP) estimation, in addition to the EM algorithm. The MAP estimation
technique derives a speaker model by adapting from a universal
background model (UBM). The “Expectation” step of EM and MAP are the
same. MAP adapts the new sufficient statistics by combining them with
old statistics from the prior mixture parameters.</p>
<p>Given a prior model and training vectors from the desired class, \( X =
{x_1 … , x_T } \) , we first determine the probabilistic alignment
of the training vectors into the prior mixture components. For mixture 
\( i \) in the prior model  \( Pr(i|x_t,\lambda_{UBM}) \) is
computed as the percentage of the mixture component  \( i \) to the
total likelihood,</p>
<p>\[ Pr(i|x_t,\lambda_{UBM})= \frac {w_i\,g(x_t|\mu_i,\Sigma_i)}
{\sum_{i=1}^{M} w_i\,g(x_t|\mu_i,\Sigma_i)} \]</p>
<p>Then, the sufficient statistics for the weight, mean and variance
parameters is computed as follows:</p>
<p>\[ n_i=\sum_{t=1}^{T}Pr(i|x_t,\lambda_{prior}) \, weight \]
\[
E_i(x)=\frac{1}{n_i}\sum_{t=1}^{T}Pr(i|x_t,\lambda_{prior})x_t
\;\; mean \] \[
E_i(x^2)=\frac{1}{n_i}\sum_{t=1}^{T}Pr(i|x_t,\lambda_{prior})x_t^2
\;\; variance \]</p>
<p>Finally, the new sufficient statistics from the training data are used
to update the prior sufficient statistics for mixture  \( i \) to
create the adapted mixture weight, mean and variance for mixture
\textit{i} as follows:</p>
<p>\[ w_i=[\alpha^w_in_i/T + (1-\alpha^w_i)w_i]\gamma \] \[
\mu_i=\alpha^m_iE_i(x) + (1-\alpha^m_i)\mu_i \] \[
\mu^2_i=\alpha^v_iE_i(x^2) +
(1-\alpha^v_i)(\sigma^2_i+\mu^2_i)-\mu^2_i \]</p>
<p>The adaptation coefficients controlling the balance between old and new
estimates are  \( \{\alpha^w_i, \alpha^m_i, \alpha^v_i\} \) for
the weights, means and variances, respectively. The scale factor, \(
\gamma \) , is computed over all adapted mixture weights to ensure
they sum to unity.</p>
</div>
<div class="section" id="i-vectors">
<h2><span class="section-number">8.4.5. </span>3.2. i-Vectors<a class="headerlink" href="#i-vectors" title="Permalink to this headline">¶</a></h2>
<p>Different approaches have been developed recently to improve the
performance of speaker recognition systems. The most popular ones were
based on GMM-UBM. The Joint Factor Analysis (JFA) is then built on the
success of the GMM-UBM approach. JFA modeling defines two distinct
spaces: the speaker space defined by the eigenvoice matrix and the
channel space represented by the eigen-channel matrix. The channel
factors estimated using JFA, which are supposed to model only channel
effects, also contain information about speakers. A new speaker
verification system has been proposed using factor analysis as a feature
extractor that defines only a single space, instead of two separate
spaces. In this new space, a given speech recording is represented by a
new vector, called total factors as it contains the speaker and channel
variabilities simultaneously. Speaker recognition based on the i-vector
framework is currently the state-of-the-art in the field.</p>
<p>Given an utterance, the speaker and channel dependent GMM supervector is
defined as follows:</p>
<p>\[ M=m+Tw \]</p>
<p>where  \( m \) is a speaker and channel independent supervector,  \(
T \) is a rectangular matrix of low rank and  \( w \) is a random
vector having a standard normal distribution \( N(0,1) \) . The
components of the vector  \( w \) are the total factors. These new
vectors are called i-vectors.  \( M \) is assumed to be normally
distributed with mean vector and covariance matrix \( TT^t \) .</p>
<p>The total factor is a hidden variable, which can be defined by its
posterior distribution conditioned to the Baum–Welch statistics for a
given utterance. This posterior distribution is a Gaussian distribution
and the mean of this distribution corresponds exactly to i-vector. The
Baum–Welch statistics are extracted using the UBM.</p>
<p>Given a sequence of L frames  \( \{y_1,y_2,……,y_n\} \) and a
UBM  \( \Omega \) composed of \( C \) mixture components defined in
some feature space of dimension \( F \) , the Baum–Welch statistics
needed to estimate the i-vector for a given speech utterance  \( u \)
is given by :</p>
<p>\[ N_c=\sum_{t=1}^{L}P(c|y_t,\Omega) \] \[
F_c=\sum_{t=1}^{L}P(c|y_t,\Omega)y_t \]</p>
<p>where  \( m_c \) is the mean of UBM mixture component \( c \) .
The i-vector for a given utterance can be obtained using the following
equation:</p>
<p>\[ w=(I + T^t \Sigma^{-1} N(u)T)^{-1}. \,T^t\Sigma^{-1}\hat{F}(u)
\]</p>
<p>where  \( N_u \) is a diagonal matrix of dimension  \( CF \times CF
\) whose diagonal blocks are \( N_cI(c=1,……, C) \) . The
supervector obtained by concatenating all first-order Baum–Welch
statistics \( F_c \) for a given utterance  \( u \) is represented
by  \( \hat{F}(u) \) which has  \( CF \times 1 \) dimension. The
diagonal covariance matrix,  \( \Sigma \) , with dimension \( CF
\times CF \) estimated during factor analysis training models the
residual variability not captured by the total variability matrix \( T
\) .</p>
<p><img src="attachments/165125114/165126497.png" class="image-center"
data-image-src="attachments/165125114/165126497.png"
data-unresolved-comment-count="0" data-linked-resource-id="165126497"
data-linked-resource-version="1" data-linked-resource-type="attachment"
data-linked-resource-default-alias="image2020-1-20_20-26-53.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="165125114"
data-linked-resource-container-version="99" height="250" />Process of
i-Vector extraction</p>
<p>One of the most widely used feature normalization techniques of
i-vectors is length normalization. Length normalization ensures that the
distribution of i-vectors matches the Gaussian normal distribution and
makes the distributions of i-vector more similar. Performing whitening
before length normalization improves the performance of speaker
verification systems.  i-vector normalization improves the gaussianity
of the i-vectors and reduces the gap between the underlying assumptions
of the data and real distributions. It also reduces the dataset shift
between development and test i-vectors.</p>
<p>\[ w\leftarrow
\frac{\Sigma^{-\frac{1}{2}}(w-\mu)}{||\Sigma^{-\frac{1}{2}}(w-\mu)||}
\]</p>
<p>where  \( \mu \) and  \( \Sigma \) are the mean and the covariance
matrix of a training corpus, respectively. The data is standardized
according to  covariance matrix  \( \Sigma \) and length-normalized
(i.e., the i-vectors are confined to the hypersphere of unit radius.</p>
<p>The two most widely and common intersession compensation techniques of
i-vectors are Within-Class Covariance Normalization (WCCN) and Linear
Discriminant Analysis (LDA). WCCN uses the within-class covariance
matrix to normalize the cosine kernel functions in order to compensate
for intersession variability. LDA attempts to define a reduced special
axes that minimize the within-speaker variability caused by channel
effects, and maximize between-speaker variability.</p>
<p><strong>Cosine Distance</strong></p>
<p>Once the i-vectors are extracted from the outputs of speech clusters,
cosine distance scoring tests the hypothesis if two i-vectors belong to
the same speaker or different speakers. Given two i-vectors, the cosine
distance among them is calculated as follows:</p>
<p>\[ cos(w_i,w_j)=\frac{w_i.w_j}{||w_i||.||w_j||}\gtreqless
\theta \]</p>
<p>where  \( \theta \) is the threshold value, and  \( cos(w_i,w_j) \)
is the cosine distance score between clusters  \( i \) and \( j \) .
The corresponding i-vectors extracted for clusters  \( i \) and  \( j
\) are represented by  \( w_i \) and \( w_j \) , respectively.</p>
<p>The cosine distance scoring considers only the angle between two
i-vectors, not their magnitude. Since the non-speaker information such
as session and channel variabilities affect the i-vector magnitude,
removing the magnitudes can increase the robustness of i-vector systems.</p>
<p><strong>Probabilistic Linear Discriminant Analysis</strong></p>
<p>The i-vector representation followed by probabilistic linear
discriminant analysis (PLDA) modeling technique is the state-of-the-art
in speaker verification systems. PLDA has been successfully applied in
speaker recognition experiments. It is also applied to handle speaker
and session variability in speaker verification task. It has also been
successfully applied in speaker clustering since it can separate speaker
and noise specific parts of an audio signal which is essential for
speaker diarization.</p>
<p><img src="attachments/165125114/165126539.png" class="image-center"
data-image-src="attachments/165125114/165126539.png"
data-unresolved-comment-count="0" data-linked-resource-id="165126539"
data-linked-resource-version="1" data-linked-resource-type="attachment"
data-linked-resource-default-alias="image2020-1-20_21-46-46.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="165125114"
data-linked-resource-container-version="99" height="250" /></p>
<p>Example of PLDA model</p>
<p>In PLDA, assuming that the training data consists of \textit{J}
i-vectors where each of these i-vectors belong to speaker \textit{I},
the \textit{j’th} i-vector of the \textit{I’th} speaker is denoted by:</p>
<p>\[ w_{ij}=\mu + Fh_i + Gy_{ij} + \Sigma_{ij} \]</p>
<p>where  \( \mu \)  is the overall speaker and segment independent mean
of the i-vectors in the training dataset, columns of the matrix F define
the between-speaker variability and columns of the matrix G define the
basis for the within-speaker variability subspace.  \( \Sigma_{ij}
\) represents any unexplained data variation. The components of the
vector  \( h_i \) are the eigenvoice factor loadings and components of
the vector  \( y_{ij} \) are the eigen-channel factor loadings. The
term  \( Fh_i \) depends only on the identity of the speaker, not on
the particular segment.</p>
<p>Although the PLDA model assumes Gaussian behavior, there is empirical
evidence that channel and speaker effects result in i-vectors that are
non-Gaussian.  It is reported in that the use of Student’s
t-distribution, on the assumed Gaussian PLDA model, improves the
performance. Since this normalization technique is complicated, a
non-linear transformation of i-vectors called radial Gaussianization has
been proposed. It whitens the i-vectors and performs length
normalization. This restores the Gaussian assumptions of the PLDA model.</p>
<p>A variant of PLDA model called Gaussian PLDA (GPLDA) is shown to provide
better results.  Because of its low computational requirements, and its
performance, it is the most widely used PLDA modeling. In GPLDA model,
the within-speaker variability is modeled by a full covariance residual
term  which allows us to omit the channel subspace. The generative PLDA
model for the i-vector is  represented by</p>
<p>\[ w_{ij}=\mu + Fh_i + \Sigma_{ij} \]</p>
<p>The residual term  \( \Sigma \) representing the within-speaker
variability is assumed to have a normal distribution with full
covariance matrix \( \Sigma \) .</p>
<p>Given two i-vectors  \( w_1 \) and \( w_1 \) , the PLDA computes the
likelihood ratio of the two i-vectors as follows:</p>
<p>\[ Score(w_1,w_2)= \frac{p(w_1,w_2|H_1)}{p(w_1|H_2) p(w_2|H_2)}
\]</p>
<p>where the hypothesis  \( H_1 \) indicates that both i-vectors belong
to the same speaker and  \( H_0 \) indicates they belong to two
different speakers.</p>
</div>
<div class="section" id="deep-learning-dl">
<h2><span class="section-number">8.4.6. </span>3.3.  Deep Learning (DL)<a class="headerlink" href="#deep-learning-dl" title="Permalink to this headline">¶</a></h2>
<p>The recent advances in computing hardware, new DL architectures and
training methods, and access to large amount of training data has
inspired the research community to make use of DL technology again as in
speaker recognition systems.  DL techniques can be used in the frontend
or/and backend of a speaker recognition system. The whole end-to-end
recognition process can even be performed by a DL architecture.</p>
<p>**Deep Learning Frontends: **The traditional i-vector approach consists
of mainly three stages: Baum-Welch statistics collection, i-vector
extraction, and PLDA backend. Recently, it is shown that if the
Baum-Welch statistics are computed with respect to a DNN rather than a
GMM or if bottleneck features are used in addition to conventional
spectral features, a substantial improvement can be achieved. Another
possible use of DL in the frontend is to represent the speaker
characteristics of a speech signal with a single low dimensional vector
using a DL architecture, rather than the traditional i-vector algorithm.
These vectors are often referred to as speaker embeddings. Typically,
the inputs of the neural network are a sequence of feature vectors and
the outputs are speaker classes.</p>
<p>**Deep Learning Backends: **One of the most effective backend techniques
for i-vectors is PLDA which performs the scoring along with the session
variability compensation. Usually, a large number of different speakers
with several speech samples each are necessary for PLDA to work
efficiently. Access to the speaker labeled data is costly and in some
cases almost impossible. Moreover, the amount of the performance gain,
in terms of accuracy, for short utterances is not as much as that for
long utterances. These facts motivated the research community to
look for DL based alternative backends. Several techniques have been
proposed. Most of these approaches use the speaker labels of the
background data for training, as in PLDA, and mostly with no significant
gain compared to PLDA.</p>
<p>**Deep Learning End-to-Ends: **It is also interesting to train an
end-to-end recognition system capable of doing multiple stages of signal
processing with a unified DL architecture. The neural network will be
responsible for the whole process from the feature extraction to the
final similarity scores. However, working directly on the audio signals
in the time domain is still computationally too expensive and,
therefore, the current end-to-end DL systems take mainly the handcrafted
feature vectors, e.g., MFCCs, as inputs. Recently, there have been
several attempts to build an end-to-end speaker recognition system using
DL though most of them focus on text-dependent speaker recognition.</p>
</div>
<div class="section" id="applications-of-speaker-recognition">
<h2><span class="section-number">8.4.7. </span>4. Applications of Speaker Recognition<a class="headerlink" href="#applications-of-speaker-recognition" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Transaction authentication – Toll fraud prevention, telephone credit
card purchases, telephone brokerage (e.g., stock trading)</p></li>
<li><p>Access control – Physical facilities, computers and data networks</p></li>
<li><p>Monitoring – Remote time and attendance logging, home parole
verification, prison telephone usage</p></li>
<li><p>Information retrieval – Customer information for call centers, audio
indexing (speech skimming device), speaker diarization</p></li>
<li><p>Forensics – Voice sample matching</p></li>
</ul>
</div>
<div class="section" id="performance-evaluations">
<h2><span class="section-number">8.4.8. </span>5. Performance Evaluations<a class="headerlink" href="#performance-evaluations" title="Permalink to this headline">¶</a></h2>
<p>The performance of the speaker verification is measured in terms of
errors. The types of error and evaluation metrics commonly used in
speaker verification systems are the following.</p>
</div>
<div class="section" id="types-of-errors">
<h2><span class="section-number">8.4.9. </span>5.1 Types of errors<a class="headerlink" href="#types-of-errors" title="Permalink to this headline">¶</a></h2>
<p>False acceptance: A false acceptance occurs when the speech segments
from an imposter speaker are falsely accepted as a target speaker by the
system.</p>
<p>\[ False\; Acceptance = \frac{Total \; number \;of\; false\;
acceptance\; errors} {Total\; number\; of\; imposter\; speaker\;
attempts} \]</p>
<p>**<br />
**</p>
<p><strong>False rejection:</strong> A false rejection occurs when the target speaker is
rejected by the verification systems.</p>
<p>\[ False\;rejection = \frac{Total \;number\; of\; false\;
rejection\; errors} {Total\; number\; of\; enrolled\; speaker\;
attempts} \]</p>
</div>
<div class="section" id="performance-metrics">
<h2><span class="section-number">8.4.10. </span>5.2 Performance metrics<a class="headerlink" href="#performance-metrics" title="Permalink to this headline">¶</a></h2>
<p>The performance metrics of speaker verification systems can be measured
using the equal error rate (EER) and minimum decision cost function
(mDCF). These measures represent different performance characteristics
of system though the accuracy of the measurements is based on the number
of trials evaluated in order to robustly compute the relevant
statistics. Speaker verification performance can also be represented
graphically by using the detection error trade-off (DET) plot. The EER
is obtained when the false acceptance rate and false rejection rate have
the same value. The performance of the system improves if the value of
ERR is lower because the sum of total error of the false acceptance and
false rejection at the point of ERR decreases. The decision cost
function (DCF) is defined by assigning a cost of each error and taking
into account the prior probability of target and impostor trails. The
decision cost function is defined as:</p>
<p>\[ DCF = C_{miss}P_{miss}P_{target} + C_{fa}P_{fa}P_{impostor}
\]</p>
<p>where  \( C_{miss} \) and  \( C_{fa} \) are the cost functions of
a missed detection and false alarm, respectively. The prior
probabilities of target and impostor trails are given by  \(
P_{target} \) and   \( P_{impostor} \) , respectively. The
percentages of the missed target and falsely accepted impostors’ trails
are represented by  \( P_{miss} \) and \( P_{fa} \) ,
respectively. The mDCF is used to evaluate speaker verification by
selecting the minimum value of DCF estimated by changing the threshold
value. The mDCF can be used to evaluate speaker verification by
selecting the minimum value of DCF estimated by changing the threshold
value.</p>
<p>\[ mDCF = min[C_{miss}P_{miss}P_{target} +
C_{fa}P_{fa}P_{impostor}] \]</p>
<p>where  \( P_{miss} \) and  \( P_{fa} \) are the miss and false
alarm rates recorded from the trials, and the other parameters are
adjusted to suit the evaluation of application-specific requirements.</p>
<div class="pageSectionHeader">
</div>
<div class="section" id="attachments">
<h2><span class="section-number">8.4.11. </span>Attachments:<a class="headerlink" href="#attachments" title="Permalink to this headline">¶</a></h2>
</div>
<div class="greybox" align="left">
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[image2020-1-20_15-34-38.png](attachments/165125114/165126354.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[image2020-1-20_20-26-53.png](attachments/165125114/165126497.png)
(image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[image2020-1-20_21-46-46.png](attachments/165125114/165126539.png)
(image/png)  
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Recognition"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Speech_Recognition.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">8.3. </span>Speech Recognition</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Speaker_Diarization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8.5. </span>Speaker Diarization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>