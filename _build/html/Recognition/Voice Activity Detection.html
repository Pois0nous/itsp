
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice Activity Detection (VAD) &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Representations/Representations.html">
   3. Basic Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_analysis.html">
     3.1. Short-time analysis of speech and audio signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Short-time_processing.html">
     3.2. Short-time processing of speech signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Waveform.html">
     3.3. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Windowing.html">
     3.4. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Signal_energy_loudness_and_decibel.html">
     3.5. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Spectrogram_and_the_STFT.html">
     3.6. Spectrogram and the STFT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Autocorrelation_and_autocovariance.html">
     3.7. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Melcepstrum.html">
     3.8. The cepstrum, mel-cepstrum and mel-frequency cepstral coefficients (MFCCs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Linear_prediction.html">
     3.9. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Fundamental_frequency_F0.html">
     3.10. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Zero-crossing_rate.html">
     3.11. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Deltas_and_Delta-deltas.html">
     3.12. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.13. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Jitter_and_shimmer.html">
     3.14. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Analysis/Measurements_for_medical_applications.html">
     7.2. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
       7.2.1. Inverse filtering for glottal activity estimation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Forensic_analysis.html">
     7.3. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Wake-word_and_keyword_spotting.html">
     8.1. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speech_Recognition.html">
     8.2. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speaker_Recognition_and_Verification.html">
     8.3. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speaker_Diarization.html">
     8.4. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Paralinguistic_speech_processing.html">
     8.5. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Concatenative_speech_synthesis.html">
     9.2. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Statistical_parametric_speech_synthesis.html">
     9.3. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Basic_tools.html">
     10.2. Basic tools
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.3. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Entropy_coding.html">
       10.3.4. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.3.5. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Code-excited_linear_prediction_CELP.html">
     10.4. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Frequency-domain_coding.html">
     10.5. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://landbot.io/blog/guide-to-conversational-design/">
   Chatbots / Conversational design (external link)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Security_and_privacy_in_speech_technology.html">
   13. Security and privacy in speech technology
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Recognition/Voice Activity Detection.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/Recognition/Voice Activity Detection.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#low-noise-vad-trivial-case">
   Low-noise VAD = Trivial case
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vad-objective-and-performance-measurement">
   VAD objective and performance measurement
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-in-noise-3db-4db-threshold">
     Performance in noise – -3dB / -4dB threshold
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#post-processing">
   Post-processing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vad-for-noisy-speech">
   VAD for noisy speech
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#features">
     Features
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classifier">
   Classifier
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-trees-historical">
     Decision trees (historical)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-classifier">
     Linear classifier
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-bit-of-math">
       A bit of math
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pre-whitening-advanced-topic">
       Pre-whitening (advanced topic)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Post-processing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-advanced-classifiers">
     More advanced classifiers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speech-presence-probability">
   Speech Presence Probability
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#output-before-thresholding">
     Output before thresholding
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#noise-types">
   Noise types
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   Conclusions
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Voice Activity Detection (VAD)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#low-noise-vad-trivial-case">
   Low-noise VAD = Trivial case
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vad-objective-and-performance-measurement">
   VAD objective and performance measurement
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-in-noise-3db-4db-threshold">
     Performance in noise – -3dB / -4dB threshold
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#post-processing">
   Post-processing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vad-for-noisy-speech">
   VAD for noisy speech
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#features">
     Features
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classifier">
   Classifier
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-trees-historical">
     Decision trees (historical)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-classifier">
     Linear classifier
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-bit-of-math">
       A bit of math
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pre-whitening-advanced-topic">
       Pre-whitening (advanced topic)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Post-processing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-advanced-classifiers">
     More advanced classifiers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speech-presence-probability">
   Speech Presence Probability
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#output-before-thresholding">
     Output before thresholding
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#noise-types">
   Noise types
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   Conclusions
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="voice-activity-detection-vad">
<h1>Voice Activity Detection (VAD)<a class="headerlink" href="#voice-activity-detection-vad" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p><img alt="vadspp" src="../_images/vad_spp-1.png" /></p>
<ul class="simple">
<li><p><em>Voice activity detection</em> (VAD) (or speech activity detection, or
speech detection) refers to a class of methods which detect whether
a sound signal contains speech or not.</p></li>
<li><p>A closely related and partly overlapping task is <em>speech presence
probability</em> (SPP) estimation.</p>
<ul>
<li><p>Instead of a binary present/not-present decision, SPP gives a
probability level that the signal contains speech.</p></li>
<li><p>A VAD can be derived from SPP by setting a threshold probability
above which the signal is considered to contain speech.</p></li>
<li><p>In most cases, SPP is thus the more fundamental problem.</p></li>
</ul>
</li>
<li><p>Voice activity detection is used as a pre-processing algorithm for
almost all other speech processing methods.</p>
<ul>
<li><p>In <em>speech coding</em>, it is used to to determine when speech
transmission can be switched off to reduce the amount of
transmitted data.</p></li>
<li><p>In <em>speech recognition</em>, it is used to find out what parts of
the signal should be fed to the recognition engine. Since
recognition is a computationally complex operation, ignoring
non-speech parts saves CPU power.</p></li>
</ul>
</li>
<li><p>VAD or SPP is thus used mostly as a resource-saving operation.</p></li>
<li><p>In <em>speech enhancement</em>, where we want to reduce or remove noise in
a speech signal, we can estimate noise characteristics from
non-speech parts (learn/adapt) and remove noise from the speech
parts (apply).</p></li>
<li><p>A closely related method in <em>audio</em> applications is <em>noise gateing</em>,
where typically a microphone signal is muted whenever there is no
signal present.</p>
<ul>
<li><p>For example, when a singer is not singing in the microphone,
then the microphone is off. When the singer is not singing,
microphone signal is only noise and therefore the noise gate
removes (gates) noise.</p></li>
</ul>
</li>
<li><p>VADs can thus also be used in improving signal quality.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialization</span>
<span class="c1">#import sounddevice as sd</span>
<span class="kn">import</span> <span class="nn">IPython.display</span> <span class="k">as</span> <span class="nn">ipd</span>
<span class="kn">from</span> <span class="nn">scipy.io</span> <span class="kn">import</span> <span class="n">wavfile</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1">#from scipy import signal</span>
<span class="c1">#from scipy import linalg</span>


<span class="n">fs</span> <span class="o">=</span> <span class="mi">48000</span>

<span class="c1"># Define windowing</span>
<span class="n">window_length_ms</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">window_step_ms</span> <span class="o">=</span> <span class="n">window_length_ms</span><span class="o">/</span><span class="mi">2</span>
<span class="n">window_step</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">fs</span><span class="o">*</span><span class="n">window_step_ms</span><span class="o">/</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">window_length</span> <span class="o">=</span> <span class="n">window_step</span><span class="o">*</span><span class="mi">2</span>
<span class="n">spectrum_length</span> <span class="o">=</span> <span class="n">window_length</span><span class="o">//</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># Network parameters</span>
<span class="n">past_frames_lookback</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">hidden_layer_size</span> <span class="o">=</span> <span class="n">spectrum_length</span><span class="o">*</span><span class="mi">2</span>
<span class="n">bottleneck_size</span> <span class="o">=</span> <span class="mi">30</span>


<span class="k">def</span> <span class="nf">stft</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">window_length</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mi">1440</span><span class="p">),</span><span class="n">window_step</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mi">1440</span><span class="o">/</span><span class="mi">2</span><span class="p">)):</span>
    <span class="n">windowing_fn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="n">window_length</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="n">window_length</span><span class="p">)</span><span class="o">/</span><span class="n">window_length</span><span class="p">)</span> <span class="c1"># half-sine window</span>
    <span class="n">window_count</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">window_length</span><span class="p">)</span><span class="o">/</span><span class="n">window_step</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">spectrogram_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">window_length</span><span class="p">,</span><span class="n">window_count</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">complex</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">window_ix</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">window_count</span><span class="p">):</span>    
        <span class="n">data_frame</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">windowing_fn</span><span class="p">,</span><span class="n">data</span><span class="p">[</span><span class="n">window_ix</span><span class="o">*</span><span class="n">window_step</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">window_length</span><span class="p">)])</span>
        <span class="n">spectrogram_matrix</span><span class="p">[:,</span><span class="n">window_ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">data_frame</span><span class="p">)</span>
    <span class="n">spectrum_length</span> <span class="o">=</span> <span class="n">window_length</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> 
    <span class="k">return</span> <span class="n">spectrogram_matrix</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">spectrum_length</span><span class="p">),:]</span>

<span class="k">def</span> <span class="nf">istft</span><span class="p">(</span><span class="n">spectrogram</span><span class="p">):</span>
    <span class="n">window_count</span> <span class="o">=</span> <span class="n">spectrogram</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">spectrum_length</span> <span class="o">=</span> <span class="n">spectrogram</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">window_step</span> <span class="o">=</span> <span class="n">spectrum_length</span><span class="o">-</span><span class="mi">1</span>
    <span class="n">window_length</span> <span class="o">=</span> <span class="n">window_step</span><span class="o">*</span><span class="mi">2</span>
    <span class="n">data_length</span> <span class="o">=</span> <span class="p">(</span><span class="n">window_count</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">window_step</span> <span class="o">+</span> <span class="n">window_length</span>
    <span class="n">windowing_fn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="n">window_length</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="n">window_length</span><span class="p">)</span><span class="o">/</span><span class="n">window_length</span><span class="p">)</span> <span class="c1"># half-sine window</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">data_length</span><span class="p">])</span>
    <span class="n">backward_index</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">spectrum_length</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">window_ix</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">window_count</span><span class="p">):</span>
        <span class="n">full_spectrum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">spectrogram</span><span class="p">[:,</span><span class="n">window_ix</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">conjugate</span><span class="p">(</span><span class="n">spectrogram</span><span class="p">[</span><span class="n">backward_index</span><span class="p">,</span><span class="n">window_ix</span><span class="p">])))</span>
        <span class="n">data_window</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">ifft</span><span class="p">(</span><span class="n">full_spectrum</span><span class="p">)</span> <span class="o">*</span> <span class="n">windowing_fn</span>
        <span class="n">data</span><span class="p">[</span><span class="n">window_ix</span><span class="o">*</span><span class="n">window_step</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">window_length</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">data_window</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="low-noise-vad-trivial-case">
<h2>Low-noise VAD = Trivial case<a class="headerlink" href="#low-noise-vad-trivial-case" title="Permalink to this headline">¶</a></h2>
<p><img alt="vadsad" src="../_images/vad_sad-1.png" /></p>
<ul class="simple">
<li><p>To introduce basic vocabulary and methodology, let us consider a
case where a speaker is speaking in an (otherwise) silent
environment.</p>
<ul>
<li><p>When there is no speech, there is silence.</p></li>
<li><p>(Any) Signal activity indicates voice activity.</p></li>
</ul>
</li>
<li><p>Signal activity can be measured by, for example, estimating signal
energy per frame <span class="math notranslate nohighlight">\(\Rightarrow\)</span> the <em>energy thresholding algorithm</em>.</p></li>
</ul>
<p><img alt="vadene" src="../_images/vad_ene-1.png" /></p>
<ul class="simple">
<li><p>Clearly energy thresholding works for silent speech signals.</p>
<ul>
<li><p>Low-energy frames are correctly labeled as non-speech and speech
parts are likewise correctly labeled.</p></li>
</ul>
</li>
<li><p>It is however not trivial to choose an appropriate threshold-level.</p>
<ul>
<li><p>A low threshold level would make sure that all speech-frames are
correctly labeled. However, we might then also label frames with
other sounds, like breathing sounds or other background noises,
as speech frames.</p></li>
<li><p>A high threshold would make sure that all detected speech-frames
actually are truly speech frames. But then we could miss offsets
(sounds which are trailing off), since they often have a low
energy.</p></li>
</ul>
</li>
<li><p>What strategy should we use to choose a threshold?</p>
<ul>
<li><p>What is the correct label for something like breathing-noises?</p></li>
<li><p>How do we actually measure performance of a VAD?</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="vad-objective-and-performance-measurement">
<h2>VAD objective and performance measurement<a class="headerlink" href="#vad-objective-and-performance-measurement" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>The objective of a VAD implementation depends heavily on the
application.</p>
<ul>
<li><p>In speech coding, our actual objective is to reduce bitrate
without decreasing quality. <span class="math notranslate nohighlight">\(\Rightarrow\)</span> We want to make sure
that no speech frames are classified as background noise,
because that would reduce quality.</p>
<p><span class="math notranslate nohighlight">\(\Rightarrow\)</span> We make a conservative estimate.</p>
</li>
<li><p>In keyword spotting (think “Siri” or “OK Google”), we want to
detect the start of a particular combination of words. The VADs
task is to avoid running a computationally expensive keyword
spotting algorithm all the time. Missing one keyword is not so
bad (the user would then just try again), but if it is too
sensitive then the application would drain the battery.</p>
<p><span class="math notranslate nohighlight">\(\Rightarrow\)</span> We want to be sure that only keywords are spotted.</p>
</li>
</ul>
</li>
<li><p>The objective of a VAD implementation depends heavily on the
application.</p>
<ul class="simple">
<li><p>In speech enhancement, we want to find non-speech areas such
that we can there estimate noise characteristics, such that we
can remove anything which looks like noise. We want to be sure
that there is no speech in the noise estimate, otherwise we
would end up removing some speech and not only noise.</p></li>
<li><p>In speech recognition, VAD is used purely for resource saving.
We do not want to reduce accuracy of the recognition, but want
to minimize CPU usage.</p></li>
</ul>
</li>
<li><p>We need a set of performance measures which reflect these different
objectives.</p></li>
<li><p>The performance is then often described by looking at how often are
frames which do contain speech labeled as speech/non-speech, and how
often is non-speech labeled as speech/non-speech?</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Input</p></th>
<th class="head"><p>Speech</p></th>
<th class="head"><p>Non-speech</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Speech</p></td>
<td><p>True positive</p></td>
<td><p>False negative</p></td>
</tr>
<tr class="row-odd"><td><p>Non-speech</p></td>
<td><p>False positive</p></td>
<td><p>True negative</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>For speech coding, we want to keep the number of false negatives low, and false positives are of only secondary importance.</p></li>
<li><p>For keyword spotting, we want to keep the number of false positives low, and false negatives are secondary importance.</p></li>
</ul>
<div class="section" id="performance-in-noise-3db-4db-threshold">
<h3>Performance in noise – -3dB / -4dB threshold<a class="headerlink" href="#performance-in-noise-3db-4db-threshold" title="Permalink to this headline">¶</a></h3>
<p><img alt="vadene3" src="../_images/vad_ene_noise3-1.png" />
<img alt="vadene4" src="../_images/vad_ene_noise4-1.png" /></p>
</div>
</div>
<div class="section" id="post-processing">
<h2>Post-processing<a class="headerlink" href="#post-processing" title="Permalink to this headline">¶</a></h2>
<p><img alt="vadhyst" src="../_images/vad_ene_hyst-1.png" /></p>
<ul class="simple">
<li><p>We already saw that speech coding wants to avoid false negatives
(=speech frames labeled as non-speech).</p></li>
<li><p>Can we identify typical situations where false negatives occur?</p>
<ul>
<li><p>Offsets (where a phonation ends) often have low energy<br />
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> Easily misclassified as non-speech.</p></li>
<li><p>Stops have a silence in the middle of an utterance.<br />
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> Easily misclassified as non-speech.</p></li>
</ul>
</li>
<li><p>We should be careful at the end of phonations.</p>
<ul>
<li><p>We can use a <em>hangover</em> time, such that after a speech segment
we keep the label as speech for a while until we are sure that
speech has ended.</p></li>
<li><p>For onsets (starts of phonemes) we usually want to be very
sensitive.</p></li>
</ul>
</li>
<li><p>We obtain a hysteresis rule;</p>
<ul>
<li><p>If any of the last <span class="math notranslate nohighlight">\(K\)</span> frames was identified as speech, then the
current frame is labelled as speech. Otherwise non-speech.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="vad-for-noisy-speech">
<h2>VAD for noisy speech<a class="headerlink" href="#vad-for-noisy-speech" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Clean speech (absolutely no background noise) is very rare if not
impossible to achieve.</p></li>
<li><p>Real-life speech recordings practically always have varying amounts
of background noise.</p>
<ul>
<li><p>Performance of energy thresholding decreases rapidly when the
SNR drops.</p></li>
<li><p>For example, weak offsets easily disappear in noise.</p></li>
</ul>
</li>
<li><p>We need more advanced VAD methods for noisy speech.</p>
<ul>
<li><p>We need to identify characteristics which differentiate between
speech and noise.</p></li>
<li><p>Measures for such characteristics are known as <em>features</em>.</p></li>
</ul>
</li>
</ul>
<div class="section" id="features">
<h3>Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h3>
<p><img alt="vadfeat" src="../_images/vad_features1-1.png" /></p>
<ul class="simple">
<li><p>In VAD, with features we try to measure some property of the signal
which would give an indication to whether the signal is speech or
non-speech.</p>
<ul>
<li><p>Signal energy is naturally a useful feature, since the energy of
speech varies a lot.</p></li>
<li><p>Voiced sounds generally have energy mainly at the low
frequencies, whereby estimators for spectral tilt are often
useful. For example,</p>
<ul>
<li><p>Zero-crossings (per time unit) is high for high-frequency
signals (noise) and low for low-frequency signals (voiced
speech), whereby it can be used as a feature.</p></li>
<li><p>The lag-1 autocorrelation is high (close to one) for
low-frequency signals and low (close to -1) for
high-frequency signals.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Speech sounds can be efficiently modelled by linear prediction.</p>
<ul>
<li><p>If the prediction error is small, then it is likely that the
signal is speech.</p></li>
<li><p>If the prediction error is large, then it is probably
non-speech.</p></li>
</ul>
</li>
<li><p>Voiced speech has by definition a prominent pitch.</p>
<ul>
<li><p>If we can identify a prominent pitch in the range then it is
likely voiced speech.</p></li>
</ul>
</li>
<li><p>Speech information is described effectively by their spectral
envelope.</p>
<ul>
<li><p>MFCC can be used as a description of envelope information and it
is thus a useful set of features.</p></li>
<li><p>Linear prediction parameters (esp. prediction residual) also
describe envelope information and can thus also be used as a
feature-set.
<img alt="vadfeat2" src="../_images/vad_features2-1.png" /></p></li>
</ul>
</li>
<li><p>Speech features vary rapidly and frequently.</p>
<ul>
<li><p>By looking at the rate of change <span class="math notranslate nohighlight">\(\Delta_k=f_{k+1}-f_k\)</span> in other
features <span class="math notranslate nohighlight">\(f_k\)</span>, we obtain information about the rate of change
of the signal. (Estimate of derivative)</p></li>
<li><p>Likewise, we can look at the second difference
<span class="math notranslate nohighlight">\(\Delta\Delta_k=\Delta_{k+1}-\Delta_k\)</span>. (Estimate of second
derivative)</p></li>
<li><p>These first and second order differences can be used as features
and they are known as <span class="math notranslate nohighlight">\(\Delta\)</span>- and <span class="math notranslate nohighlight">\(\Delta\Delta\)</span>-features.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="classifier">
<h2>Classifier<a class="headerlink" href="#classifier" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>We have collected a set of indicators for speech, the features,
whereby the next step is to merge the information from these
features to make a decision between speech and non-speech.</p></li>
<li><p>Classification is generic problem, with plenty of solutions such as</p>
<ul>
<li><p>decision trees (low-complexity, requires manual tuning)</p></li>
<li><p>linear classifier (relatively low-complexity, training from
data)</p></li>
<li><p>advanced methods such as neural networks, Gaussian mixture
models etc. (high-complexity, high-accuracy, training from data)
<img alt="vad2" src="../_images/vad2-1.png" /></p></li>
</ul>
</li>
</ul>
<div class="section" id="decision-trees-historical">
<h3>Decision trees (historical)<a class="headerlink" href="#decision-trees-historical" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Make a sequence of binary decisions (for example, is low or high
energy?) to decide whether signal is speech or non-speech.</p></li>
<li><p>Decision trees are very simple to implement.</p></li>
<li><p>Hard-coded <span class="math notranslate nohighlight">\(\Rightarrow\)</span> not very flexible.</p></li>
<li><p>Noise in one feature can cause us to follow wrong path.</p>
<ul>
<li><p>One noisy feature can break whole decision tree.</p></li>
</ul>
</li>
<li><p>Requires that each decision is manually tuned<br />
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> Lots of work, especially when tree is large</p></li>
<li><p>Structure and development becomes very complex if the number of
features increase.</p></li>
<li><p>Suitable for low-complexity systems and low-noise scenarios where
accuracy requirements are not so high.</p></li>
<li><p>I did not prepare an illustration/figure of result.</p></li>
</ul>
<p><img alt="vadtree" src="../_images/vad_decisiontree-1.png" /></p>
</div>
<div class="section" id="linear-classifier">
<h3>Linear classifier<a class="headerlink" href="#linear-classifier" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Instead of manually-tuned, binary decisions, can we use observed
data to make a statistical estimate?</p>
<ul>
<li><p>Using training data would automate the tuning of the model.<br />
Accuracy can be improved by adding more data.</p></li>
<li><p>By replacing binary decisions, we can let tendencies in several
features improve accuracy.</p></li>
</ul>
</li>
<li><p>Linear classifiers attempt to achieve a decision as a weighted sum
of the features.</p>
<ul>
<li><p>Let <span class="math notranslate nohighlight">\(\xi_k\)</span> be the features.</p></li>
<li><p>The decision is then obtained by <span class="math notranslate nohighlight">\(\eta = \sum_k\omega_k\xi_k\)</span>,
where <span class="math notranslate nohighlight">\(\omega_k\)</span> are scalar weights.</p></li>
<li><p>The objective is to find weights <span class="math notranslate nohighlight">\(\omega_k\)</span> such that $<span class="math notranslate nohighlight">\(\eta = 
      \begin{cases}
        -1 &amp; \text{non-speech} \\
        +1 &amp; \text{speech}.
      \end{cases}\)</span>$</p></li>
</ul>
</li>
<li><p>We then need to develop a method for choosing optimal
weights <span class="math notranslate nohighlight">\(\omega_k\)</span>.</p></li>
<li><p>The first step is to define an <em>objective function</em>, which we can
minimize.</p>
<ul>
<li><p>A good starting point is the classification error.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\eta\)</span> is the desired class for a frame and our classifier
gives <span class="math notranslate nohighlight">\(\hat\eta\)</span>, then the classification error is
<span class="math notranslate nohighlight">\(\nu^2 = (\eta -\hat\eta)^2\)</span>.</p></li>
<li><p>By minimizing the classification error, we can determine optimal
parameters <span class="math notranslate nohighlight">\(\omega_k\)</span>.</p></li>
</ul>
</li>
<li><p>Let <span class="math notranslate nohighlight">\(x_k\)</span> be the vector of all features for a frame <span class="math notranslate nohighlight">\(k\)</span> and
<span class="math notranslate nohighlight">\(X=[x_0,\,x_1\dots]\)</span> a matrix with all features for all frames.</p>
<ul>
<li><p>The classification of a single frame is then <span class="math notranslate nohighlight">\(\eta_k=x_k^Tw\)</span>.</p></li>
<li><p>The classification of all frames is then a vector <span class="math notranslate nohighlight">\(y=X^Tw\)</span>,
where <span class="math notranslate nohighlight">\(w\)</span> is the vector of weights <span class="math notranslate nohighlight">\(\omega_k\)</span>.</p></li>
<li><p>The sum of classification errors is the norm <span class="math notranslate nohighlight">\(\|y-\hat y\|^2\)</span>.</p></li>
</ul>
</li>
</ul>
<p><img alt="vadlin" src="../_images/vad_linearclassifier-1.png" /></p>
<div class="section" id="a-bit-of-math">
<h4>A bit of math<a class="headerlink" href="#a-bit-of-math" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>The minimum of the classification error <span class="math notranslate nohighlight">\(\|y-\hat y\|^2\)</span> can be
found by setting the partial derivative to zero. $<span class="math notranslate nohighlight">\(\begin{split}
      0 &amp;= \frac\partial{\partial w}\|y-\hat y\|^2
      = \frac\partial{\partial w}\|y-X^Tw\|^2
      \\&amp;
      = \frac\partial{\partial w}(y^Ty+w^TXX^Tw-2w^TXy)
      \\&amp;
      =2XX^Tw - 2Xy.
    \end{split}\)</span>$</p></li>
<li><p>The solution is the Moore-Penrose pseudo-inverse
$<span class="math notranslate nohighlight">\(w = (XX^T)^{-1}Xy^T := X^\dagger y.\)</span>$</p></li>
<li><p><em>Note:</em> This is a very common mathematical approach for solving
problems in speech processing, so it is much more important and
broadly applicable than “only” VAD.</p></li>
</ul>
</div>
<div class="section" id="pre-whitening-advanced-topic">
<h4>Pre-whitening (advanced topic)<a class="headerlink" href="#pre-whitening-advanced-topic" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>If the range of values from features are very different, we end up
with problems.</p>
<ul>
<li><p>A very loud voice will overrun weaker ones, even if the loud one
is full of crap.</p></li>
<li><p>The range (mean and variance) of features need to be normalized.</p></li>
<li><p>Correlations between features are also undesirable.</p></li>
</ul>
</li>
<li><p>The first step is removal of the mean, <span class="math notranslate nohighlight">\(x' = x - E[x]\)</span>, where
<span class="math notranslate nohighlight">\(E[x]\approx \frac1N\sum_kx_k\)</span> and <span class="math notranslate nohighlight">\(N\)</span> is the number of frames.</p></li>
<li><p>The covariance of the features is then
<span class="math notranslate nohighlight">\(C=E[x'(x')^T]\approx \frac1N X^TX\)</span>, where <span class="math notranslate nohighlight">\(X\)</span> now contains the
zero-mean features.</p></li>
<li><p>The eigenvalue decomposition of <span class="math notranslate nohighlight">\(C\)</span> is <span class="math notranslate nohighlight">\(C=V^TDV\)</span>, whereby we can
define the pre-whitening transform <span class="math notranslate nohighlight">\(A=D^{1/2}V\)</span> and <span class="math notranslate nohighlight">\(x'' = Ax'\)</span>.</p>
<ul>
<li><p>The covariance of the modified vector is
<span class="math notranslate nohighlight">\(E[x''(x'')^T] = AE[x'(x')^T]A^T = A C A^T = D^{1/2}V V^TDV V^T D^{1/2}=I\)</span>.</p></li>
<li><p>That is, <span class="math notranslate nohighlight">\(x''\)</span> has uncorrelated samples with equal variance and
zero-mean.</p></li>
</ul>
</li>
<li><p>What you need to know is that pre-whitening is a pre-processing
step, applied <em>before</em> training <span class="math notranslate nohighlight">\(w\)</span>.</p></li>
<li><p>We thus train the classifier on the modified vectors
<span class="math notranslate nohighlight">\(x''=A(x-E[x])\)</span>, to obtain the weights <span class="math notranslate nohighlight">\(w\)</span>.</p></li>
<li><p>The classifier with pre-whitening is
$<span class="math notranslate nohighlight">\(\nu=w^T x''=w^T A(x-E[x]) = \hat w^T (x-E[x])\)</span><span class="math notranslate nohighlight">\( where \)</span>\hat w=Aw$.</p></li>
<li><p>In other words, the pre-whitening can be included in the weights, so
no additional complexity is introduced other than removal of the
mean (which is trivial).</p></li>
</ul>
<p><img alt="vadfeat3" src="../_images/vad_features3-1.png" />
<img alt="vadfeat3" src="../_images/vad_features4-1.png" /></p>
</div>
</div>
<div class="section" id="id1">
<h3>Post-processing<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p><img alt="vadfeat5" src="../_images/vad_features5-1.png" /></p>
<ul class="simple">
<li><p>Above, we trained the classifier to get values <span class="math notranslate nohighlight">\(\pm1\)</span>, but when
using the classifier, we choose classes based on a threshold.</p></li>
<li><p>In practice, it does not matter if the output value is <span class="math notranslate nohighlight">\(0.1\)</span>, <span class="math notranslate nohighlight">\(1\)</span> or
<span class="math notranslate nohighlight">\(1000\)</span>, because if it is above threshold, then it belongs to the
same class.</p></li>
<li><p>Trying to hit <span class="math notranslate nohighlight">\(\pm1\)</span> is therefore an unnecessarily difficult task!</p>
<ul>
<li><p>We should be just trying to obtain a multidimensional threshold
which separates classes!</p></li>
</ul>
</li>
<li><p>We can truncate the output such that big values are reduced to
<span class="math notranslate nohighlight">\(\pm1\)</span>.</p></li>
<li><p>The sigmoid function is a map <span class="math notranslate nohighlight">\({\mathbb R}\rightarrow[0,1]\)</span> defined
as $<span class="math notranslate nohighlight">\(f(\nu) = \frac1{1+\exp(-\nu)}.\)</span>$</p></li>
<li><p>If we apply the sigmoid function on the output of the linear
classifier, then overshooting is not a problem anymore:
$<span class="math notranslate nohighlight">\(\hat \nu = f(w^T x) \qquad\in[0,1].\)</span>$</p></li>
</ul>
<p><img alt="vadfeat5" src="../_images/sigmoid-1.png" /></p>
<ul class="simple">
<li><p>Then there is no analytic solution, but we must use iterative
methods.</p></li>
<li><p>This function is known as a <em>perceptron</em>. Combining perceptrons into
a interconnected network gives a <em>neural network.</em><br />
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> A topic for other courses.</p></li>
<li><p>Linear classifiers are only slightly more complex than decisions
trees, but much more accurate.</p>
<ul>
<li><p>Main complexity of VAD lies in feature-extraction anyway, so the
differences in complexity of decision trees and linear
classifiers is negligible.</p></li>
</ul>
</li>
<li><p>The main advantages of linear classifiers in comparison to decision
trees are that</p>
<ul>
<li><p>(unbiased) we can use real data to train the model, whereby we
can be certain that it corresponds to reality (no bias due to
manual tuning),</p></li>
<li><p>(robust) whereas noise in one feature can break a decision tree,
linear classifiers merge information from all features, thus
reducing effect of noise.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="more-advanced-classifiers">
<h3>More advanced classifiers<a class="headerlink" href="#more-advanced-classifiers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>There exists a large range of better and more complex classifiers in
the general field of machine learning.</p>
<ul>
<li><p>Linear discriminant analysis (LDA) – splits the feature space
using hyper-planes.</p></li>
<li><p>Gaussian mixture models (GMM) – the feature space is modelled
by a sum of Gaussians.</p></li>
<li><p>Deep neural networks (DNN) – similar to linear classifiers but adds
non-linear mappings and several layers of sums.</p></li>
<li><p>K-nearest neighbors (kNN), support vector machine (SVM), random
forest classifiers, etc.</p></li>
</ul>
</li>
<li><p>These methods are in general more effective, but training and
application is more complex.</p>
<ul>
<li><p>Advice: Try a simple approach first and see if its good enough.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="speech-presence-probability">
<h2>Speech Presence Probability<a class="headerlink" href="#speech-presence-probability" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The output of the classifier is a continuous number, but it is
thresholded to obtain a decision.</p></li>
<li><p>The continuous output contains a lot of information about the signal
which is lost when applying thresholding.</p>
<ul>
<li><p>With a high value we are really certain that the signal is
speech, while a value near the threshold is relatively
uncertain.</p></li>
</ul>
</li>
<li><p>We can use the classifier output as an estimate of the probability
that the signal is speech <span class="math notranslate nohighlight">\(\Rightarrow\)</span> It is an estimator for
<em>speech presence probability</em>.</p></li>
<li><p>Subsequent applications can use this information as input to improve
performance.</p></li>
</ul>
<p><img alt="vadspp" src="../_images/vad_spp-1.png" /></p>
<div class="section" id="output-before-thresholding">
<h3>Output before thresholding<a class="headerlink" href="#output-before-thresholding" title="Permalink to this headline">¶</a></h3>
<p><img alt="vadfeat5" src="../_images/vad_features5-1.png" /></p>
</div>
</div>
<div class="section" id="noise-types">
<h2>Noise types<a class="headerlink" href="#noise-types" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>As noted before, VAD is trivial in noise-free scenarios.</p></li>
<li><p>In practice, typical background noise types are for example, office
noise, car noise, cafeteria (babble) noise, street noise, factory
noise, …</p></li>
<li><p>Clearly the problem is easier if the noise has a very different
character than the speech signal.</p>
<ul>
<li><p>Speech is quickly varying <span class="math notranslate nohighlight">\(\Rightarrow\)</span> stationary noises are
easy.</p></li>
<li><p>Speech is dominated by low frequencies <span class="math notranslate nohighlight">\(\Rightarrow\)</span> high
frequency noises are easy.</p></li>
</ul>
</li>
<li><p>The classic worst case is a competing (undesired) speaker, when
someone else is speaking in the background (babble noise).</p>
<ul>
<li><p>However, that would be difficult also for a human listener,
whereby it actually is a very difficult problem.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="conclusions">
<h2>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Voice activity detection is a type of methods which attempt to
determine if a signal is speech or non-speech.</p>
<ul>
<li><p>In a noise-free scenario the task is trivial, but it is also not
a realistic scenario.</p></li>
</ul>
</li>
<li><p>The basic idea of algorithms is:</p>
<ol class="simple">
<li><p>Calculate a set of features from the signal which are designed
to analyze properties which differentiate speech and non-speech.</p></li>
<li><p>Merge the information from the features in a classifier, which
returns the likelihood that the signal is speech.</p></li>
<li><p>Threshold the classifier output to determine whether the signal
is speech or not.</p></li>
</ol>
</li>
<li><p>VADs are used as a low-complexity pre-processing method, to save
resources (e.g. complexity or bitrate) in the main task.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Recognition"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>