
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8.1. Voice activity detection (VAD) &#8212; Introduction to Speech Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8.2. Wake-word and keyword spotting" href="Wake-word_and_keyword_spotting.html" />
    <link rel="prev" title="8. Recognition tasks in speech processing" href="../Recognition_tasks_in_speech_processing.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Introduction to Speech Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Preface.html">
   1. Preface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Introduction.html">
   2. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Why_speech_processing.html">
     2.1. Why speech processing?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Speech_production_and_acoustic_properties.html">
     2.2. Physiological speech production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech_perception">
     Speech perception (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Linguistic_structure_of_speech.html">
     2.5. Linguistic structure of speech
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Speech-language_pathology">
     Speech-language pathology (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Introduction/Applications_and_systems_structures.html">
     2.6. Applications and systems structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="http://pressbooks-dev.oer.hawaii.edu/messageprocessing/">
     Social and cognitive processes in human communication (external)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Representations/Representations.html">
   3. Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Waveform.html">
     3.1. Waveform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Windowing.html">
     3.2. Windowing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Signal_energy_loudness_and_decibel.html">
     3.3. Signal energy, loudness and decibel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Autocorrelation_and_autocovariance.html">
     3.4. Autocorrelation and autocovariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Cepstrum_and_MFCC.html">
     3.5. Cepstrum and MFCC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Linear_prediction.html">
     3.6. Linear prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Fundamental_frequency_F0.html">
     3.7. Fundamental frequency (F0)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Zero-crossing_rate.html">
     3.8. Zero-crossing rate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Deltas_and_Delta-deltas.html">
     3.9. Deltas and Delta-deltas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Pitch-Synchoronous_Overlap-Add_PSOLA.html">
     3.10. Pitch-Synchoronous Overlap-Add (PSOLA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Representations/Jitter_and_shimmer.html">
     3.11. Jitter and shimmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Crest_factor">
     https://en.wikipedia.org/wiki/Crest_factor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Pre-processing.html">
   4. Pre-processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Preprocessing/Pre-emphasis.html">
     4.1. Pre-emphasis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Noise_gate">
     Noise gate (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Dynamic_range_compression">
     Dynamic Range Compression (Wikipedia)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Modelling_tools_in_speech_processing.html">
   5. Modelling tools in speech processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Source_modelling_and_perceptual_modelling.html">
     5.1. Source modelling and perceptual modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Linear_regression.html">
     5.2. Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Sub-space_models.html">
     5.3. Sub-space models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Vector_quantization_VQ.html">
     5.4. Vector quantization (VQ)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Gaussian_mixture_model_GMM.html">
     5.5. Gaussian mixture model (GMM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Neural_networks.html">
     5.6. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Modelling/Non-negative_Matrix_and_Tensor_Factorization.html">
     5.7. Non-negative Matrix and Tensor Factorization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Evaluation_of_speech_processing_methods.html">
   6. Evaluation of speech processing methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Subjective_quality_evaluation.html">
     6.1. Subjective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Objective_quality_evaluation.html">
     6.2. Objective quality evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Other_performance_measures.html">
     6.3. Other performance measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Evaluation/Analysis_of_evaluation_results.html">
     6.4. Analysis of evaluation results
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_analysis.html">
   7. Speech analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Fundamental_frequency_estimation.html">
     7.1. Fundamental frequency estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Inverse_filtering_for_glottal_activity_estimation.html">
     7.2. Inverse filtering for glottal activity estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference external" href="https://en.wikipedia.org/wiki/Voice_analysis">
     Voice and speech analysis (Wikipedia)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Analysis/Measurements_for_medical_applications.html">
     7.3. Measurements for medical applications
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Electroglottograph">
       Electroglottography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference external" href="https://en.wikipedia.org/wiki/Videokymography">
       Videokymography (Wikipedia)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Analysis/Glottal_inverse_filtering.html">
       7.3.1. Glottal inverse filtering
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Analysis/Forensic_analysis.html">
     7.4. Forensic analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../Recognition_tasks_in_speech_processing.html">
   8. Recognition tasks in speech processing
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.1. Voice activity detection (VAD)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Wake-word_and_keyword_spotting.html">
     8.2. Wake-word and keyword spotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speech_Recognition.html">
     8.3. Speech Recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speaker_Recognition_and_Verification.html">
     8.4. Speaker Recognition and Verification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Speaker_Diarization.html">
     8.5. Speaker Diarization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Paralinguistic_speech_processing.html">
     8.6. Paralinguistic speech processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_Synthesis.html">
   9. Speech Synthesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Concatenative_speech_synthesis.html">
     9.2. Concatenative speech synthesis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Synthesis/Statistical_parametric_speech_synthesis.html">
     9.3. Statistical parametric speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transmission_storage_and_telecommunication.html">
   10. Transmission, storage and telecommunication
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Design_goals.html">
     10.1. Design goals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Basic_tools.html">
     10.2. Basic tools
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transmission/Modified_discrete_cosine_transform_MDCT.html">
     10.3. Modified discrete cosine transform (MDCT)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Entropy_coding.html">
       10.3.4. Entropy coding
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transmission/Perceptual_modelling_in_speech_and_audio_coding.html">
       10.3.5. Perceptual modelling in speech and audio coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Code-excited_linear_prediction_CELP.html">
     10.4. Code-excited linear prediction (CELP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Transmission/Frequency-domain_coding.html">
     10.5. Frequency-domain coding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Speech_enhancement.html">
   11. Speech enhancement
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Noise_attenuation.html">
     11.1. Noise attenuation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Echo_cancellation.html">
     11.2. Echo cancellation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Bandwidth_extension_BWE.html">
     11.3. Bandwidth extension (BWE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Enhancement/Multi-channel_speech_enhancement_and_beamforming.html">
     11.4. Multi-channel speech enhancement and beamforming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://landbot.io/blog/guide-to-conversational-design/">
   Chatbots / Conversational design (external link)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Computational_models_of_human_language_processing.html">
   12. Computational models of human language processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Security_and_privacy_in_speech_technology.html">
   13. Security and privacy in speech technology
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Recognition/Voice_activity_detection_VAD.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-and-motivation">
   8.1.1. Introduction and motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-formulation">
   8.1.2. Problem formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-approach-1-energy-thresholding">
   8.1.3. Naïve approach 1 - Energy thresholding
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-criteria">
   8.1.4. Performance criteria
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-approach-2-more-features">
   8.1.5. Naïve approach 2 - More features
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modern-approach-machine-learning">
   8.1.6. Modern approach - Machine learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#post-processing">
   8.1.7. Post-processing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attachments">
   8.1.8. Attachments:
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Voice activity detection (VAD)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-and-motivation">
   8.1.1. Introduction and motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-formulation">
   8.1.2. Problem formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-approach-1-energy-thresholding">
   8.1.3. Naïve approach 1 - Energy thresholding
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-criteria">
   8.1.4. Performance criteria
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-approach-2-more-features">
   8.1.5. Naïve approach 2 - More features
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modern-approach-machine-learning">
   8.1.6. Modern approach - Machine learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#post-processing">
   8.1.7. Post-processing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attachments">
   8.1.8. Attachments:
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="voice-activity-detection-vad">
<h1><span class="section-number">8.1. </span>Voice activity detection (VAD)<a class="headerlink" href="#voice-activity-detection-vad" title="Permalink to this headline">¶</a></h1>
<div class="contentLayout2">
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
<div class="section" id="introduction-and-motivation">
<h2><span class="section-number">8.1.1. </span>Introduction and motivation<a class="headerlink" href="#introduction-and-motivation" title="Permalink to this headline">¶</a></h2>
<p>Many speech processing algorithms are resource intensive and require
significant computing power or transmission bandwidth. Yet speech is
discontinuous such that we often have pauses between sentences and
breaks even within sentences. Moreover, in a dialogue, a speaker would
typically use polite turn-taking, such that others are silent when one
person is speaking. Resource-intensive processing is not necessary
during breaks in speech. Consequently, there is great potential in
saving resources by deactivating advanced speech processing methods
whenever the input signal does not contain speech.</p>
<p><em>Voice activity detection</em> (VAD) refers to the task of determining
whether a signal contains speech or not. It is thus a binary decision. A
related task is to determine the <em>probability</em> that an input signal
contains speech or not, referred to as the <em>speech presence probability</em>
(SPP). The SPP is typically then expressed as the probability in the
range 0 to 1. Speech presence probability is typically an intermediate
step in voice activity detection, such that the voice activity
classification is obtained by thresholding the output of the speech
presence probability estimator.</p>
<p>Generally, voice activity detection algorithms are relatively simple,
such that the more complex tasks such as speech recognition, need to be
applied only when speech is present. Similarly, in speech coding, we
need to transmit speech only when speech is present and we can reduce
bitrate whenever speech is absent.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
<div class="section" id="problem-formulation">
<h2><span class="section-number">8.1.2. </span>Problem formulation<a class="headerlink" href="#problem-formulation" title="Permalink to this headline">¶</a></h2>
<p>For an input signal <em>x</em>, our objective is to determine whether it is
speech or not. We express the VAD algorithm as a function <em>y=VAD(x),</em>
where the desired target output is</p>
<p>\[ y^* := \begin{cases} 0, &amp; x \text{ is not speech,} \ 1, &amp; x
\text{ is speech.} \end{cases} \]</p>
<p>Correspondingly, the speech presence probability is the probability that
<em>x</em> is speech, <em>SPP(x)=P(x is speech).</em> A possible definition for the
VAD is then</p>
<p>\[ VAD(x) := \begin{cases} 0, &amp; SPP(x) &lt; \theta\ 1, &amp; SPP(x)
\geq \theta, \end{cases} \]</p>
<p>where θ is a scalar threshold. An example of speech presence probability
is illustrated in the following section.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
<div class="section" id="naive-approach-1-energy-thresholding">
<h2><span class="section-number">8.1.3. </span>Naïve approach 1 - Energy thresholding<a class="headerlink" href="#naive-approach-1-energy-thresholding" title="Permalink to this headline">¶</a></h2>
<p>A speech signal is not a stationary signal. Most prominently, sometimes
we speak energetically and sometimes we do not speak. It is then obvious
that <em>signal energy</em> can be used as an indicator of speech presence.
Speech adds energy to the signal, such that high-energy regions of the
signal are likely speech. For example, we can set a threshold \(
\theta_{SILENCE} \) such that when the energy of the signal \(
\sigma^2(x) \) is above the threshold, the VAD indicates speech
activity</p>
<p>\[ VAD(x) := \begin{cases} 0, &amp; \sigma^2(x) &lt;
\theta_{SILENCE}\ 1, &amp; \sigma^2(x) \geq \theta_{SILENCE}.
\end{cases} \]</p>
<p>To implement this approach, we first apply <span class="xref myst">windowing</span> to the
input signal with 30 ms windows and 50 % overlap. For each window, we
calculate signal energy as</p>
<p>\[ \sigma^2(x) := \|x\|^2 = \sum_{k=0}^{N-1} x_k^2. \]</p>
<p>To choose a suitable threshold, in the figure on the right, we plot the
energy over a speech signal \( \sigma^2(x) \) over a speech signal.
We can observe that areas in the speech signal with little activity have
an energy below 17 dB, whereby we can set the threshold at \(
\theta_{SILENCE}:=17dB. \) The resulting voice activity estimate is
illustrated in the lowest pane.</p>
<p>The result does seem reasonable. High-amplitude speech sounds are
clearly identified as speech. However, in the middle of the sentence,
the VAD frequently identifies non-speech frames. Is this correct?</p>
<p>In fact, it is not entirely clear what the output should be. It is a
matter of definition. On a heuristic level, we can define that speech
starts at the beginning of a sentence and finishes when the sentence
ends. But how should the VAD then handle sentences like “<em>What if … we
would go on a holiday?.</em>” where there is a break in the middle of a
grammatically correct sentence. Should the break be identified as
non-speech? How long breaks do we allow? What about grammatically
incorrect sentence like “<em>We could go to..</em>.”?</p>
<p>Moreover, sentences often have a trail-off, where the signal energy
decreases. For example in the figure on the right, the last word
diminishes in energy up to about the time 2.7 s, where the signal energy
goes below 10 dB. However, with the threshold of 17 dB, we have cut of
the VAD already before 2.6 s. If we would lower the threshold to 10dB,
then everything between 0.2 s and 1 s would be labelled incorrectly as
speech.</p>
<p>It is then clear that even in this simple example, it is not easy to set
a threshold which gives a good result. To make things worse, often
speech signals are corrupted by background noises, which makes
energy-thresholding even more difficult. To avoid labelling everything
as speech, the threshold must be higher, but then less of the speech
frames are labelled correctly. Which leads to the question, is it more
important to label speech or non-speech correctly?</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>Input sound sample</p>
<p><a href="attachments/151500905/155463338.wav"
data-linked-resource-id="155463338" data-linked-resource-version="1"
data-linked-resource-type="attachment"
data-linked-resource-default-alias="sound_example.wav"
data-nice-type="Multimedia"
data-linked-resource-content-type="audio/x-wav"
data-linked-resource-container-id="151500905"
data-linked-resource-container-version="27">sound_example.wav</a></p>
<hr class="docutils" />
<p>VAD example with energy thresholding</p>
<p><img src="attachments/151500905/155463332.png"
data-image-src="attachments/151500905/155463332.png"
data-unresolved-comment-count="0" data-linked-resource-id="155463332"
data-linked-resource-version="1" data-linked-resource-type="attachment"
data-linked-resource-default-alias="vad.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="151500905"
data-linked-resource-container-version="27" width="640" /></p>
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
<div class="section" id="performance-criteria">
<h2><span class="section-number">8.1.4. </span>Performance criteria<a class="headerlink" href="#performance-criteria" title="Permalink to this headline">¶</a></h2>
<p>The task of voice activity detection (VAD) is seemingly straightforward,
but even evaluation of performance is more difficult than one perhaps
would expect. As with the definition of correct labels, also the
performance criteria depend on the application;</p>
<ul class="simple">
<li><p>In <span class="xref myst">speech transmission</span>
(i.e. speech coding), we would like to save bandwidth by shutting
off transmission when there is no speech present. However, turning
off transmission during a speech segment would leave to severe
degradations.</p></li>
<li><p>Some <span class="xref myst">speech enhancement</span> algorithms can
estimate noise-statistics during non-speech segments, which are
identified using a VAD. During speech segments, the algorithm can
then remove everything which looks like noise. If however speech is
present in the segment where we estimate characteristics of the
noise, then we would remove also such features which appear during
noise segments. This would lead to a degradation of the desired
speech signal.</p></li>
<li><p>In speech recognition, we would like to limit CPU usage by
activating the algorithm only during speech segments. We would then
like to be sure that no speech segments are omitted, because that
would severely degrade output quality.</p></li>
<li><p>In wake-word spotting, we want to activate a device when a specific
word is pronounced, such that the device can be in a sleep mode when
the wake-word has not been pronounced. We can also turn the
wake-word algorithm off for further reductions when there is no
speech present. The latter functionality is achieved with the VAD.
Overall, if some wake-words are missed, the user just has to say it
again - no big deal. We can therefore allow some speech segments to
be identified as non-speech. However, assuming that the wake-word
spotting is not a too big load on the CPU, we can allow the VAD to
be more sensitive. Detailed CPU cost vs. performance -optimization
can however be complicated.</p></li>
</ul>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>We can observe that different applications emphasise different types of
errors. Speech transmission and recognition prefer a sensitive VAD where
we are certain that no speech-segments are lost. In contrast, speech
enhancement and wake-word spotting tend to prefer that the VAD is
conservative and labels audio to speech only when certain.</p>
<p>To quantify such differences, we can label performance with the
following attributes:</p>
<div class="table-wrap">
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Input \ VAD-output</p></td>
<td><p>VAD=Speech</p></td>
<td><p>VAD=Non-speech</p></td>
</tr>
<tr class="row-odd"><td><p>Input=Speech</p></td>
<td><p>True positive (TP)</p></td>
<td><p>False negative (FN)</p></td>
</tr>
<tr class="row-even"><td><p>Input=Non-speech</p></td>
<td><p>False positive (FP)</p></td>
<td><p>True negative (TN)</p></td>
</tr>
</tbody>
</table>
</div>
<p>Then for example for speech recognition, we would penalise <em>less for
false positives</em> and penalise <em>more for false negatives</em>. In comparison,
in wake-word spotting, we would perhaps penalise equally for both false
positives and negatives. It really depends on your overall design.</p>
<p>In the above example of thresholding energy, we can then choose
different values for the threshold and plot the values for true positive
and negatives for each threshold (see figure on the right). Performance
for a threshold of 17 dB is indicated with a red cross. We can readily
see that there, all non-speech segments are correctly identified (TN=1
and FP=0), however, speech segments are not all correct (TP=0.72 and
FN=0.28). In fact, by reducing the threshold to 14.6 dB (yellow circle
in the figure), we would retain perfect false positives, (TN=1 and
FP=0), but we would improve false negatives (TP=0.80 and FN=0.20).</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>VAD performance in terms of percentage of true positives and true
negatives (left) and false negatives and false positives (right).</p>
<p><img src="attachments/151500905/155463472.png"
data-image-src="attachments/151500905/155463472.png"
data-unresolved-comment-count="0" data-linked-resource-id="155463472"
data-linked-resource-version="3" data-linked-resource-type="attachment"
data-linked-resource-default-alias="vad_tptn.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="151500905"
data-linked-resource-container-version="27" width="640" /></p>
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
<div class="section" id="naive-approach-2-more-features">
<h2><span class="section-number">8.1.5. </span>Naïve approach 2 - More features<a class="headerlink" href="#naive-approach-2-more-features" title="Permalink to this headline">¶</a></h2>
<p>To improve performance of the voice activity detector (VAD), we can
analyse more properties of the speech signal, which we usually call
<em>features</em>. For example,</p>
<ul class="simple">
<li><p><span class="xref myst"><em>Linear predictors</em></span> describe the spectral shape
of speech signals efficiently. In other words, if the modelling
error of a linear predictor is small, then the signal is likely a
speech signal.</p></li>
<li><p>Alternatively, if we prefer a lower-complexity solution instead of
linear prediction, we can analyse the normalised
<span class="xref myst">autocorrelation</span> \( c_k/c_0
\) at lag <em>k=1</em>. Speech signals are highly correlated over time,
such that if the absolute value  \( |c_1|/c_0 \) is high, then
it is likely a speech signal. However, noise signals can also have a
negative correlation which is close to -1, such<br />
that we can prefer to omit the absolute value.<br />
An alternative which is roughly equivalent to the covariance at
lag-1, is to use to <span class="xref myst">zero-crossing rate</span>.</p></li>
<li><p>Signals with a prominent <span class="xref myst">fundamental
frequency</span> in the range which is typical
to humans (approx 80 to 450 Hz) are likely to be voiced speech
signals. However, unvoiced speech signals have by definition no
fundamental frequency and would not be detected by this method.</p></li>
<li><p><span class="xref myst">Mel-frequency cepstral coefficients (MFCCs)</span>
have been shown to give excellent performance as features in many
classification tasks.</p></li>
</ul>
<p>These features can be further augmented by their trends over time. A
classic method is to measure time-derivatives, known as the deltas and
delta-deltas, defined for a feature  \( f_k(t) \) at time <em>t</em> as</p>
<p>\[ \begin{cases} \Delta_k(t) &amp;= f_k(t) - f_k(t-1) \
\Delta\Delta_k(t) &amp;= \Delta_k(t) - \Delta_k(t-1). \end{cases} \]</p>
<p>Alternatively, we can just consider features explicitly over time \(
f_k(t-N) … f_k(t). \)</p>
<p>We can use all these parameters and many more to effectively
characterize speech signals. The question however is how we can merge
the information from a vector of features into a single output value? A
naïve approach would be to implement a binary decision tree, such that
for example,</p>
<ul class="simple">
<li><p>If energy is sufficiently high, then output is speech and we return
VAD(x)=1.</p></li>
<li><p>If covariance is sufficiently high, then output is speech and we
return VAD(x)=1.</p></li>
<li><p>If there is a prominent fundamental frequency in the range 80 to 450
Hz, then return VAD(x) = 1.</p></li>
<li><p>Otherwise return VAD(x) = 0.</p></li>
</ul>
<p>This is an entirely heuristic and non-scientific approach which is
difficult to design when the number of features increases.</p>
<p>A better way is the classic method of linear estimation, even if it is
now considered old-fashioned (with good reasons). In this method we take
all features \( f=[f_0\dots f_{N-1}]^T \) and take their linear
combination weights \( w=[w_0\dots w_{N-1}]^T \) , as</p>
<p>\[ \hat y = f^T w. \]</p>
<p>The objective is to find weights such that the estimate \( \hat y \)
is approximately equal to the desired output \( y\approx\hat y. \)
By collecting the features of a large amount of speech samples in a
matrix \( F=[f(t) \dots f(t+T)] \) , the output is \( \hat y =
F^T w \) and we can minimize the squared estimation error</p>
<p>\[ \| y -\hat y\|^2 = \|y- F^T w\|^2. \]</p>
<p>To find the optimum, we set the derivative to zero</p>
<p>\[ 0=\frac{\partial}{\partial w}\|y- F^T w\|^2 = F(y-F^Tw) =
Fy-FF^T w. \]</p>
<p>Clearly the optimal weight vector is then</p>
<p>\[ w^* = (FF^T)^{-1} Fy = F^\dagger y. \]</p>
<p>Here the superscript \( \dagger \) denotes the pseudo-inverse.</p>
<p>Once we have found the optimal weights, for an individual frame we then
have</p>
<p>\[ \hat y = f(t)^T w^*, \]</p>
<p>and we can apply thresholding to get the VAD output as</p>
<p>\[ VAD(x) := \begin{cases} 0, &amp; f^Tw^* &lt; \theta\ 1, &amp; f^Tw^*
\geq \theta. \end{cases} \]</p>
<p>The figure on the right illustrates a linear classifier with this
approach using the 8 first autocorrelation values as well as their delta
and delta-delta values as features. The figure illustrates the desired,
raw as well as the thresholded output. Compared to energy thresholding
implemented above, the result seems much better. In the
false-positives/false-negatives plot, we see a comparison of the two
methods, energy thresholding and linear classifier. Both error-types are
reduced by an order of magnitude using the improved method.</p>
<p>It is important to note, however, that voice activity detection in
silence is a very easy task. The good results we obtained here are
therefore not unexpected. Speech distorted by background noises is much
more difficult for VAD algorithms.</p>
<p>The problems with this approach include that it hiddenly assumes that
the distribution of the input features for speech and non-speech signals
are linearly separable. That is, it cannot take into account any
non-linear shapes of the distribution, nor can it take into account the
fact that speech signals are represented by a multitude of sub-classes
such as voiced and unvoiced samples.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p><img src="attachments/151500905/155463830.png" class="image-center"
data-image-src="attachments/151500905/155463830.png"
data-unresolved-comment-count="0" data-linked-resource-id="155463830"
data-linked-resource-version="1" data-linked-resource-type="attachment"
data-linked-resource-default-alias="vad_linear.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="151500905"
data-linked-resource-container-version="27" width="640" /></p>
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
<div class="section" id="modern-approach-machine-learning">
<h2><span class="section-number">8.1.6. </span>Modern approach - Machine learning<a class="headerlink" href="#modern-approach-machine-learning" title="Permalink to this headline">¶</a></h2>
<p>Executive summary: Use machine learning for voice activity detection
tasks.</p>
<p>The basic design principle is:</p>
<ol class="simple">
<li><p>Choose a selection of the features described above.</p></li>
<li><p>Choose the design of your machine learning method.</p></li>
<li><p>Train the parameters over a large database of speech, which is
representative of your intended application.</p></li>
</ol>
<p>Unfortunately, the scientific methods for feature-selection and choosing
designs of machine learning methods have not yet matured. It is however
important to remember that voice activity detection is intended to be a
low-complexity method which saves resources. We can therefore accept
using a smaller number of parameters and a simpler design, even if that
sacrifices quality to some extent.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
</div>
</div>
<div class="columnLayout two-equal" layout="two-equal">
<div class="cell normal" data-type="normal">
<div class="innerCell">
</div>
<div class="section" id="post-processing">
<h2><span class="section-number">8.1.7. </span>Post-processing<a class="headerlink" href="#post-processing" title="Permalink to this headline">¶</a></h2>
<p>In general, speech onsets (when an utterance begins) are relatively easy
to detect, whereas it is difficult to determine where an utterance ends,
especially when speech trails off slowly. Another typical error in voice
activity detectors (VAD) are isolated errors, where one or small number
of consecutive frames are incorrectly labelled. And even if onsets are
generally easily detected, sometimes the VAD activates a bit too slowly,
such that the first frame of the speech segment is incorrectly
classified as non-speech.</p>
<p>Such errors can be easily corrected with heuristic methods such as
hangover, where we define a modified output as</p>
<p>\[ VAD’(x) = \max\left(VAD(k-K)..VAD(k)\right). \]</p>
<p>In other words, if any of the last <em>K</em> frames was speech, then also this
frame is speech. If future frames are available, we can even extend this
to the future by defining</p>
<p>\[ VAD’(x) = \max\left(VAD(k-K)..VAD(k+H)\right). \]</p>
<p>Such hangover -type functionalities can reduce false negatives with a
substantial amount. A similar approach can be implemented to remove very
short segments labelled as speech, since very short speech utterances
are both non-informative, but also cannot realistically be speech
sounds.</p>
<p>The figure on the right illustrates post-processing applied on the
output of the linear model implemented above. In this case, we first
remove isolated peaks with \( VAD’(x) =
\min\left(VAD(k-1)..VAD(k+1)\right) \) and then apply a hangout with
a two-samples backward and one-sample lookahead as \( VAD’’(x) =
\max\left(VAD’(k-2)..VAD’(k+1)\right). \) In this simple case, we
get then a perfect output.</p>
</div>
</div>
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p><img src="attachments/151500905/155463844.png" class="image-center"
data-image-src="attachments/151500905/155463844.png"
data-unresolved-comment-count="0" data-linked-resource-id="155463844"
data-linked-resource-version="1" data-linked-resource-type="attachment"
data-linked-resource-default-alias="vad_postproc.png"
data-base-url="https://wiki.aalto.fi"
data-linked-resource-content-type="image/png"
data-linked-resource-container-id="151500905"
data-linked-resource-container-version="27" width="640" /></p>
</div>
</div>
</div>
</div>
<div class="pageSectionHeader">
</div>
<div class="section" id="attachments">
<h2><span class="section-number">8.1.8. </span>Attachments:<a class="headerlink" href="#attachments" title="Permalink to this headline">¶</a></h2>
</div>
<div class="greybox" align="left">
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[vad.pdf](attachments/151500905/155463331.pdf) (application/pdf)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[vad.png](attachments/151500905/155463332.png) (image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[sound_example.wav](attachments/151500905/155463338.wav) (audio/x-wav)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[vad_tptn.png](attachments/151500905/155463474.png) (image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[vad_tptn.png](attachments/151500905/155463476.png) (image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[vad_tptn.png](attachments/151500905/155463472.png) (image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[vad_linear.png](attachments/151500905/155463830.png) (image/png)  
<img src="images/icons/bullet_blue.gif" width="8" height="8" />
[vad_postproc.png](attachments/151500905/155463844.png) (image/png)  
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Recognition"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../Recognition_tasks_in_speech_processing.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">8. </span>Recognition tasks in speech processing</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Wake-word_and_keyword_spotting.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8.2. </span>Wake-word and keyword spotting</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das<br/>
    
      <div class="extra_footer">
        <p>
<img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width: 0;" alt="Creative Commons License" />
This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>