%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Introduction to Speech Processing}
\date{Apr 28, 2022}
\release{}
\author{Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo, Sneha Das}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxAtStartPar
\sphinxstylestrong{2nd Edition}

\sphinxAtStartPar
This is an open access and creative commons book of speech processing, intended as pedagogical material for engineering students.
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Preface::doc}]{\sphinxcrossref{Preface}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Introduction::doc}]{\sphinxcrossref{Introduction}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Representations::doc}]{\sphinxcrossref{Basic Representations}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Pre-processing::doc}]{\sphinxcrossref{Pre\sphinxhyphen{}processing}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Modelling_tools_in_speech_processing::doc}]{\sphinxcrossref{Modelling tools in speech processing}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Evaluation_of_speech_processing_methods::doc}]{\sphinxcrossref{Evaluation of speech processing methods}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Speech_analysis::doc}]{\sphinxcrossref{Speech analysis}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Recognition_tasks_in_speech_processing::doc}]{\sphinxcrossref{Recognition tasks in speech processing}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Speech_Synthesis::doc}]{\sphinxcrossref{Speech Synthesis}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Transmission_storage_and_telecommunication::doc}]{\sphinxcrossref{Transmission, storage and telecommunication}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Speech_enhancement::doc}]{\sphinxcrossref{Speech enhancement}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Computational_models_of_human_language_processing::doc}]{\sphinxcrossref{Computational models of human language processing}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Security_and_privacy::doc}]{\sphinxcrossref{Security and privacy in speech technology}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{References::doc}]{\sphinxcrossref{References}}}

\end{itemize}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large Contributing}
\end{DUlineblock}

\sphinxAtStartPar
We welcome all type of useful contributions! Content contributors will, unless author requests to remain anonymous, be automatically included on the \DUrole{xref,myst}{list of authors} and for bugfixers we will create an “also featuring” list.
The best way of contributing however depends on type of contribution:
\begin{itemize}
\item {} 
\sphinxAtStartPar
For small bugfixes
\begin{itemize}
\item {} 
\sphinxAtStartPar
Post an \sphinxhref{https://gitlab.com/speech-interaction-technology-aalto-university/itsp/-/issues}{issue on gitlab.com}, or

\item {} 
\sphinxAtStartPar
Modify the code directly and push a \sphinxhref{https://gitlab.com/speech-interaction-technology-aalto-university/itsp/-/merge\_requests}{merge request}.

\end{itemize}

\item {} 
\sphinxAtStartPar
For clearly missing or incomplete content
\begin{itemize}
\item {} 
\sphinxAtStartPar
Create the content and push a \sphinxhref{https://gitlab.com/speech-interaction-technology-aalto-university/itsp/-/merge\_requests}{merge request}.

\end{itemize}

\item {} 
\sphinxAtStartPar
Extensions of topic area
\begin{itemize}
\item {} 
\sphinxAtStartPar
Discuss it first with other by posting an \sphinxhref{https://gitlab.com/speech-interaction-technology-aalto-university/itsp/-/issues}{issue on gitlab.com}.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
The source material is stored at \sphinxurl{https://gitlab.com/speech-interaction-technology-aalto-university/itsp}.

\sphinxAtStartPar
See also \DUrole{xref,myst}{Instructions for developers}.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Referencing}
\end{DUlineblock}
\begin{quote}

\sphinxAtStartPar
Tom Bäckström, Okko Räsänen, Abraham Zewoudie, Pablo Pérez Zarazaga, Liisa Koivusalo and Sneha Das, “\sphinxstyleemphasis{Introduction to Speech Processing}”, 2nd Edition, 2022. URL: \sphinxurl{https://speechprocessingbook.aalto.fi}
\end{quote}

\sphinxAtStartPar
\sphinxhref{http://www.bibtex.org/}{Bibtex} format:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
@book\PYGZob{}itsp2022,
   title = \PYGZob{}Introduction to Speech Processing\PYGZcb{},
   edition = 2,
   year = 2022,
   author = \PYGZob{}Tom Bäckström and Okko Räsänen and Abraham Zewoudie and Pablo Pérez Zarazaga and Liisa Koivusalo and Sneha Das\PYGZcb{},
   url = \PYGZob{}https://speechprocessingbook.aalto.fi\PYGZcb{},
   \PYGZcb{}
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{Preface}
\label{\detokenize{Preface:preface}}\label{\detokenize{Preface::doc}}
\sphinxAtStartPar
This is a collection of pedagogical material within the topic of speech
and language technology. The idea is to provide
\begin{itemize}
\item {} 
\sphinxAtStartPar
teachers material for their courses, where they can pick and choose
material which is appropriate for their own courses.

\item {} 
\sphinxAtStartPar
self\sphinxhyphen{}study material on\sphinxhyphen{}line for anyone interested.

\end{itemize}

\sphinxAtStartPar
By licensing the material under creative commons (share alike), we want
to encourage people to contribute improvements and additions to the
content.


\section{Design philosophy of this document}
\label{\detokenize{Preface:design-philosophy-of-this-document}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Target audience
\begin{itemize}
\item {} 
\sphinxAtStartPar
Primary target:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Master’s level students with some background in digital
signal processing (=signals and systems), machine learning,
linear algebra and stochastic processes

\end{itemize}

\item {} 
\sphinxAtStartPar
Secondary targets:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Researchers in related areas who want to expand their
expertise, for example, researchers in machine learning,
signal processing, audio processing, linguistics,
human\sphinxhyphen{}computer interfaces etc

\end{itemize}

\end{itemize}

\item {} 
\sphinxAtStartPar
Keep a pedagogical approach
\begin{itemize}
\item {} 
\sphinxAtStartPar
Explain why this tool is needed and how it solves a problem.

\item {} 
\sphinxAtStartPar
Give an example of how it is used in practice, including
demonstrations and pictures.

\item {} 
\sphinxAtStartPar
Favour tools which everyone are using, rather than inventions of
your own team

\end{itemize}

\item {} 
\sphinxAtStartPar
Small steps
\begin{itemize}
\item {} 
\sphinxAtStartPar
Better to have something than nothing. Though it is a good
direction, reaching perfection is not a requirement.

\item {} 
\sphinxAtStartPar
Fix errors immediately when you find one.

\end{itemize}

\end{itemize}


\section{Foreword to the First Edition}
\label{\detokenize{Preface:foreword-to-the-first-edition}}
\sphinxAtStartPar
Foreword by Tom Bäckström

\sphinxAtStartPar
As I was teaching the course “Speech processing” at Aalto University, I
was always looking for good teaching material. I was not really content
with what I found. Some good books were available but they were
expensive. I was not comfortable with demanding the students to pay
hundreds of Euros for a book they’d use once. I was also not comfortable
in illegally copying content. The alternatives were then to accept
lower\sphinxhyphen{}quality material or write my own.

\sphinxAtStartPar
Part of the issue I have with expensive books is that the money does not
go to the authors themselves, but to middle\sphinxhyphen{}men. Moreover, in the
Internet\sphinxhyphen{}era, paper books seem so last\sphinxhyphen{}century. Why print a book on
paper when we can make it a web document? Why put it behind a pay\sphinxhyphen{}wall?
I mean, I really would not receive any significant part of my income
from such a book. Putting it on the web then seems like the only sane
solution.

\sphinxAtStartPar
Besides, once you’re free from the constraints of a conventional book,
you can do all kinds of fun stuff. Like why would I limit access to
modifying content and why not something more wikipedia\sphinxhyphen{}like? I’m paid by
the government, so it seems also obvious that I should put my work out
in the public domain. No, more accurately, I’m putting this out with a
Creative Commons licence (attribution \& share\sphinxhyphen{}alike). Perhaps it’s
vanity, but I would like to receive credit for this work, if there is
any credit due.

\sphinxAtStartPar
The desired consequence of Creative commons licensing is that the
material would find multiple contributors, to improve the content. To
follow the old, worn but accurate adage; to stand on the shoulders of
giants, and so forth. By collaboration we can do better.

\sphinxAtStartPar
The way I intend to use this in my own teaching is that the on\sphinxhyphen{}line
version of the document follows its own natural grouping of topics.
Start with basics and progress to more complex topics and applications.
For my own, course, however, I want to have exercises in parallel with
the course. The problem is then that the most basic chapters do not lend
themselves to exercises which are useful for my teaching goals. So I
design exercises to match my teaching goals and organize lecture
material to give sufficient background to the exercises. In this
web\sphinxhyphen{}based document this is no problem. I’ll just create a new table of
contents, where the ordering of chapters and sections is reorganized.


\section{Foreword to the 2nd Edition}
\label{\detokenize{Preface:foreword-to-the-2nd-edition}}
\sphinxAtStartPar
Foreword by Tom Bäckström

\sphinxAtStartPar
I have been positively encouraged and surprised by the feedback I have received for the first edition. So many people have spontaneously given feedback. I find it safe to assume that many more have used this material than those who have contacted me. I therefore conclude that the impact of this material has been much larger than I anticipated. Great! This encourages me to continue putting effort into the document, to make it better and expand it.

\sphinxAtStartPar
For the second edition, I wanted to address the following issues:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Platform:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Integration with \sphinxhref{https://jupyter.org/}{JupyterLab} and other similar platforms for interactive coding examples and visualizations are not easily possible on the original platform. Still, in my own teaching I have found such tools mmensly effective and popular among the students.

\item {} 
\sphinxAtStartPar
Though the material was published as Open access with a Creative Commons license, the original platform did not allow for easy porting to other formats. Especially mathematical notation and equations required extra effort when porting. This is a clear contradiction to our open access intentions and desires.

\end{itemize}

\sphinxAtStartPar
Clearly both arguments lead to the conclusion that we have to switch platforms. Delaying the switch further will make it only harder. Currently the dominant way of sharing evovling community projects is \sphinxhref{https://en.wikipedia.org/wiki/Git}{git} and consequently, that is the obvious choice. Additional benefits from git\sphinxhyphen{}based platforms is that they have many practical tools integrated, like merge\sphinxhyphen{}requests, discussion boards etc.

\item {} 
\sphinxAtStartPar
Content:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Machine learning in speech processing was not well\sphinxhyphen{}enough represented. Have to add and improve the content in that area.

\item {} 
\sphinxAtStartPar
I am developing and have recently particpated in generation of additional content both for a Bachelor\sphinxhyphen{}level introductory course as well as a course about design of speech interaction technology. Those should be added here.

\item {} 
\sphinxAtStartPar
There are also many other areas which would benefit from additions, like speech recognition and NLP. I hope we find someone to contribute material also there.

\end{itemize}

\end{itemize}

\sphinxstepscope


\chapter{Introduction}
\label{\detokenize{Introduction:introduction}}\label{\detokenize{Introduction::doc}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Introduction/Why_speech_processing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Why speech processing?}}}} 

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Introduction/Speech_production_and_acoustic_properties::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Speech production and acoustic properties}}}} 

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Speech\_perception}{Speech perception} (Wikipedia)

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Introduction/Linguistic_structure_of_speech::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Linguistic structure of speech}}}} 

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Speech-language\_pathology}{Speech\sphinxhyphen{}Language pathology} (Wikipedia)

\item {} 
\sphinxAtStartPar
\DUrole{xref,myst}{Applications and systems structures} 

\item {} 
\sphinxAtStartPar
\sphinxhref{http://pressbooks-dev.oer.hawaii.edu/messageprocessing/}{Social and cognitive processes involved in human communication} (external)

\end{enumerate}

\sphinxstepscope


\section{Why speech processing?}
\label{\detokenize{Introduction/Why_speech_processing:why-speech-processing}}\label{\detokenize{Introduction/Why_speech_processing::doc}}

\subsection{Why speech?}
\label{\detokenize{Introduction/Why_speech_processing:why-speech}}
\sphinxAtStartPar
Speech is our primary mode of communication; When you want to
communicate something important, you say it face\sphinxhyphen{}to\sphinxhyphen{}face. Think about
your first “I love you”, your last job interview and a nice evening with
friends. Everything \sphinxstyleemphasis{important} is communicated in a spoken form.

\sphinxAtStartPar
Speech is \sphinxstyleemphasis{about} communication. A characteristic trait of humans in
comparison to other animals is our refined abilities to communicate. To
work efficiently as a group, we need to communicate. To learn from our
mistakes, we need to communicate. Where hand waving and smoke signals
can be used to communicate, speech remains as our best way to
communicate abstract thoughts.

\sphinxAtStartPar
However, a common idiom is “\sphinxstyleemphasis{a picture is worth a thousand words}”. It
is also the reason why this document has pictures on the side. They help
in capturing the essential information. An important difference between
speech and images is however that where pictures excel in transmission
of information, speech excels in interaction. The game
“\sphinxhref{https://en.wikipedia.org/wiki/Pictionary}{Pictionary}” is fun because
interaction through a picture is difficult.

\sphinxAtStartPar
Speech interaction is part of a large research in its own right (see
e.g. the book \sphinxhref{http://pressbooks-dev.oer.hawaii.edu/messageprocessing/}{Message
processing}).

\sphinxAtStartPar
\sphinxincludegraphics{{148294653}.jpg}


\subsection{Early communications technology}
\label{\detokenize{Introduction/Why_speech_processing:early-communications-technology}}
\sphinxAtStartPar
The expressive power of speech is tremendous, it is a powerful tool for
interaction, but in early human cultures, it was difficult to \sphinxstyleemphasis{store}
information. Story\sphinxhyphen{}telling was a way to memorize history, but our
capability to accurately reproduce stories is limited.

\sphinxAtStartPar
Cave paintings was an early way to store information more permanently,
and this technology later evolved to stone tablets, papyrus and paper
letters. Such ancient documents provide the most accurate information we
have about our past. We would not know about Socrates, without the
writings by Plato. Innovations in communications technology, such as
cave paintings, book printing and the Internet, have been so important
that they characterize historical eras.

\sphinxAtStartPar
\sphinxincludegraphics{{148294605}.jpg}


\subsection{Evolution of speech technology}
\label{\detokenize{Introduction/Why_speech_processing:evolution-of-speech-technology}}
\sphinxAtStartPar
Telecommunications was another milestone in human history. Though the
telegraph was an effective way for communicating, it also required
specialized training. The invention of the
\sphinxhref{https://en.wikipedia.org/wiki/Telephone}{telephone} in 1849 was
therefore a great invention because it was the first technology to
provide instantaneous telecommunication without specialized training.

\sphinxAtStartPar
The first wireless (\sphinxhref{https://en.wikipedia.org/wiki/Radio}{radio})
transmission of speech came 50 years later in 1900, quickly to become an
important broadcast media. Again, while newspapers had an important role
in broadcasting news, the radio was faster and more accessible (does not
require the ability to read).

\sphinxAtStartPar
Another important step was the introduction of mobile phones in the
1990’s. The importance of its impact is easily demonstrated by the
changes in our behaviour which are a consequence of the new technology:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Before, we would agree on a specific time to talk on the phone \sphinxhyphen{}
“I’ll call you at home around 18 o’clock.”. The other party would
then know to stay at home waiting for the phone call. Today we just
say “I’ll talk to you later”. There is no need to know where the
other person is and we also do not agree on a specific time.

\item {} 
\sphinxAtStartPar
Before, we would agree on a specific time and place where to meet \sphinxhyphen{}
“I’ll meet you at the main building at 12:15.” Both people would
then adjust the timing of their arrival to match the agreed time.
Today, we can just say “I’ll call you when I’m nearby.”

\end{itemize}

\sphinxAtStartPar
In both cases, we are more flexible in our scheduling, making for more
efficient use of time.

\sphinxAtStartPar
\sphinxincludegraphics{{Telefono}.jpg}


\subsection{Further development}
\label{\detokenize{Introduction/Why_speech_processing:further-development}}
\sphinxAtStartPar
We have thus determined that speech is an important and powerful mode of
communication for humans. For improving technology, this gives two
prominent opportunities;
\begin{itemize}
\item {} 
\sphinxAtStartPar
If we can make communication with speech easier using technology, it
can be very useful. For example, if telecommunication, such as
telephony, teleconferences, and voice\sphinxhyphen{}over\sphinxhyphen{}IP, can be improved, then
that would allow people to use speech more efficiently.

\item {} 
\sphinxAtStartPar
We can use to our advantage the people’s preference of speech
communication. For example, interactions with devices and computers
could be improved by allowing spoken interaction with them. In
particular, typing on a keyboard and other tactile interfaces are
difficult for children, the elderly and handicapped people, whereas
a majority of people (but not all) can speak. Similarly, user
interfaces based on visual information is often based on accessing
information and services through menus. Using natural language can
be more intuitive and simple to use; we could just say to the
washing machine “Wash this small amount of dirty curtains. ” instead
of searching for washing options from a menu.

\end{itemize}

\sphinxAtStartPar
The devices and services which use speech and language are extremely
wide\sphinxhyphen{}spread. By now, a majority of people in the world has access to a
mobile phone and there are almost \sphinxhref{https://data.worldbank.org/indicator/IT.CEL.SETS}{8 billion active mobile\sphinxhyphen{}phone
subscriptions}. If we
can improve the technology used by those 8 billion people, by say,
reducing energy consumption, then the impact of such improvements would
be majestic.

\sphinxstepscope

\sphinxAtStartPar
conda install \sphinxhyphen{}c conda\sphinxhyphen{}forge jupyter\_contrib\_nbextensions \# Speech production and acoustic properties


\section{Physiological speech production}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:physiological-speech-production}}\label{\detokenize{Introduction/Speech_production_and_acoustic_properties::doc}}

\subsection{Overview}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:overview}}
\sphinxAtStartPar
When a person has the urge or intention to speak, her or his brain forms
a sentence with the intended meaning and maps the sequence of words into
physiological movements required to produce the corresponding sequence
of speech sounds. The neural part of speech production is not discussed
further here.

\sphinxAtStartPar
The physical activity begins by contracting the lungs, pushing out air
from the lungs, through the throat, oral and nasal cavities. Airflow in
itself is not audible as a sound \sphinxhyphen{} sound is an oscillation in air
pressure. To obtain a sound, we therefore need to obstruct airflow to
obtain an oscillation or turbulence. Oscillations are primarily produced
when the \sphinxhref{https://en.wikipedia.org/wiki/Vocal\_cords}{\sphinxstyleemphasis{vocal folds}} are
tensioned appropriately. This produces \sphinxstyleemphasis{voiced sounds} and is perhaps
the most characteristic property of speech signals. Oscillations can
also be produced by other parts of the speech production organs, such as
letting the tongue oscillate against the teeth in a rolling /r/, or by
letting the uvula oscillate in the airflow, known as the uvular trill
(viz. something like a guttural /r/). Such trills, both with the tongue
and the uvula, should however not be confused with voiced sounds, which
are always generated by oscillations in the vocal folds. Sounds without
oscillations in the vocal folds are known as \sphinxstyleemphasis{unvoiced sounds.}

\sphinxAtStartPar
Most typical unvoiced sounds are caused by turbulences produced by
static constrictions of airflow in any part of the air spaces above the
vocal folds (viz. larynx, pharynx and oral or nasal cavities). For
example, by letting the tongue rest close to the teeth, we obtain the
consonant /s/, and by stopping and releasing airflow by closing and
opening the lips, we obtain the consonant /p/. A further particular
class of phonemes are nasal consonants, where airflow through the mouth
is stopped entirely or partially, such that a majority of the air flows
through the nose.


\subsection{The vocal folds}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:the-vocal-folds}}
\sphinxAtStartPar
The \sphinxstyleemphasis{vocal folds,} also known as vocal cords, are located in the throat
and oscillate to produce voiced sounds. The opening between the vocal
folds (the empty space between the vocal folds) is known as the
\sphinxstyleemphasis{glottis}. Correspondingly, the airspace between the vocal folds and the
lungs is known as the \sphinxstyleemphasis{subglottal} area.

\sphinxAtStartPar
When the pressure below the glottis, known as the \sphinxstyleemphasis{subglottal pressure}
increases, it pushes open the vocal folds. When open, air rushes through
the vocal folds. The return movement, again closing the vocal folds is
mainly caused by the \sphinxhref{https://en.wikipedia.org/wiki/Venturi\_effect}{Venturi
effect}, which causes a
drop in air pressure between the vocal folds when air is flowing through
them. As the vocal folds are closing, they will eventually clash
together. This sudden stop of airflow is the largest acoustic event in
the vocal folds and is known as the \sphinxstyleemphasis{glottal excitation}.

\sphinxAtStartPar
In terms of airflow, the effect is that during the \sphinxstyleemphasis{closed phase} (when
the vocal folds are closed), there is no airflow. At the beginning of
the \sphinxstyleemphasis{open phase} (when the vocal fold are open), air starts to flow
through the glottis and obviously, with the closing of the vocal folds
also air flow is decreasing. However, due to the momentum of air itself,
the movement of air occurs slightly after the vocal folds. In other
words, there is a phase\sphinxhyphen{}difference between vocal folds movement and
glottal airflow waveform.

\sphinxAtStartPar
The frequency of vocal folds oscillation is dependent on three main
components; amount of lengthwise tension in the vocal folds, pressure
differential above and below the vocal folds, as well as length and mass
of the vocal folds. Pressure and tension can be intentionally changed to
cause a change in frequency. The length and mass of the vocal folds are
in turn correlated with overall body size of the speaker, which explains
the fact that children and females have on average a higher pitch than
male speakers.

\sphinxAtStartPar
Note that the frequency of the vocal folds refers to the actual physical
phenomenon, whereas \sphinxstyleemphasis{pitch} refers to the perception of frequency. There
are many cases where these two may differ, for example, resonances in
the vocal tract can emphasise harmonics of the fundamental frequency
such that the harmonics are louder than the fundamental, and such that
we perceive one of the harmonics as the fundamental. The perceived pitch
is then the frequency of the harmonic instead of the fundamental.


\subsection{The vocal tract}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:the-vocal-tract}}
\sphinxAtStartPar
The vocal tract, including the larynx, pharynx and oral cavities, have a
great effect on the timbre of the sound. Namely, the shape of the vocal
tract determines the resonances and anti\sphinxhyphen{}resonances of the acoustic
space, which boost and attenuate different frequencies of the sound. The
shape is determined by a multitude of components, in particular by the
position of the jaw, lips and tongue. The resonances are easily modified
by the speaker and perceived by the listener, and they can thus be used
in communication to convey information. Specifically, the acoustic
features which differentiate \sphinxstyleemphasis{vowels} from each other are the
frequencies of the resonances in the vocal tract, corresponding to
specific \sphinxstyleemphasis{places} of articulation primarily in terms of tongue position.
Since the air can flow relatively unobstructed, vowel sounds tend to
have high energy and loudness compared to \sphinxstyleemphasis{consonants}.

\sphinxAtStartPar
In \sphinxstyleemphasis{consonant} sounds, there is a partial or full obstruction at some
part of the vocal tract. For instance, \sphinxstyleemphasis{fricative consonants} are
characterized by a narrow gap between the tongue and front/top of the
mouth, leading to hiss\sphinxhyphen{}like turbulent air flow. In plosives, the airflow
in the vocal tract is fully temporarily obstructed. As an example,
\sphinxstyleemphasis{bilabial plosives} are characterized by temporary closure of the lips,
which leads to accumulation of air pressure in the vocal tract due to
sustained lung pressure. When the lips are opened, the accumulated air
is released together with a short \sphinxstyleemphasis{burst} sound (plosion) that has
impulse\sphinxhyphen{} and noise\sphinxhyphen{}like characteristics. Similarly to vowels, the
\sphinxstyleemphasis{place} of the obstruction in the mouth (i.e., place of articulation)
will affect the acoustic characteristics of the consonant sound by
modifying the acoustic characteristics of the vocal tract. In addition,
\sphinxstyleemphasis{manner} of articulation is used to characterize different consonant
sounds, as there are several ways to produce speech while the position
of the primary obstruction can remain the same (e.g., short \sphinxstyleemphasis{taps} and
\sphinxstyleemphasis{flaps}, repeated \sphinxstyleemphasis{trills,} or already mentioned narrow constrictions
for \sphinxstyleemphasis{fricatives}).

\sphinxAtStartPar
In terms of vocal tract shape, a special class of consonants are the
\sphinxstyleemphasis{nasals}, which are produced with velum (a soft structure at the back
top of the oral cavity) open, thereby allowing air to flow to the \sphinxstyleemphasis{nasal
cavity}. When the velum is open, the vocal tract can be viewed as a
shared tube from the larynx to the back of the mouth, after which the
tract is divided into two parallel branches consisting of the oral and
nasal cavities. Coupling of the nasal cavity to the vocal tract has a
pronounced impact on the resonances and anti\sphinxhyphen{}resonance of the tract.
This is commonly perceived as \sphinxstyleemphasis{nasalization} of speech sounds by
listeners.


\bigskip\hrule\bigskip


\sphinxAtStartPar
Side\sphinxhyphen{}view of the speech production organs.

\sphinxAtStartPar
\sphinxincludegraphics{{155469911}.png}
\sphinxincludegraphics{{201855184}.png}

\sphinxAtStartPar
By BruceBlaus. When using this image in external sources it can be cited as:\sphinxhref{http://Blausen.com}{Blausen.com} staff (2014). “Medical gallery
of Blausen Medical 2014”. WikiJournal of Medicine 1 (2).
DOI:10.15347/wjm/2014.010. ISSN 2002\sphinxhyphen{}4436. \sphinxhyphen{} Own work, CC BY 3.0,
\sphinxurl{https://commons.wikimedia.org/w/index.php?curid=29294598}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Vocal folds as seen from above.

\sphinxAtStartPar
\sphinxincludegraphics{{Larynx_(top_view)}.jpg}


\bigskip\hrule\bigskip


\sphinxAtStartPar
The motion of vocal folds seen from the front (or back).

\sphinxAtStartPar
\sphinxincludegraphics{{155469942}.png}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Organs in the mouth.

\sphinxAtStartPar
\sphinxincludegraphics{{155469920}.jpg}

\sphinxAtStartPar
The four images above are from Wikipedia.


\bigskip\hrule\bigskip



\section{Acoustic properties of speech signals}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:acoustic-properties-of-speech-signals}}
\sphinxAtStartPar
The most important acoustic features of a speech signal are (roughly
speaking)
\begin{itemize}
\item {} 
\sphinxAtStartPar
The \sphinxstyleemphasis{resonance of the vocal tract}, especially the two lowest
resonances, known as the \sphinxstyleemphasis{formants} F1 and F2 (see figure below). The resonance structure can be easily examined by drawing an
“\sphinxstyleemphasis{envelope”} above the spectrum, that is, to draw a smooth line
which goes just above the spectrum, as seen on the figure below. We thus obtain the \sphinxstyleemphasis{spectral envelope}, which characterizes
the macro\sphinxhyphen{}shape of the spectrum of a speech signal, and which is
often used to model speech signals.

\item {} 
\sphinxAtStartPar
The \sphinxstyleemphasis{fundamental frequency} of a speech signal or its absence
carries a lot of information. Per definition, voiced and unvoiced
phonemes, respectively, are those with or without an oscillation in
the vocal folds. Due to its prominence, we categorize phonemes
according to whether they are voiced or unvoiced.The airflow which passes through the oscillating vocal folds will
generally have a waveform which resembles a \sphinxhref{https://en.wikipedia.org/wiki/Rectifier\#Half-wave\_rectification}{half\sphinxhyphen{}wave rectified
sinusoid}.
That is, airflow is zero when the vocal folds are closed (closed
phase) and during the open time (open phase) the waveform resembles
(somewhat) the shape of the upper part of a sinusoid. The spectrum
of this waveform will therefore have the structure of a harmonic
signal, that is, the spectrum will have peaks at the fundamental
frequency and its integer multiples (see figure below).In most languages, pitch does not differentiate between phonemes.
However, in languages that are known as \sphinxstyleemphasis{tonal} languages, the shape
of the pitch contour over time does bear semantic meaning (see
\sphinxhref{https://en.wikipedia.org/wiki/Tone\_(linguistics)}{Wikipedia:Tone
(linguistics)} for
a nice sound sample). Pitch contours are however often used to
encode \sphinxstyleemphasis{emphasis} in a sentence. Roughly speaking, exerting more
physical effort on a phoneme raises its pitch and intensity, and
that is usually interpreted as emphasis, that is, the word (or
phoneme) with emphasis is more important than other words (or
phonemes) in a sentence.

\item {} 
\sphinxAtStartPar
Signal \sphinxstyleemphasis{amplitude} or \sphinxstyleemphasis{intensity} over time is another important
characteristic and in its most crude form can be the difference
between speech and silence (see also \DUrole{xref,myst}{Voice activity detection
(VAD)}). Furthermore, there are
phonemes characterized by their temporal structure; in particular,
\sphinxstyleemphasis{stop} and \sphinxstyleemphasis{plosive\sphinxhyphen{}consonants}, where airflow is stopped and
subsequently released (e.g. /p/, /t/ and /k/). While the stop\sphinxhyphen{}part
is not prominently audible, it is the contrast of a silence before a
burst of energy which characterizes these consonants.

\end{itemize}

\sphinxAtStartPar
The spectrum of a speech segment annotated with its formants \(F_k\) (for \( k\geq 1 \) ) as well as the fundamental frequency \(F_0\) and
its integer multiples \(kF_0\). Note that it is not always clear
where the formants are; here formants F4 and F5 are not prominent and
therefore difficult to
locate.
\sphinxincludegraphics{{149889168}.png}

\sphinxAtStartPar
The waveform of a sentence of speech, illustrating variations in
amplitude and intensity.

\sphinxAtStartPar
\sphinxincludegraphics{{148294966}.png}


\section{Physiological modelling}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:physiological-modelling}}

\subsection{Vocal tract}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:vocal-tract}}
\sphinxAtStartPar
Vowels are central to spoken communication, and vowels are determined by
the shape of the vocal tract. Modelling the vocal tract is therefore of
particular interest.


\subsubsection{Simple models}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:simple-models}}
\sphinxAtStartPar
The vocal tract is essentially a tube of varying length. It has a
90\sphinxhyphen{}degree bend, where the throat turns into the mouth, but the acoustic
effect of that bend is minor and can be ignored in simple models. The
tube has two pathways, through the oral and nasal cavities. The acoustic
effect of the oral cavity dominates the output signal such that, roughly
speaking, the oral cavity generates resonances to the output sound,
while the nasal cavities contributes mainly anti\sphinxhyphen{}resonances (dips or
valleys) to the spectral envelope. Presence of energy is perceptually
more important than absence of energy and anti\sphinxhyphen{}resonances can therefore
be ignored in simple models.

\sphinxAtStartPar
A very simple model is thus a straight cylindrical tube sub\sphinxhyphen{}divided into
constant radius segments of equal length (see illustration below). If we further assume that the tube\sphinxhyphen{}segments are lossless, then
this tube is analytically equivalent with a \DUrole{xref,myst}{linear
predictor}. This is a fantastic simplification in the
sense that from a physiologically motivated model we obtain a
analytically reasonable model whose parameters we can readily estimate
from observed signals. In fact, the temporal correlation of speech
signals can be very efficiently modelled with linear predictors. It
offers a very attractive connection between physiological and signal
modelling. Unfortunately, it is not entirely accurate.

\sphinxAtStartPar
Though speech signals are very efficiently modelled by linear
predictors, and linear predictors are analytically equivalent with
tube\sphinxhyphen{}models, \sphinxstyleemphasis{linear predictors estimated from sound signals need not
correspond to the tube which generated the sound}. The mismatch in the
shape of estimated and real tubes is due to two primary reasons;
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Estimation of linear predictive coefficients assumes that the
excitation, viz. the glottal excitation, is uncorrelated (white
noise). This is certainly an incorrect assumption. Though the
periodic structure of the glottal excitation does not much bias
linear predictors, glottal excitations are also dominated by
low\sphinxhyphen{}frequency components which will bias the linear predictor. The
linear predictor cannot make a distinction between features of the
glottal excitation and contributions of the vocal tract, but model
both indiscriminately. We also do not know the precise contribution
of the glottal excitation such that it is hard to compensate for it.

\item {} 
\sphinxAtStartPar
The analytical relationship between coefficients of the linear
predictor and the radii of the tube\sphinxhyphen{}model segments is highly
non\sphinxhyphen{}linear and sensitive to estimation errors. Small errors in
predictor parameters can have large consequences in the shape of the
tube model.

\end{enumerate}

\sphinxAtStartPar
Still, since linear predictors are efficient for modelling speech, they
are useful in speech modelling even if the connection to tube\sphinxhyphen{}modelling
is sensitive to errors. Linear prediction is particularly attractive
because it gives computationally efficient algorithms.


\subsubsection{Advanced models}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:advanced-models}}
\sphinxAtStartPar
When more accurate modelling of the vocal tract is required, we have to
re\sphinxhyphen{}evaluate our assumptions. With \sphinxhref{https://en.wikipedia.org/wiki/Digital\_waveguide\_synthesis}{digital
waveguides}
we can readily formulate models which incorporate a second pathway
corresponding to the nasal tract. A starting point for such models is
linear prediction, written as a delay\sphinxhyphen{}line with reflections
corresponding to the interfaces between tube\sphinxhyphen{}segments. The nasal tract
can then be introduced by adding a second delay line. Such models are
computationally efficient in synthesis of sounds, but estimating their
parameters from real sounds can be difficult.

\sphinxAtStartPar
Stepping up the accuracy, we then already go into full\sphinxhyphen{}blown physical
modelling such as \sphinxhref{https://en.wikipedia.org/wiki/Finite\_element\_method}{finite\sphinxhyphen{}element
methods} (FEM).
Here, for example, the air\sphinxhyphen{}volume of the vocal tract can be split into
small interacting elements governed by \sphinxhref{https://en.wikipedia.org/wiki/Fluid\_dynamics}{fluid
dynamics}. The more dense
the mesh of the elements is, the more accurately the model corresponds
to physical reality. Measuring and modelling the vocal tract with this
method is involved and an \sphinxhref{http://speech.math.aalto.fi/about.html}{art form of its
own}.

\sphinxAtStartPar
\sphinxincludegraphics{{149889201}.png}

\sphinxAtStartPar
Illustration of a vocal\sphinxhyphen{}tract tube\sphinxhyphen{}model consisting of piece\sphinxhyphen{}wise
constant\sphinxhyphen{}radius tube\sphinxhyphen{}segments.


\subsection{Glottal activity}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:glottal-activity}}
\sphinxAtStartPar
As characterization of the glottal flow, we define events of a single
glottal period as follows (illustrated in the figure below):
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Opening and closing time} (or instant), are the points in time
where respectively, glottal folds open and close, and where glottal
flow starts and ends.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Open and closed} \sphinxstyleemphasis{phase}, are the periods during which the glottis
is open and closed, respectively.

\item {} 
\sphinxAtStartPar
The length of time when glottis is open and closed are,
respectively,  known as \sphinxstyleemphasis{open time (OT)} and \sphinxstyleemphasis{closed time (CT)}.
Consequently, the period length is \(T=OT+CT\).

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Opening and closing phases} are the portions of the open phase,
when the glottis is opening and closing, respectively.

\item {} 
\sphinxAtStartPar
The steepness of the closing phase is related to the “agressiveness”
of the pulse, that is, it relates to the tension of glottal folds
and is characterized by the (negative) \sphinxstyleemphasis{peak of the glottal flow
derivative}.

\item {} 
\sphinxAtStartPar
All parameters describing a length in time are often further
normalized by the period length \(t\).

\end{itemize}

\sphinxAtStartPar
Like modelling of the vocal tract, also in modelling glottal activity,
there is a range of models of different complexity:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Maximum\sphinxhyphen{}phase linear prediction}; The most significant event in a
single glottal flow pulse is its closing instant; the preceding
waveform is smooth but the closing event is abrupt. The waveform can
thus be interpreted as the impulse response of an IIR filter but
turned backwards, which also known as the impulse response of a
maximum\sphinxhyphen{}phase linear predictor (the figure on the right was
generated with this method). The beauty of this method is that it is
similar to vocal tract modelling with linear prediction, such that
we are already familiar with the method and computational complexity
is simple. Observe, however, that maximum\sphinxhyphen{}phase filters are by
definition unstable (not realizable), but we have to always process
the signal backwards, which complicates systems design.

\item {} 
\sphinxAtStartPar
The \sphinxstyleemphasis{Liljencrantz\sphinxhyphen{}Fant (LF) \sphinxhyphen{}model} is a classical model of the
glottal flow, the original form of which is a function of four
parameters (\sphinxhref{http://www.speech.kth.se/prod/publications/files/qpsr/1985/1985\_26\_4\_001-013.pdf}{defined in
article}).
It is very useful and influential because it parametrizes the flow
with a low number of easily understandable parameters. The
compromise is that the parameters are not easily estimated from real
signals and that it is based on anecdotal evidence of glottal flow
shapes and if it were presented today, to be widely accepted, we
would require more evidence to support it.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{\sphinxhref{https://en.wikipedia.org/wiki/Harmonic\_oscillator\#Spring/mass\_system}{Mass\sphinxhyphen{}spring
systems}};
the opposing glottal folds can be modelled as simple point\sphinxhyphen{}masses
connected with damped springs to fixed points. When subjected to the
Venturi\sphinxhyphen{}forces generated by the airflow, these masses can be brought
to oscillate like the vocal folds. Such models are attractive
because, again, their parameters have physical interpretations, but
since their parameters are difficult to estimate from real\sphinxhyphen{}world
data and they oscillate only a limited range of the parameters,
their usefulness in practical applications is limited.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Finite\_element\_method}{\sphinxstyleemphasis{Finite\sphinxhyphen{}element methods
(FEM)}} are
again the ultimate method for accurate analysis, suitable for
example in medical analysis, yet the computational complexity is
prohibitively large for consumer applications.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175513693}.png}

\sphinxAtStartPar
Illustration of a glottal flow pulse, its derivative and a sequence of
glottal flow pulses (corresponding sound below).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Lip radiation}
\label{\detokenize{Introduction/Speech_production_and_acoustic_properties:lip-radiation}}
\sphinxAtStartPar
Having travelled through the vocal tract, air exits primarily through
the mouth and in some extent through the nose. In leaving this tube, it
enters the free field where airflow in has little effect. Recall that
sounds are, instead, variations in air pressure. At the transition from
the tube to the free field, variations in air flow become variations in
air pressure.

\sphinxAtStartPar
The physics of this phenomenon are governed by \sphinxhref{https://en.wikipedia.org/wiki/Fluid\_dynamics}{fluid
dynamics}, an advanced
topic, but heuristically we can imagine that variations air pressure is
related to variations in airflow. Thus if we take the derivative of the
airflow, we get an approximation of its effect on air pressure
\$\( sound(t) \approx \frac d{dt} flow(t), \)\(
where \)t\$ is time.

\sphinxAtStartPar
Often we deal with signals sampled at time indices \(n\), where the
derivative can be further approximated by the first difference
\$\( 
sound(n) \approx g \left[flow(n) - flow(n-1)\right], 
\)\(
where \)g>0\$ is a scalar gain coefficient.

\sphinxstepscope


\section{Linguistic structure of speech}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:linguistic-structure-of-speech}}\label{\detokenize{Introduction/Linguistic_structure_of_speech::doc}}

\subsection{Overview}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:overview}}
\sphinxAtStartPar
Besides their acoustic characteristics, speech signals can be
characterized in terms of their \sphinxstyleemphasis{linguistic structure}. Linguistic
structure refers to the recurring regularities in spoken language as
described by linguistic theories, such as what are the basic building
blocks of speech and how they organized. Linguistic descriptions provide
a means for systematic interpretation, conceptualization, and
communication of speech\sphinxhyphen{}related phenomena.

\sphinxAtStartPar
Many linguistic properties of speech, such as syllables and words, also
have their analogs in written language. However, it is important to
distinguish the two from each other: While written language consists of
discrete categorical elements (sequences of letters, whitespaces, and
punctuation marks), speech signals are always continous and
non\sphinxhyphen{}categorical. This is due to the motor behavior in speech production,
which operates in real\sphinxhyphen{}time and finite speed under physical and
neurophysiological constraints. Therefore, the resulting speech signal
also flows in continuous time and frequency. In addition, where speech
carries extra information in terms of \sphinxstyleemphasis{how} things are said and what are
the \sphinxstyleemphasis{characteristics of the speaker}, written language is impoverished
of these aspects. In contrast, written language uses lexical and
syntactic means and special characters (and more recently, emoticons) to
differentiate more fine\sphinxhyphen{}grained meanings, such as conveying emotional
content or differentiating questions from statements. Finally, only few
languages have clear one\sphinxhyphen{}to\sphinxhyphen{}one relationship between how words are
pronounced and how they are written. Therefore, a separate system is
needed to describe structure of spoken language, and one should not
equate units of written language to that of a spoken one by default.
Naturally, the two have a systematic relationship in order to enable
reading and writing. However, this relationship also varies from
language to another.  **

\sphinxAtStartPar
Within the broad field of linguistics, \sphinxstyleemphasis{phonetics} is its branch that
focuses on understanding the physical basis of speech production, speech
signals, and speech perception. In contrast, \sphinxstyleemphasis{phonology} is a branch of
linguistics that studies sound systems of languages (both spoken and
signed). While both attempt to describe how spoken language is
organized, phonetic description attempts to be faithful to the acoustic,
articulatory and auditory aspects of the speech signal. Phonetics are
therefore strongly grounded to the measurable phenomena in the physical
world. In contrast, phonological description consists of abstract speech
units that allow more convenient study of how sounds of a language
combine to create meaningful messages. For this, phonological
description usually gets rid of signal variation that does not impact
meaning of the speech. To give an example, different speakers of the
same language may use different pronunciations of the same word, still
resulting in the same phonological form. In contrast, accurate phonetic
description would reflect the pronunciation\sphinxhyphen{}dependent differences,
allowing documentation and study of such differences. Since speech
processing is primarily concerned with physical (digitized) speech
signals and how to deal with them, we will use phonetics as the primary
language of description. However, the basic relationship between
phonetic and phonological units is also briefly discussed below.


\subsection{Elementary units of spoken language}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:elementary-units-of-spoken-language}}
\sphinxAtStartPar
In terms of basic phonetic organization, speech can be seen as a
hierarchical organization of elementary units of increasing time\sphinxhyphen{}scale
(Fig. 1). At the lowest level of hierarchy, there are \sphinxstyleemphasis{phones}, which
are considered as physical realizations of more abstract \sphinxstyleemphasis{phonemes}.
Sequences of phones are organized into \sphinxstyleemphasis{syllables}, and syllables make
up \sphinxstyleemphasis{words} (where each word consists of one or more syllables). One or
more words then make up utterances. Phones are sometimes referred to as
\sphinxstyleemphasis{segmental units}, and speech phenomena, such as intonation, taking
place at time\sphinxhyphen{}scales larger than individual phones are called as
\sphinxstyleemphasis{suprasegmental phenomena}. In addition, speech is sometimes said to
have so\sphinxhyphen{}called \sphinxstyleemphasis{double articulation} (aka. \sphinxstyleemphasis{duality of patterning}).
This refers to the fact that meaningful units of speech (words,
utterances) consist of non\sphinxhyphen{}meaningful units (phones/phonemes) that still
signify distinctions in meaning. At all levels, units and their relative
organization are language\sphinxhyphen{}dependent, such as which phones, syllables,
and words are employed and how they are allowed follow each other.
However, there are also certain common tendencies (aka. \sphinxstyleemphasis{l}\sphinxhref{https://en.wikipedia.org/wiki/Linguistic\_universal}{\sphinxstyleemphasis{inguistic
universals}}) that
result from restrictions in the speech production and perception
apparati or due to other shared characteristics of \sphinxhref{https://en.wikipedia.org/wiki/Natural\_language}{natural
languages}.

\sphinxAtStartPar
\sphinxincludegraphics{{197424504}.png}
\sphinxstylestrong{Fig. 1:}
\sphinxstylestrong{An example of the hierarchical organization of speech in terms of
phones, syllables, and words for an Estonian speech sample. Two
suprasegmental features, namely F0 and intensity, are also shown on top.
In this example, syllables are denoted in terms of their phonetic
constituents while words are represented orthographically.} \sphinxstylestrong{Example
annotations taken from Phonetic Corpus of Estonian Spontaneous Speech
(reproduced with permission) and represented in graphical form using
Praat.}

\sphinxAtStartPar
When discussing units of speech, it is often useful to distinguish \sphinxstyleemphasis{unit
types} from \sphinxstyleemphasis{unit tokens.} Type refers to a unique unit category, such
all {[}r{]} sounds belong to the same phone type. Token refers to an
individual realization of a type. For instance, word “roar” has two
{[}r{]} tokens in it.


\subsubsection{Phones}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:phones}}
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Phone\_(phonetics)}{Phones} are the
elementary units of speech, associated with \sphinxstyleemphasis{articulatory gestures}
responsible for producing them and with \sphinxstyleemphasis{acoustic cues} that make them
distinct from other phones. Phonetic \sphinxstyleemphasis{transcription} is the process of
marking down phones of speech with symbols (denoted with brackets {[}
{]}). Phonetic transcription often makes use of the \sphinxhref{https://en.wikipedia.org/wiki/International\_Phonetic\_Alphabet}{International
Phonetic
Alphabet}
(IPA). On a high level, phones can be divided into \sphinxstyleemphasis{vowels}
(e.g., {[}a{]}, {[}i{]}, and {[}u{]})  and \sphinxstyleemphasis{consonants} (e.g.\sphinxstyleemphasis{,} {[}p{]}, {[}b{]},
{[}s{]}). While all vowels are voiced sounds (see {\hyperref[\detokenize{Introduction/Speech_production_and_acoustic_properties::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{speech
production}}}}), consonants can
be voiced or unvoiced.

\sphinxAtStartPar
The primary determiner of vowel identity is position of the narrowest
gap in the vocal tract (Fig. 2), as primarily controlled by tongue
position in the mouth. Vowels can be categorized in terms of
their \sphinxstyleemphasis{openness/closeness} (how wide is the narrowest part of the vocal
tract between tongue and top of the mouth), and \sphinxstyleemphasis{frontness/backness}
(how front/back is the tongue in the mouth). In additon, vowels with the
same place of articulation can change depending on \sphinxstyleemphasis{lip rounding} (e.g.,
{[}o{]} versus {[}u{]}). Protrusion of lips during the rounding increases
the effective length of the vocal tract, hence altering the resonance
frequencies of the tract. A special vowel called “schwa” ({[}ə{]})
corresponds to an articulatory configuration, where jaw, lips, and
tongue are completely relaxed, hence corresponding a central mid vowel.

\sphinxAtStartPar
Fig. 2: IPA chart for vowels (reproduced by CC BY\sphinxhyphen{}SA 3.0)

\sphinxAtStartPar
Consonants can be categorized in terms of their \sphinxstyleemphasis{manner and place of
articulation.} Figure 3 shows IPA chart for consonants organized in
terms of place of articulation (columns)and manner of articulation
(rows).

\sphinxAtStartPar
Place of articulation refers to the point of tighest constriction in the
vocal tract, as in vowels. However, in consonants, the gap at the place
of constriction is smaller or the airflow in the tract is completely
blocked for a period of time. In addition, the constriction can be
created at different positions of the vocal tract using different
articulators beyond the tongue, such as using lips, glottis, or uvula.

\sphinxAtStartPar
Manner of articulation refers to the manner that the constriction is
created, including its temporal characteristics. For instance,
\sphinxstyleemphasis{plosives} (e.g., unvoiced {[}k{]}, {[}p{]} or voiced {[}g{]}, {[}b{]}) consist
of a complete blockage, aka. \sphinxstyleemphasis{closure,} of the vocal tract. This causes
accumulation of air pressure in the tract before the blockage, which
then results in as a noisy burst of airflow when the closure is
released. \sphinxstyleemphasis{Trill consonants}, such as {[}r{]}, consist of several rapid
and subsequent closures of the tract caused by tongue vibrating against
the oral cavity structures at the given place of articulation (e.g.,
tongue tip vibrating against alveolar ridge in {[}r{]}). \sphinxstyleemphasis{Taps and flaps}
are similar to trills, but only consist of one quick closure and its
release. \sphinxstyleemphasis{Fricatives} are turbulent sounds where the constriction is so
narrow that the air flowing though it becomes turbulent. This turbulence
causes the “hissing” sound characteristic of fricatives, such as in
{[}s{]}, {[}f{]}, and {[}h{]} (e.g., as in “\sphinxstyleemphasis{she}” or “\sphinxstyleemphasis{foam}”). *Nasals *are
sounds where the oral cavity has a complete closure, but the air can
enter the nasal cavity through the velar port and exit through nostrils,
adding another branch to the vocal tract with its own resonances and
antiresonances (see also {\hyperref[\detokenize{Introduction/Speech_production_and_acoustic_properties::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{speech
production}}}}). \sphinxstyleemphasis{Approximants}
are sounds where articulators approach each other without becoming
narrow enough to introduce turbulence to the sound. Among the
approximants, \sphinxstyleemphasis{g}lides are sounds similar to vowels, but act as syllable
boundaries instead of syllabic nuclei (see below) typically with
articulatory movement taking place throughout the sound (e.g., {[}y{]} as
in “yes” or {[}w{]} in “water”). Lateral approximants consist of sounds
where some part of the tongue touches the roof of the mouth but the air
can flow freely from both sides of the tongue (e.g., {[}l{]} as in
“love”). There are also other categories of consonants and ways to
categorize them, and an interested reader is suggested to consult some
phonetics textbook for more information.

\sphinxAtStartPar
\sphinxincludegraphics{{197423276}.png}
\sphinxstylestrong{Fig.3: IPA chart for pulmonic consonants (reproduced by CC BY\sphinxhyphen{}SA
3.0).}

\sphinxAtStartPar
The majority of consonant sounds, including those reviewed above and
listed in Fig. 3, are called \sphinxstyleemphasis{pulmonic consonants.} This means that the
air flow (energy) for sound production originates from lung pressure,
whether voiced or not. Besides the pulmonic sounds, some languages make
use of \sphinxstyleemphasis{non\sphinxhyphen{}pulmonic consonants}. These can include \sphinxstyleemphasis{implosives}
(plosives that are formed during inhalation of air), \sphinxstyleemphasis{clicks} (produced
by subpressurized pockets of air between two concurrent closures that
are then suddenly released), and \sphinxstyleemphasis{ejectives} (where the excitation
energy for the sound is formed at the glottis by lowering the phrarynx,
closing the glottis, and then raising the pharynx, resulting in
increased air pressure in the tract while an obstruction is maintained
at the place of articulation).

\sphinxAtStartPar
\sphinxstyleemphasis{\sphinxhref{https://en.wikipedia.org/wiki/Coarticulation}{Coarticulation}} is an
important speech phenomenon where realization of speech sounds is
affected by the neighboring speech sounds. This is since articulatory
gestures for speech production are almost always anticipating the
production of the next sound (aka. \sphinxstyleemphasis{anticipatory coarticulation)} or
still recovering their positions from the previous sound (aka.
\sphinxstyleemphasis{perseverative} coarticulation). Since all articulators can only move
with a finite speed from one articulatory configuration to another,
coarticulation is present in virtually all observable speech beyond
isolated vowels. Given that phones are the smallest sound units of a
language, coarticulation can have a large effect on the acoustic form
that phones take in actual continuous speech, introducing additional
non\sphinxhyphen{}phonemic variability (see below) in the acoustical form of phones.


\paragraph{Phones vs. phonemes}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:phones-vs-phonemes}}
\sphinxAtStartPar
As mentioned above, phones are sounds of a language that have an
articulatory, and thereby also acoustic, basis. Another commonly
encountered basic unit of speech is a \sphinxstyleemphasis{phoneme} (denoted with slashes /
/)\sphinxstyleemphasis{.} Phonemes are defined in terms of their meaning contrasting
function: two different phones of a language are also different
phonemes, if they can change the meaning of a word.  For example,
consider words “\sphinxstyleemphasis{cat}” ({[}k{]} {[}ae{]} {[}t{]}) and “\sphinxstyleemphasis{bat}” ({[}b{]}
{[}ae{]} {[}t{]}). In case of English, the initial {[}k{]} and {[}b{]} phones
are also distinct phonemes /k/ and /b/, as they change the meaning of
the otherwise identical word. Also note that {[}k{]} {[}ae{]} {[}t{]} and
{[}b{]} {[}ae{]} {[}t{]} are so\sphinxhyphen{}called \sphinxstyleemphasis{minimal pairs}, as they only differ by
one phoneme. A good rule of thumb is that \sphinxstyleemphasis{phones are defined in terms
of their articulatory or acoustic properties,} whereas \sphinxstyleemphasis{each} *phonemic
category consists of all possible sounds that can be substituted for
each other without affecting the meaning of any word in the given
language. *

\sphinxAtStartPar
\sphinxstyleemphasis{Allophones} are the alternative phones that all stand for the same
phoneme in the given language. For instance, phones {[}r{]} and {[}l{]} can
be considered as allophones of the same Japanese phoneme, as they can be
used interchangeable in Japanese without affecting the communicated
meanings. In English, {[}r{]} and {[}l{]} are not allophones, as they
distinguish meanings (e.g., as in “\sphinxstyleemphasis{lock}” and “\sphinxstyleemphasis{rock}”). In fact,
listeners tend to become worse in discriminating those non\sphinxhyphen{}native
phonemic contrasts from each other that do not mark a phonemic contrast
in their native language. This change in discrimination comes with
language experience, and young infants start with the capability to
discriminate both native and non\sphinxhyphen{}native contrasts (e.g., Werker \& Tees,
1984).


\subsubsection{Syllables}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:syllables}}
\sphinxAtStartPar
The second unit in the “size hierarchy” of spoken language is a
syllable. Syllables are sequences of sounds (one or more), consisting of
a syllable \sphinxstyleemphasis{nucleus} (typically a vowel) and optional initial and final
sound sequences, also known as \sphinxstyleemphasis{onset} and \sphinxstyleemphasis{coda}, respectively. Onsets
and coda tend to have consonant sounds. Sometimes onset is separated
from the \sphinxstyleemphasis{rime}, where the rime then consists of the nucleus and coda.
Syllables can be considered as rhythmic units, as the alternation
between consonants and vocalic syllabic nuclei give rise to the typical
rhythmic patterns of different languages, as vowels generally have
higher energy (are louder) than consonants. Concept of \sphinxstyleemphasis{sonority
sequencing principle} (SSP) refers to this sequential alternation
between less loud consonants and louder syllable nuclei. More
specifically, in SSP, the first phone of syllable onset is supposed to
be the least sonorous of the sounds preceding the syllable nucleus. Then
the sonority increases towards the nucleus if more than one consonant
exists in the onset. In the same manner, sonority of the consonants in
the syllable coda (offset) decreases towards the end of the syllable.
\sphinxstyleemphasis{Sonority hierarchy} determines the relative sonority (“loudness”) of
different phones. As
\sphinxhref{https://en.wikipedia.org/wiki/Sonority\_Sequencing\_Principle}{wikipedia}
states, “*typically they *{[}relative sonorities{]} \sphinxstyleemphasis{are
\sphinxhref{https://en.wikipedia.org/wiki/Vowel}{vowel} >
\sphinxhref{https://en.wikipedia.org/wiki/Semivowel}{glide} >
\sphinxhref{https://en.wikipedia.org/wiki/Liquid\_consonant}{liquid}
>
\sphinxhref{https://en.wikipedia.org/wiki/Nasal\_consonant}{nasal}
> \sphinxhref{https://en.wikipedia.org/wiki/Obstruent}{obstruent} (or
>
\sphinxhref{https://en.wikipedia.org/wiki/Fricative\_consonant}{fricative}
>
\sphinxhref{https://en.wikipedia.org/wiki/Stop\_consonant}{plosive}
>
\sphinxhref{https://en.wikipedia.org/wiki/Click\_consonant}{click)}}”.

\sphinxAtStartPar
Due to coarticulation that can have large impact on individual phone
segments and due to rhythmic transparency of syllables, some authors
consider syllables as more robust perceptual units of language than
individual phones (e.g., Nusbaum \& DeGroot, 1991), including also young
children (see Hallé \& Cristia, 2012, for an overview). In addition,
children and illiterate listeners have better introspective access to
syllabic structure of speech in contrast to underlying phonetic or
phonemic constituents (Liberman et al., 1974; Morais et al., 1989).
However, coarticulation has also effects across subsequent syllables,
and human speech perception also makes use of cues beyond the time\sphinxhyphen{}scale
of individual syllables. However, syllabic organization of speech is
still central to understanding how speech is organized. Studies on
conversational speech show that onset and nucleus of a syllable are much
more central to succesful comprehension of speech, and hence also
produced more accurately. In contrast, coda often undergo various types
of \sphinxstyleemphasis{syllabic reduction} where one or more phones of the coda are not
pronunced at all (e.g., Greenberg et al., 2003).

\sphinxAtStartPar
Different languages employ different syllabic structures. Syllables (of
a language) are sometimes denoted in terms of their constituent vowels
(V) and consonants (C), where CV\sphinxhyphen{}syllables (one consonant onset + a
vowel nucleus) are universally the most common. In addition, each
language has its own inventory of syllables that can be of form CVC,
CVV, CVCC, CVVCC etc..  For instance, English has on average quite long
syllables, resulting on many \sphinxstyleemphasis{monosyllabic words (bat: {[}b{]}}
{[}ae{]} {[}t{]}, \sphinxstyleemphasis{food:} {[}f{]} {[}u:{]} {[}d{]}, *laugh: *{[}l{]} {[}ae{]} {[}f{]}).
In Finnish, CV syllables are frequent and hence many frequently spoken
words emerge from combinations of several syllables (e.g., \sphinxstyleemphasis{kissa :} {[}k
i s{]} . {[}s a{]} for *cat, *or \sphinxstyleemphasis{nauraa:} {[}n a u{]}.{[}r a:{]} for \sphinxstyleemphasis{to}
\sphinxstyleemphasis{laugh}; note that . marks for syllable boundary and : for a long
vowel/consonant quantity).


\subsubsection{Words}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:words}}
\sphinxAtStartPar
Words are the minimum meaning\sphinxhyphen{}bearing units of spoken (and written)
language, i.e., a word in isolation has some meaning attached to it. In
contrast, isolated phones and syllables do not carry a meaning (unless
the word is a monosyllabic one). Every word has at least one syllable,
and, in principle, syllables do not cross word bondaries but align with
them. In several languages, spoken words align relatively well with
written words. However, words in speech are not separated by clear
articulatory or acoustic markers, such as pauses, whereas written text
transparently delimits individual words by intervening whitespaces.


\subsubsection{Utterances}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:utterances}}
\sphinxAtStartPar
Utterance is the smallest unpaused act of speech produced by one
speaker, as delineated by clear pauses (or changes of speaker). In
contrast to written language, where \sphinxstyleemphasis{a sentence} is one grammatical
expression with a communicated meaning, utterances in spoken language
can vary from individual words to much longer streams of words and
grammatical constructs. In other words, speech does not consist of
clearly delinated sentences (or clauses), but of speaking acts of
varying durations. Speakers may use
\sphinxhref{https://en.wikipedia.org/wiki/Filler\_(linguistics)}{\sphinxstyleemphasis{fillers}} (aka.
\sphinxstyleemphasis{hesitations}, \sphinxstyleemphasis{filled pauses}) such as “\sphinxstyleemphasis{uhm}” or “\sphinxstyleemphasis{ah}” or prolonged
vowels to signal that they aim to continue their utterance, but need
some to reorganize their thinking in order to mentally construct the
subsequent speech.


\subsubsection{Morphological units}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:morphological-units}}
\sphinxAtStartPar
Besides the above\sphinxhyphen{}listed units of speech, there are a number other units
and structural concepts of language that linguistic theories make use
of. They are perhaps less frequently utilized in speech processing, but
become relevant when speech is being mapped to written language or vice
versa, or when speech processing tools are used for linguistic research.
One such an area of linguistics is \sphinxstyleemphasis{morphology}, which studies word
forms, formation, and relationships between words in the same language.
In morphology, \sphinxstyleemphasis{morpheme} is the smallest grammatical unit of speech,
which may, but does not have to be, a word. Morphemes can be divided
into \sphinxstyleemphasis{free morphemes}, which can act as words in isolation (e.g.,
“cat”), and \sphinxstyleemphasis{bound morphemes}, which occur as prefixes or suffixes of
words (e.g., \sphinxhyphen{}s in “cats”). In addition, bound morphemes can be
classified into \sphinxstyleemphasis{inflectional morphemes} and \sphinxstyleemphasis{derivational morphemes.}
Inflectional morphemes change the grammatical meaning of the sentence,
but do not alter the basic meaning of the word that is being inflected.
Derivational morphemes alter the original word by creating a new word
with a separate meaning. \sphinxstyleemphasis{Allomorphs} are different pronunciation forms
of the same underlying morpheme.

\sphinxAtStartPar
Besides morphemes, morphology deals with \sphinxstyleemphasis{lexemes.} Lexeme is a unit of
meaning from which different inflections (\sphinxstyleemphasis{word\sphinxhyphen{}forms}) can be derived.
Hence, all words belong to some lexeme, but one lexeme can have many
word\sphinxhyphen{}forms (e.g., \sphinxstyleemphasis{“do”, “did”, “does”}). Set of lexemes in a language
is called \sphinxstyleemphasis{a lexicon}.


\subsection{Prosody, aka. suprasegmental properties of speech}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:prosody-aka-suprasegmental-properties-of-speech}}
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Prosody\_(linguistics)}{Prosody} aka.
suprasegmental phenomena in speech refers to those patterns in speech
that take place at time\sphinxhyphen{}scales larger than individual phones (segments).
Many of the suprasegmental phenomena, such as intonation, stress, and
rhythm, play a linguistic function, hence providing an additional means
to alter the meaning and implications of spoken message without changing
the lexical and grammatical structure of the sentence. Others are
related to other information encoded in speech, such as cues for
speaker’s emotional state, attitude, or social background. Here we only
focus on those aspects of suprasegmentals that play a linguistic role.


\subsubsection{Intonation}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:intonation}}
\sphinxAtStartPar
\sphinxstyleemphasis{Intonation} corresponds to variations in fundamental frequency (F0) of
speech as a function of time in speech, as perceived in terms of pitch.
By altering the relative pitch as a function of underlying linguistic
constituents or as a function of relative position in the utterance, the
speaker can signal information such as \sphinxstyleemphasis{emphasis}, \sphinxstyleemphasis{surprisal},
\sphinxstyleemphasis{question} (vs. statement), \sphinxstyleemphasis{or irony.} For instance, English speakers
often use falling intonation contour (across the utterance) for
statements and rising intonation pattern for questions. Marking of focus
and stress can be done with sudden increase in pitch during the marked
syllable or word. In addition, \sphinxstyleemphasis{tone languages} (e.g., Mandarin Chinese)
use pitch to phonemically to distinguish meanings of phonetically
otherwise equivalent words. In general, the presence and function of
intonational patterns depends on the language in question. Intonation
can also contain other structural cues to speech, such as \sphinxhref{https://en.wikipedia.org/wiki/Boundary\_tone}{\sphinxstyleemphasis{boundary
tones}} to signify end of
an sentence or utterance (Pierrehumbert, 1980), thereby also signifying
end of an intonational phrase.


\subsubsection{Stress}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:stress}}
\sphinxAtStartPar
\sphinxstyleemphasis{Stress} (aka. \sphinxstyleemphasis{accent}) corresponds to relative emphasis given to one
syllable or word over the others in the given phrasal context. Stress
can be realized by many means, including alternations in energy
(loudness), pitch, and segment (typically vowel) duration with respect
to the surrounding speech. Besides signifying emphasis, some languages
also employ fixed or semi\sphinxhyphen{}regular stress patterns on words.

\sphinxAtStartPar
\sphinxstyleemphasis{Word stress} refers to the emphasis of a syllable or particular set of
syllables within a word, and where multiple stressed syllables can be
divided into those with primary and secondary stress. For instance,
Finnish as nearly always primary stress on the first syllable of the
word, and secondary stress falls on the following odd\sphinxhyphen{}numbered
syllables. In English, words tend to have stress on on the initial
syllables, but there are multiple exceptions to this (e.g., word
“\sphinxstyleemphasis{guitar}”, where the stress is on “\sphinxhyphen{}tar”).  Some languages are
sometimes considered to be completely void of stress.

\sphinxAtStartPar
\sphinxstyleemphasis{Sentence stress} (aka. \sphinxstyleemphasis{prosodic stress}) refers to stress on certain
words or parts of words within an utterance, either signifying emphasis
or contrast (e.g., “No, \sphinxstylestrong{I} went home” vs. “No, I went \sphinxstylestrong{home}”;
stressed word highlighted).


\subsubsection{Rhythm}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:rhythm}}
\sphinxAtStartPar
Speech \sphinxstyleemphasis{rhythm} is a result of complex interplay of several factors in
speech production, where the flow of chosen words is also affected by
stress patterns, segmental and pause durations, and general syllable
structure of the given language. At a general level, rhythm refers to
some kind of sense of recurrence in the speech, such as alternation
between stressed and unstressed syllables.

\sphinxAtStartPar
A more narrow definition of rhythm relates to the idea of \sphinxstyleemphasis{isochrony},
according to which languages can be categorized into three rhythmic
categories in terms of what is the determining recurrent structure in
the signal (Pike, 1945; see also, e.g., Nespor et al., 2011). The first
category consists of syllable\sphinxhyphen{}timed languages, where duration of each
syllable is equal. The second corresponds to mora\sphinxhyphen{}timed languages, where
duration of each mora is equal (see
\sphinxhref{https://en.wikipedia.org/wiki/Mora\_(linguistics)}{mora} on Wikipedia).
The last category consists of stress\sphinxhyphen{}timed languages, where the interval
between stressed syllablees is equal. In practice, evidence for such a
precise rhythmic regularities in actual speech data is limited, although
many would likely agree that some languages tend to share some rhythmic
characteristics that make them distinct from the rhythm of others.


\subsection{Phonetic transcription and speech annotation}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:phonetic-transcription-and-speech-annotation}}
\sphinxAtStartPar
\sphinxstyleemphasis{Phonetic transcription} is the process of marking down the phonetic
structure of speech, typically aligned with the timeline of the speech
waveform. Transcription is usually conducted according to IPA standards.
The process can be carried out using either \sphinxstyleemphasis{narrow transcription} or
\sphinxstyleemphasis{broad transcription}. In broad transcription, the marking typically
consists of the most distinct phonetic elements in the speech data, such
as individual phones and their basic allophonic variations. In \sphinxstyleemphasis{phonemic
transcription}, which is the broadest level possible, only the phonemes
corresponding to the speech are transcribed. In contrast, narrow
transcription contains more phonetic detail regarding realization of the
speech sounds. These details can include information on stress and
intonation, and the transcription may also consists of diacritics, which
provide more details on the articulatory/acoustic realization of the
phones compared to their standard definitions in the IPA system (see the
full \sphinxhref{https://upload.wikimedia.org/wikipedia/commons/8/8f/IPA\_chart\_2020.svg}{IPA
chart}).

\sphinxAtStartPar
\sphinxstyleemphasis{Annotation of speech} is a more general term than phonetic
transcription. Annotations may consist of different layers of
information in addition to (or instead of) the phonetic information. For
instance, speech annotations may contain syllable and word boundaries
and identities of the corresponding units. These identities can be
marked down phonetically or ortographically, and are recorded either as
actually pronunced or as \sphinxstyleemphasis{canonical} (i.e., as they would be listed in a
dictionary). When contents of speech are marked as regular text, it is
called \sphinxstyleemphasis{orthographic transcription.} Other commonly utilized annotation
layers include utterance boundaries, speaker identities/turns,
grammatical information such as parts of speech, morphological
information, or potential presence of special non\sphinxhyphen{}speech events such as
different categories of background noise. Since production of
high\sphinxhyphen{}quality annotations usually requires manual human work and is very
slow, different speech datasets tend to contain annotations that were of
primary research interest to the person or team collecting and preparing
the dataset.

\sphinxAtStartPar
Automatic speech processing tools can be of use in speeding up the
annotation by, e.g., providing an initial version of annotations for
humans to verify and correct. A particularly useful instance of
automatic tools is the so\sphinxhyphen{}called \sphinxstyleemphasis{forced\sphinxhyphen{}alignment} with an automatic
speech recognizer, which is suitable for cases where the acoustic speech
signal and the corresponding spoken text are known. Assuming that the
text is faithful to the underlying speech contents, a good recognizer
can produce much more accurate phonemic transcription and/or timestamps
for word boundaries than what would be achievable by regular automatic
speech recognition without the reference text.


\subsection{References}
\label{\detokenize{Introduction/Linguistic_structure_of_speech:references}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Greenberg, S., Carvey, H., Hitchcock, L., and Chang, S. (2003). Temporal properties of spontaneous speech – a syllable centric perspective. \sphinxstyleemphasis{Speech Communication}, \sphinxstyleemphasis{31}, 465–485,
\sphinxurl{https://doi.org/10.1016/j.wocn.2003.09.005}

\item {} 
\sphinxAtStartPar
Hallé, P., \& Christia, A. (2012). Global and detailed speech representations in early language acquisition. In S. Fuchs, M. Weirich, D. Pape, \& P. Perrier (Eds.). \sphinxstyleemphasis{Speech planning and dynamics}. Frankfurt am Main: Peter Lang.

\item {} 
\sphinxAtStartPar
Liberman, I. Y., Shankweiler, D., Fischer, W. F., \& Carter, B. (1974). Explicit syllable and phoneme segmentation in the young child. Journal of Experimental Child Psychology, 18, 201–212.

\item {} 
\sphinxAtStartPar
Morais, J., Content, A., Cary, L., Mehler, J., \& Segui, J. (1989). Syllabic segmentation and literacy. Language and Cognitive Processes, 4(1), 56–67.

\item {} 
\sphinxAtStartPar
Nespor, M., Shukla, M., \& Mehler, J. (2011). Stress‐timed vs. syllable‐timed languages. In van Oostendorp et al. (Eds.), The Blackwell Companion to Phonology (pp. 1147\sphinxhyphen{}1159). Malden, MA: Blackwell.

\item {} 
\sphinxAtStartPar
Nusbaum, H. C., \& DeGroot, J. (1991). The role of syllables in speech perception. In M. S. Ziolkowski, M. Noske, \& K. Deaton (Eds.). \sphinxstyleemphasis{Papers from the parasession on the syllable in phonetics and phonology}. Chicago: Chicago Linguistic Society.

\item {} 
\sphinxAtStartPar
Pierrehumbert, Janet B. (1980) “The Phonology and Phonetics of English Intonation” Ph.D. Thesis, Massachusetts Institute of Technology.

\item {} 
\sphinxAtStartPar
Pike, K. (1945). The intonation of American English, vol 1. Ann Arbor: University of Michigan Press. pp. 34–35.

\item {} 
\sphinxAtStartPar
Werker, J. F., \& Tees, R. C. (1984). Cross\sphinxhyphen{}language speech perception: Evidence for perceptual reorganization during the first year of life. Infant Behavior \& Development, 7(1), 49–63. \sphinxurl{https://doi.org/10.1016/S0163-6383(84)80022-3}

\end{itemize}

\sphinxAtStartPar
Also, substantial reuse of materials from related
\sphinxhref{https://www.wikipedia.org/}{Wikipedia} articles.

\sphinxstepscope


\section{Applications and systems structures}
\label{\detokenize{Introduction/Applications_and_systems_structures:applications-and-systems-structures}}\label{\detokenize{Introduction/Applications_and_systems_structures::doc}}

\subsection{Applications}
\label{\detokenize{Introduction/Applications_and_systems_structures:applications}}
\sphinxAtStartPar
Speech processing is used in, for example;
\begin{itemize}
\item {} 
\sphinxAtStartPar
Telecommunication
\begin{itemize}
\item {} 
\sphinxAtStartPar
Phones and mobile phones

\item {} 
\sphinxAtStartPar
Teleconferencing systems

\item {} 
\sphinxAtStartPar
Voice\sphinxhyphen{}over\sphinxhyphen{}IP, like
\sphinxhref{https://en.wikipedia.org/wiki/Skype}{Skype}, \sphinxhref{https://en.wikipedia.org/wiki/Google\_Hangouts}{Google
Hangouts},
\sphinxhref{https://en.wikipedia.org/wiki/FaceTime}{Facetime},
\sphinxhref{https://en.wikipedia.org/wiki/Zoom\_Video\_Communications}{Zoom}

\item {} 
\sphinxAtStartPar
Podcasts, digital radio and TV

\item {} 
\sphinxAtStartPar
Virtual reality and gaming applications

\end{itemize}

\item {} 
\sphinxAtStartPar
Speech operated virtual assistants, like
\sphinxhref{https://en.wikipedia.org/wiki/Siri}{Siri},
\sphinxhref{https://en.wikipedia.org/wiki/Amazon\_Alexa}{Alexa}, \sphinxhref{https://en.wikipedia.org/wiki/Google\_Assistant}{Google
Assistant},
\sphinxhref{https://en.wikipedia.org/wiki/Mycroft\_(software)}{Mycroft},
\sphinxhref{https://en.wikipedia.org/wiki/Cortana\_\%28software\%29}{Cortana} etc.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Also speech interfaces of robots

\end{itemize}

\item {} 
\sphinxAtStartPar
Navigators with speech feedback, like
\sphinxhref{https://en.wikipedia.org/wiki/TomTom}{TomTom} and
\sphinxhref{https://en.wikipedia.org/wiki/Garmin}{Garmin}

\item {} 
\sphinxAtStartPar
Automated telephone services (like the helpdesk of an airline)

\item {} 
\sphinxAtStartPar
Automated transcription
\begin{itemize}
\item {} 
\sphinxAtStartPar
Youtube subtitles

\item {} 
\sphinxAtStartPar
Recorded notes from doctors

\end{itemize}

\item {} 
\sphinxAtStartPar
Microphones and microphone systems
\begin{itemize}
\item {} 
\sphinxAtStartPar
Headsets, with or without noise attenuation, with or without
active noise cancelling

\item {} 
\sphinxAtStartPar
Stage microphones (related to audio processing)

\end{itemize}

\item {} 
\sphinxAtStartPar
Studio recording systems (related to audio processing)
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Auto-Tune}{Autotune}

\item {} 
\sphinxAtStartPar
Noise attenuation/reduction

\end{itemize}

\item {} 
\sphinxAtStartPar
Dubbing movies e.g. for translation of audio
\begin{itemize}
\item {} 
\sphinxAtStartPar
Also on\sphinxhyphen{}line translation

\end{itemize}

\item {} 
\sphinxAtStartPar
Speech synthesis services
\begin{itemize}
\item {} 
\sphinxAtStartPar
Automated e\sphinxhyphen{}book readers

\item {} 
\sphinxAtStartPar
User\sphinxhyphen{}interfaces for the blind and the handicapped

\end{itemize}

\item {} 
\sphinxAtStartPar
Less common applications
\begin{itemize}
\item {} 
\sphinxAtStartPar
Access control and fraud detection with speaker identification
and verification

\item {} 
\sphinxAtStartPar
Anonymization and obfuscation (e.g. witness protection) of
speech signals

\item {} 
\sphinxAtStartPar
Speech synthesizers for disabled people (e.g. Stephen Hawking)

\item {} 
\sphinxAtStartPar
Medical analysis of speech signals (e.g.
\sphinxhref{https://en.wikipedia.org/wiki/Alzheimer\%27s\_disease}{Alzheimer}
detection)

\end{itemize}

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{165138615}.jpeg}
\sphinxincludegraphics{{165138616}.png}
\sphinxincludegraphics{{165138617}.png}
\sphinxincludegraphics{{165138618}.png}
\sphinxincludegraphics{{165138684}.png}

\sphinxAtStartPar
Such applications can be categorized according to functionality, roughly
as:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Transmission and storage of speech
\begin{itemize}
\item {} 
\sphinxAtStartPar
Enable communication with a far\sphinxhyphen{}away person

\end{itemize}

\item {} 
\sphinxAtStartPar
Speech operated user\sphinxhyphen{}interfaces
\begin{itemize}
\item {} 
\sphinxAtStartPar
Enable spoken interaction with a machine

\end{itemize}

\item {} 
\sphinxAtStartPar
Information extraction from a speech signal
\begin{itemize}
\item {} 
\sphinxAtStartPar
Like automated transcription to generate subtitles for movies

\end{itemize}

\item {} 
\sphinxAtStartPar
Speech synthesis, i.e. automated generation of speech

\item {} 
\sphinxAtStartPar
“Improvement” of a speech signal, such as
\begin{itemize}
\item {} 
\sphinxAtStartPar
Such as noise reduction, translation

\end{itemize}

\end{itemize}

\sphinxAtStartPar
Note that these categories are in many senses overlapping; for example,
noise reduction can be a part of any speech processing system and
information extraction is practically a mandatory part of speech
operated user\sphinxhyphen{}interfaces.


\subsection{Systems structures}
\label{\detokenize{Introduction/Applications_and_systems_structures:systems-structures}}

\subsubsection{Transmission and storage}
\label{\detokenize{Introduction/Applications_and_systems_structures:transmission-and-storage}}
\sphinxAtStartPar
The objective of speech transmission systems is to compress the signal
to as few bits as possible, while keeping the sound quality at the
output as good as possible. This requires that the degradations that we
introduce are chosen such that their perceptual influence is as small as
possible. In other words, we would not like the listener to notice (or
to notice as little as possible) that the signal has been degraded. In
the illustration on the right, at the encoder on the sender side, we
therefore have a model of perceptual importance, which determines how
the signal is quantized. The quantized signal is then compressed to as
few bits as possible. For such compression, we use statistical
information about speech signals.

\sphinxAtStartPar
The decoder at the receiving side, reverses the steps by decompression
and dequantization.

\sphinxAtStartPar
Pre\sphinxhyphen{}processing operations would typically include noise attenuation and
voice activity detection (see below).

\sphinxAtStartPar
\sphinxincludegraphics{{165138696}.png}


\subsubsection{Information extraction}
\label{\detokenize{Introduction/Applications_and_systems_structures:information-extraction}}
\sphinxAtStartPar
We can extract many types of information from a speech signal, like
\DUrole{xref,myst}{text content} and \DUrole{xref,myst}{speaker
identity}. Many such forms of
information can be categorized by \sphinxstyleemphasis{labels}, that is, we give a label to
a particular speech signal. That label can be for example to word which
was pronounced or the speaker identity. Alternatively, such extracted
information can be continuous\sphinxhyphen{}valued, such as the age of the speaker or
mood (how glad/angry are you?), but we can treat both types of
information as labels.

\sphinxAtStartPar
Such information extraction methods are today predominantly machine
learning methods. A typical configuration is illustrated on the right,
where the systems is trained off\sphinxhyphen{}line with a database of speech and
corresponding labels. Once the system has been trained, it “knows” how
to derive labels from speech input, such that in the actual use
(application) of the model, it can classify input speech to give an
estimate of the label.

\sphinxAtStartPar
In many cases, information extraction can also be implemented as a
signal processing task, where we use prior knowledge of the signal to
device our algorithm. For example, for estimating the \DUrole{xref,myst}{fundamental
frequency} (pitch) of a speech signal, we can
readily use our knowledge to device efficient algorithms. Such
algorithms are usually an order of magnitude simpler than machine
learning methods, but if the task is complicated, then the accuracy the
output is reduced correspondingly.

\sphinxAtStartPar
\sphinxincludegraphics{{165138741}.png}


\subsection{Speech synthesis}
\label{\detokenize{Introduction/Applications_and_systems_structures:speech-synthesis}}
\sphinxAtStartPar
When we want to make a computer speak, we need a speech synthesiser,
which takes text as input and outputs speech. It is thus the reverse of
the information extraction \sphinxhyphen{}task, in that the roles of speech and labels
(text) have been switched (see figure on the right). As in information
extraction, also here we can also use simpler methods when applicable.
The classical method is concatenative synthesis, where segments of
speech, from a database, are fused together to form continuous
sentences. Such methods are common for example in public announcement
systems (e.g. train stations), where the range of possible announcements
is known in advance.

\sphinxAtStartPar
\sphinxincludegraphics{{165139247}.png}


\subsection{User\sphinxhyphen{}interfaces}
\label{\detokenize{Introduction/Applications_and_systems_structures:user-interfaces}}
\sphinxAtStartPar
User interfaces can employ speech to accept speech commands and/or
respond with speech. For example,
\begin{itemize}
\item {} 
\sphinxAtStartPar
A car navigator can give instructions with speech, while accepting
commands only through the touch (tactile) interface.

\item {} 
\sphinxAtStartPar
An automatic door can accept speech commands (“Door, open”) but give
no audible feedback.

\item {} 
\sphinxAtStartPar
Fully speech operated interfaces, like smart speakers, both accept
speech commands and give spoken feedback.

\end{itemize}

\sphinxAtStartPar
The unidirectional systems with only speech recognition or only speech
generation are thus subsets of “full” speech operated systems. The stack
of modules of such a complete speech operated system is illustrated on
the right. Here the acoustic front\sphinxhyphen{}end (can) contain such pre\sphinxhyphen{}processing
methods described in the following section. Natural language
understanding assigns meaning to a sequence of words, dialogue
management maps that to a specific action, implemented by the
actuator(s), and natural language generation refers to the generation of
an answer, in text from.

\sphinxAtStartPar
\sphinxincludegraphics{{165139542}.png}


\subsection{Processing and preprocessing}
\label{\detokenize{Introduction/Applications_and_systems_structures:processing-and-preprocessing}}
\sphinxAtStartPar
Irrespective of application, most systems which operate with speech
signals suffer from similar type so of problems;
\begin{itemize}
\item {} 
\sphinxAtStartPar
The main application is expensive to run, so it would be useful to
have a pre\sphinxhyphen{}processing unit which detects when it makes sense to
trigger the main application. For example, when nobody is speaking,
it does not make sense to run a speech recognizer.

\item {} 
\sphinxAtStartPar
Devices are used in real\sphinxhyphen{}world environments, where background noises
and room echo are common. It would therefore be useful to clean up
the signal prior to feeding to the main application.

\end{itemize}

\sphinxAtStartPar
Such functions typically constitute the acoustic front\sphinxhyphen{}end.


\subsubsection{Voice activity detection}
\label{\detokenize{Introduction/Applications_and_systems_structures:voice-activity-detection}}
\sphinxAtStartPar
When there is no speech, most speech processing operations are
meaningless. \DUrole{xref,myst}{Voice activity detection}
refers to the classification of signal segments according to whether
they contain speech or not.


\subsubsection{Keyword spotting or Wake\sphinxhyphen{}word detection}
\label{\detokenize{Introduction/Applications_and_systems_structures:keyword-spotting-or-wake-word-detection}}
\sphinxAtStartPar
In most practical situations where voice operated devices are present,
we want to be able to talk with people and not only the device. In other
words, we need to know when the user is speaking to the device and when
not. The device then doesn’t have to try make sense of speech which is
directed to someone else. Wake\sphinxhyphen{}word detection (or spotting) refers to a
process which is just waiting for a specific word, which triggers the
main application. For example, smart speakers of the Amazon Alexa brand
wait for the user to say the wake\sphinxhyphen{}word “Alexa”, and only after
identifying that word, it starts the main process.


\subsubsection{Speech enhancement}
\label{\detokenize{Introduction/Applications_and_systems_structures:speech-enhancement}}
\sphinxAtStartPar
We can try to remove the detrimental effect of background noises and
room echoes with speech enhancement methods. Imagine for example how
difficult it is to understand someone on the phone, when the remote
speaker is standing on a busy street loud cars driving by. With noise
attenuation we can reduce such noises to make speech more pleasant to
listen at the receiving end. Also any other processing becomes simpler
if the speech signal is more clean. Thus a voice user\sphinxhyphen{}interface can
feature noise attenuation as pre\sphinxhyphen{}processing. Note however that also
other processes such as voice activity detection and wake\sphinxhyphen{}word detection
become easier on the clean signal (see alternative 1 in the figure on
the right). However, since noise attenuation can be a computationally
expensive process, it might be better to apply when we already know that
the signal is a speech command (see alternative 2 in the figure on the
right), though this will reduce the accuracy of voice activity and
wake\sphinxhyphen{}word detection.


\subsubsection{Other signal processing}
\label{\detokenize{Introduction/Applications_and_systems_structures:other-signal-processing}}
\sphinxAtStartPar
Speech signals can be processed further by an array of different
algorithms as desired. For example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Signals can be modified for artistic purposes with tools such as
auto\sphinxhyphen{}tune, where the pitch of a singing voice is modified to match a
desired pitch.

\item {} 
\sphinxAtStartPar
Speech signals can be translated to some other language.

\item {} 
\sphinxAtStartPar
The identity of a speaker can be hidden (or spoofed) for security
purposes like witness\sphinxhyphen{}protection, or for illegal activities like
fraud.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{165139583}.png}

\sphinxstepscope


\chapter{Basic Representations}
\label{\detokenize{Representations/Representations:basic-representations}}\label{\detokenize{Representations/Representations::doc}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Waveform::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Waveform}}}} 

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Short-time_analysis::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Short\sphinxhyphen{}time analysis}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Short-time_processing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Short\sphinxhyphen{}time processing}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Windowing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Windowing}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Signal_energy_loudness_and_decibel::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Signal energy, loudness and decibel}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Spectrogram_and_the_STFT::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Spectrogram and the STFT}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Autocorrelation_and_autocovariance::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Autocorrelation and autocovariance}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Melcepstrum::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Cepstrum and MFCC}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Linear_prediction::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Linear prediction}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Fundamental_frequency_F0::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Fundamental frequency (F0)}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Zero-crossing_rate::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Zero\sphinxhyphen{}crossing rate}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Deltas_and_Delta-deltas::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Deltas and Delta\sphinxhyphen{}deltas}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Pitch-Synchoronous_Overlap-Add_PSOLA::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{PSOLA}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Jitter_and_shimmer::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Jitter and shimmer}}}}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Crest\_factor}{Crest factor} (Wikipedia)

\end{enumerate}

\sphinxstepscope


\section{Short\sphinxhyphen{}time analysis of speech and audio signals}
\label{\detokenize{Representations/Short-time_analysis:short-time-analysis-of-speech-and-audio-signals}}\label{\detokenize{Representations/Short-time_analysis::doc}}

\subsection{The speech signal}
\label{\detokenize{Representations/Short-time_analysis:the-speech-signal}}
\sphinxAtStartPar
\sphinxincludegraphics{{1482949661}.png}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Speech sounds are acoustic signals
\begin{itemize}
\item {} 
\sphinxAtStartPar
Emitted from the mouth

\item {} 
\sphinxAtStartPar
Pressure variations travelling through the air

\item {} 
\sphinxAtStartPar
Received by the ears

\end{itemize}

\item {} 
\sphinxAtStartPar
The figure on the right has an example of a speech sample, together with a time\sphinxhyphen{}aligned phonetic and text transcription.

\item {} 
\sphinxAtStartPar
The acoustic perssure waveform is captured by a microphone
\begin{itemize}
\item {} 
\sphinxAtStartPar
The microphone converts it to an analog electric signal

\item {} 
\sphinxAtStartPar
Further converted to a digital signal with an AD converter

\item {} 
\sphinxAtStartPar
Usually a zero\sphinxhyphen{}mean signal (zero is average/ambient pressure level)

\end{itemize}

\item {} 
\sphinxAtStartPar
We will here always assume that we have access to a digital representation of the speech signal

\item {} 
\sphinxAtStartPar
It is a highly time\sphinxhyphen{}variant signal – it is more often changing then stationary

\item {} 
\sphinxAtStartPar
Analysis tools assume that the signal is stationary
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhyphen{}> Fundamental problem!

\item {} 
\sphinxAtStartPar
For example, if we analyze the (Fourier) spectrum of a speech signal, we would see the spectrum of a mixture of all the phonemes.

\item {} 
\sphinxAtStartPar
\sphinxhyphen{}> Not useful!

\end{itemize}

\item {} 
\sphinxAtStartPar
In \sphinxstyleemphasis{short\sphinxhyphen{}time analysis} of speech, the signal is split into small segments (windows).
\begin{itemize}
\item {} 
\sphinxAtStartPar
If windows are short enough, signal is stationary

\item {} 
\sphinxAtStartPar
Can use standard DSP tools within windows

\end{itemize}

\end{itemize}


\subsection{Sound example}
\label{\detokenize{Representations/Short-time_analysis:sound-example}}
\sphinxAtStartPar
Let’s take a demonstration sound.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}
\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}
\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Short-time_analysis_4_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{What would be a suitable window size?}
\label{\detokenize{Representations/Short-time_analysis:what-would-be-a-suitable-window-size}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Choose a window length in milliseconds.

\item {} 
\sphinxAtStartPar
Try to find the longest window where segments still look stationary (waveform character does not change much during segment).

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{window\PYGZus{}length\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{30}

\PYG{n}{window\PYGZus{}length} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{window\PYGZus{}length\PYGZus{}ms}\PYG{o}{*}\PYG{n}{fs}\PYG{o}{/}\PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Window length in samples }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{data\PYGZus{}length} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} choose segment from random position in sample}
\PYG{n}{starting\PYGZus{}position} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{n}{data\PYGZus{}length} \PYG{o}{\PYGZhy{}} \PYG{n}{window\PYGZus{}length}\PYG{p}{)}

\PYG{n}{time\PYGZus{}vector} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{window\PYGZus{}length\PYGZus{}ms}\PYG{p}{,}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{time\PYGZus{}vector}\PYG{p}{,}\PYG{n}{data}\PYG{p}{[}\PYG{n}{starting\PYGZus{}position}\PYG{p}{:}\PYG{p}{(}\PYG{n}{starting\PYGZus{}position}\PYG{o}{+}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (ms)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Amplitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Window length in samples 1440
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Short-time_analysis_6_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}\begin{itemize}
\item {} 
\sphinxAtStartPar
A window length in the range 20 to 30 ms typically gives fairly stationary windows.
\begin{itemize}
\item {} 
\sphinxAtStartPar
An exception is plosives, like /t/ or /p/, where speech suddenly starts (i.e. onset). There we will never get a stationary waveform, but then the single event characterizes the window, so it is then a single\sphinxhyphen{}event window and thus “stationary” in a wide sense.

\end{itemize}

\item {} 
\sphinxAtStartPar
Longer windows would give more data to analyze = more efficient/accurate

\item {} 
\sphinxAtStartPar
However, analysis methods such as Fourier analysis, typically assume that a window is stationary.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Breaking the assumptions reduce efficiency/accuracy

\end{itemize}

\end{itemize}


\subsection{Windowing functions}
\label{\detokenize{Representations/Short-time_analysis:windowing-functions}}

\subsubsection{Problem}
\label{\detokenize{Representations/Short-time_analysis:problem}}\begin{itemize}
\item {} 
\sphinxAtStartPar
A signal \(\{x_1,\dots x_n\}\) of a signal is equivalent with a zero\sphinxhyphen{}extended signal \(\{0,\dots 0,\,x_1\dots x_n,\,0\dots0\}\)

\item {} 
\sphinxAtStartPar
Heuristic interpretation: The window looks identical to one which has a discontinuity at the border. It does not look stationary = Inaccurate and inefficient.

\end{itemize}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_analysis_9_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsubsection{Solution}
\label{\detokenize{Representations/Short-time_analysis:solution}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Multiply window with smooth function, the windowing function, which goes to zero at the borders.

\item {} 
\sphinxAtStartPar
Local structure of the signal is not changed much = similar to original signal.

\item {} 
\sphinxAtStartPar
Goes smoothly to zero at borders = no discontinuity = no problem.

\item {} 
\sphinxAtStartPar
Typically, we multiply the signal \(x_n\) with a windowing function \(w_n\)
\$\(
     \tilde x_n := w_n x_n
  \)\(
  where 
  \)\(
     \begin{cases}
       w_n > 0, & N\in[0,N-1], \\
       w_n = 0, & \text{otherwise}.
     \end{cases}
  \)\(
  and \)w\_n\( goes smoothly to zero at the segment borders
  \)\(
  \begin{cases}
    \lim_{n\rightarrow 0}w_n &= 0, \\
    \lim_{n\rightarrow N-1}w_n &= 0      .
  \end{cases}
  \)\$

\item {} 
\sphinxAtStartPar
Typical windowing functions include the Hann
\$\( w_n = \frac12\left[1 - \cos\left(\frac{\pi \left(n+\frac12\right)}{N}\right)\right] \)\(
and the Hamming windows
\)\( w_n = 0.54 - 0.46\cos\left(\frac{\pi \left(n+\frac12\right)}{N}\right). \)\$

\item {} 
\sphinxAtStartPar
The choice of windowing function is usually based on preference with respect to resolution in the spectrum.

\item {} 
\sphinxAtStartPar
More details at \sphinxurl{https://en.wikipedia.org/wiki/Window\_function}

\end{itemize}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_analysis_11_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Spectral analysis}
\label{\detokenize{Representations/Short-time_analysis:spectral-analysis}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The discrete Fourier transform (DFT) can be used to compute the frequency spectrum
\$\(
      \tilde X_k = \sum_{n=0}^{N-1} \tilde x_n e^{-i2\pi kn/N},
\)\(
where the windowed signal is \)\textbackslash{}tilde x\_n = w\_n x\_n\$.

\item {} 
\sphinxAtStartPar
Consequences
\begin{itemize}
\item {} 
\sphinxAtStartPar
The Fourier transform is \(\tilde X_k = W_k*X_k\) where ‘\(*\)’ is the convolution.

\item {} 
\sphinxAtStartPar
The true spectrum \(X_k\) is smoothed with windowing function

\item {} 
\sphinxAtStartPar
Observation is biased (but best we can do for a non\sphinxhyphen{}stationary signals)

\end{itemize}

\end{itemize}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_analysis_13_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}\begin{itemize}
\item {} 
\sphinxAtStartPar
Observations
\begin{itemize}
\item {} 
\sphinxAtStartPar
The DFT is complex\sphinxhyphen{}valued and cannot be easily visualized as such.

\item {} 
\sphinxAtStartPar
The absolute \(|X_k|\) or absolute squared \(|X_k|^2\)  value of the spectrum can be plotted, but different frequencies have very different ranges so it is hard to see anything useful.

\item {} 
\sphinxAtStartPar
In the log\sphinxhyphen{}magnitude spectrum \(\log|X_k|\)  spectral features are much better visualized.

\item {} 
\sphinxAtStartPar
The usual unit is decibel, \(20\log_{10}|X_k|\) (dB).

\end{itemize}

\item {} 
\sphinxAtStartPar
In fact, the log\sphinxhyphen{}intensity scale corresponds roughly to a perceptual scale, that is, it corresponds more or less to how humans perceive loudness for individual frequencies.

\end{itemize}


\subsection{Speech features in the spectrum}
\label{\detokenize{Representations/Short-time_analysis:speech-features-in-the-spectrum}}

\subsubsection{Envelope}
\label{\detokenize{Representations/Short-time_analysis:envelope}}
\sphinxAtStartPar
The macro\sphinxhyphen{}shape of the spectrum is known as the envelope. It can be also be defined as the smooth shape which connects spectral peaks.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_analysis_16_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsubsection{Formants}
\label{\detokenize{Representations/Short-time_analysis:formants}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Peaks of the envelope are known as \sphinxstyleemphasis{formants}. They are thus high\sphinxhyphen{}energy areas of the spectrum. The formants are numbered from left to right, F1, F2, F3 etc.

\item {} 
\sphinxAtStartPar
The location of formants uniquely identify vowels. In other words, each vowel is defined by a unique combination of the two first formants.

\end{itemize}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_analysis_18_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsubsection{Fundamental frequency}
\label{\detokenize{Representations/Short-time_analysis:fundamental-frequency}}
\sphinxAtStartPar
Note: A more thorough description of the fundamental frequency is found at \DUrole{xref,myst}{Fundamental frequency}.

\sphinxAtStartPar
Voiced phonations are “more or less” periodic or equivalently, we call them quasi\sphinxhyphen{}periodic. The periodicity is generated by the vocal folds which oscillate in the airflow, periodically closing the passing airflow to open again, with a frequency approximately in the range 80 to 400 Hz. It is visible in the waveform as a repeated waveform, in the spectrum as a comb\sphinxhyphen{}structure, that is, relatively densly spaced peaks in the spectrum.

\sphinxAtStartPar
The fundamental frequency is often called the F0. It is a bit of an unfortunate choice, because it can be confused to the formants, which are F1, F2, F3, although there is no connection between formants and the fundamental frequency.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_analysis_20_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\paragraph{Harmonics of the fundamental}
\label{\detokenize{Representations/Short-time_analysis:harmonics-of-the-fundamental}}
\sphinxAtStartPar
Observe that voiced signals are \sphinxhref{https://en.wikipedia.org/wiki/Harmonic}{harmonic}. That is, when the glottal folds oscillate, the main event in each oscillation is when the glottal folds smash into each other. This event forms a node in the waveform, which means that all sub\sphinxhyphen{}components of the waveform must align at this point. If the length of the glottal period is \(T\), then any waveform which has the length \(T/k\), where \(k\) is a positive integer would then be periodic with length \(T\). The frequencies of the harmonics are correspondingly \(kF_0\), for a fundamental \(F_0\).

\sphinxAtStartPar
In other words, in the spectrum, we will see a sequence of peaks at frequencies \(kF_0\). Such a structure is known as a comb\sphinxhyphen{}structure.

\sphinxAtStartPar
Observe that the demonstration code will always return \sphinxstyleemphasis{some} fundamental frequency, even if the signal does not really have any periodicity. Similarly, it will always show a harmonic structure even if there is none. This is though a typical feature of speech analysis; measurements are noisy at best, and sometimes the desired effect is not even present at all. The software however must be somehow able to cope with that.

\sphinxAtStartPar
Typical errors in the observation are
\begin{itemize}
\item {} 
\sphinxAtStartPar
Sometimes there is no fundamental frequency, but a harmonic model will still be plotted

\item {} 
\sphinxAtStartPar
Sometimes the fundamental frequency estimate is inaccurate, such that the higher harmonics are very much inaccurate (why?)

\item {} 
\sphinxAtStartPar
Sometimes a multiple of the fundamental is incorrectly identified as the fundamental. Then the identified harmonics only cover some of the true harmonics.

\end{itemize}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_analysis_22_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Spectrogram}
\label{\detokenize{Representations/Short-time_analysis:spectrogram}}
\sphinxAtStartPar
One window gives a “snapshot” image of the signal.
Analysis of multiple, consecutive windows give a “movie”. We can therefore analyze how the signal is changing over time.

\sphinxAtStartPar
We use a sliding window
\$\(
\tilde x_{n,k} = w_{n-k} x_n
\)\(
where each value of \)k\$ gives a different snapshot of the signal.

\sphinxAtStartPar
By taking the DFT of each window, we obtain the \sphinxstyleemphasis{short\sphinxhyphen{}time Fourier transform (STFT)}, which can be plotted as a \sphinxstyleemphasis{spectrogram}\sphinxhyphen{}representation of the signal. Since many of the fundamental features of speech signals are visible in the spectrum, we can then expect to see the same features also in the spectrogram. Moreover, we will be able to see how these features change over time.

\sphinxAtStartPar
Algorithm:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
At position \(k\), apply windowing (typically, Hamming windowing)
to obtain segment of the signal of length \(N\).

\item {} 
\sphinxAtStartPar
Apply the fast Fourier transform to obtain the spectrum \(X_k(\omega)\).

\item {} 
\sphinxAtStartPar
Take the logarithm of the absolute value \(20\log_{10}|X_k(\omega)|\) to obtain
the logarithmic spectrum.

\item {} 
\sphinxAtStartPar
Advance position by \(K\), that is, \(k:=k+K\) and return to 1.

\end{enumerate}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_analysis_25_0}.png}

\noindent\sphinxincludegraphics{{Short-time_analysis_25_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsubsection{Exercise}
\label{\detokenize{Representations/Short-time_analysis:exercise}}\begin{itemize}
\item {} 
\sphinxAtStartPar
What is the fundamental, its harmonics and where can you see them change?

\item {} 
\sphinxAtStartPar
Where are the formants?

\item {} 
\sphinxAtStartPar
Can you find vertical lines in the plot? Listen to the corresponding audio; can you find out what the vertical lines correspond to?

\end{itemize}


\subsubsection{Accuracy}
\label{\detokenize{Representations/Short-time_analysis:accuracy}}
\sphinxAtStartPar
The resolution or accuracy of the spectrogram depends on
\begin{itemize}
\item {} 
\sphinxAtStartPar
Step between windows

\item {} 
\sphinxAtStartPar
Length of windows

\item {} 
\sphinxAtStartPar
Amount of zero\sphinxhyphen{}extension of signal

\item {} 
\sphinxAtStartPar
Shape of windowing function

\end{itemize}

\sphinxAtStartPar
We will discuss each (except the shape of the windowing function) below.

\sphinxAtStartPar
Analyzing windows is similar to sampling a signal. The more often you take windows (or samples), the higher the resolution over time will be. The price is a larger amount of data. Moreover, at some point, there really is not that much to be gained by a denser sampling. Similarly, we can reduce the density to lower computationaly cost, but at some point you start loosing essential information.

\sphinxAtStartPar
The number of points in the spectrum corresponds to the length of the vector given to the DFT. That is, a longer window will give more spectral components and thus a higher spectral resolution. More accurate is good in the sense that, for example, from a accurate spectrum we can estimate the fundamental frequency more accurately. However, the compromise is due to the inherent changes in speech signals. Because speech signals are continuosly changing, when we take longer windows, we will more frequently have instances where a single window contains two or more distinct sounds, such that the spectrum is a combination of these sounds. The spectrum is then smeared and less accurate, even if we wanted to get a more accurate spectrum. This is a compromise where we have to always seek a balance depending on the objectives in each particular application.

\sphinxAtStartPar
A second method for increasing the accuracy of the spectrum is known as zero\sphinxhyphen{}extension. It is somewhat artificial in the sense that it does not actually bring any new information to the analysis. It is achieved by simply extending the windowed signal by zeros. Since it thus has more samples, the spectrum will have more resolution. We can then, for example, find a more accurate estimate of the fundamental frequency. However, since zero\sphinxhyphen{}extension does not introduce any new information into the analysis, it should be treated as a interpolation method and it thus gives an interpolated spectrum. In analysis applications that is often a desirable feature.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} try different length, step and zero\PYGZhy{}extension factors}
\PYG{n}{window\PYGZus{}length\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{30}
\PYG{n}{window\PYGZus{}step\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{zero\PYGZus{}extension\PYGZus{}factor} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} multiplicator by which we extend the length of the spectrum}

\PYG{c+c1}{\PYGZsh{} derived parameters}
\PYG{n}{window\PYGZus{}length} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{window\PYGZus{}length\PYGZus{}ms}\PYG{o}{*}\PYG{n}{fs}\PYG{o}{/}\PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{window\PYGZus{}step} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{o}{*}\PYG{n}{fs}\PYG{o}{/}\PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{total\PYGZus{}length} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{window\PYGZus{}length}\PYG{o}{*}\PYG{n}{zero\PYGZus{}extension\PYGZus{}factor}\PYG{p}{)}


\PYG{n}{window\PYGZus{}count} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{floor}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{o}{/}\PYG{n}{window\PYGZus{}step}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{spectrum\PYGZus{}length} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{p}{(}\PYG{n}{total\PYGZus{}length}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}
\PYG{n}{windowing\PYGZus{}function} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{n}{window\PYGZus{}length}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}

\PYG{n}{spectrogram} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{window\PYGZus{}count}\PYG{p}{,}\PYG{n}{spectrum\PYGZus{}length}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{time\PYGZus{}vector} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{window\PYGZus{}length\PYGZus{}ms}\PYG{p}{,}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}
\PYG{n}{frequency\PYGZus{}vector} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{fs}\PYG{o}{/}\PYG{l+m+mi}{2000}\PYG{p}{,}\PYG{n}{spectrum\PYGZus{}length}\PYG{p}{)}


\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{window\PYGZus{}count}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{starting\PYGZus{}position} \PYG{o}{=} \PYG{n}{k}\PYG{o}{*}\PYG{n}{window\PYGZus{}step}
    
    \PYG{n}{data\PYGZus{}vector} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{starting\PYGZus{}position}\PYG{p}{:}\PYG{p}{(}\PYG{n}{starting\PYGZus{}position}\PYG{o}{+}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{p}{,}\PYG{p}{]}
    \PYG{n}{window\PYGZus{}spectrum} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{scipy}\PYG{o}{.}\PYG{n}{fft}\PYG{o}{.}\PYG{n}{rfft}\PYG{p}{(}\PYG{n}{data\PYGZus{}vector}\PYG{o}{*}\PYG{n}{windowing\PYGZus{}function}\PYG{p}{,}\PYG{n}{n}\PYG{o}{=}\PYG{n}{total\PYGZus{}length}\PYG{p}{)}\PYG{p}{)}
    
    \PYG{n}{spectrogram}\PYG{p}{[}\PYG{n}{k}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{window\PYGZus{}spectrum}


\PYG{n}{black\PYGZus{}eps} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}1} \PYG{c+c1}{\PYGZsh{} minimum value for the log\PYGZhy{}value in the spectrogram \PYGZhy{} helps making the background really black}
    
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib} \PYG{k}{as} \PYG{n+nn}{mpl}
\PYG{n}{default\PYGZus{}figsize} \PYG{o}{=} \PYG{n}{mpl}\PYG{o}{.}\PYG{n}{rcParamsDefault}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{rcParams}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{n}{val}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{val} \PYG{o+ow}{in} \PYG{n}{default\PYGZus{}figsize}\PYG{p}{]}

\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{rcParams}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{n}{val}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{val} \PYG{o+ow}{in} \PYG{n}{default\PYGZus{}figsize}\PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log10}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{spectrogram}\PYG{p}{)}\PYG{p}{)}\PYG{o}{+}\PYG{n}{black\PYGZus{}eps}\PYG{p}{)}\PYG{p}{,}\PYG{n}{aspect}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{origin}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{extent}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{o}{/}\PYG{n}{fs}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{fs}\PYG{o}{/}\PYG{l+m+mi}{2000}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (s)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency (kHz)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{o}{/}\PYG{n}{fs}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Spectrogram zoomed to 8 kHz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_analysis_28_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsubsection{Speech features visible in the spectrogram}
\label{\detokenize{Representations/Short-time_analysis:speech-features-visible-in-the-spectrogram}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Comb\sphinxhyphen{}structure of the fundamental frequency F0 is visible as horizontal lines. Changes in F0 over time can be easily followed.

\item {} 
\sphinxAtStartPar
Time\sphinxhyphen{}structure of speech is clearly visible. We can spot breaks in speech as well as changes in phonemes.

\item {} 
\sphinxAtStartPar
Unvoiced phonemes (speech sounds without a fundamental frequency) are also visible. Typically sustained consonants (fricatives, such as /h/ or /s/) have noise at the high frequencies.
Stop consonants (/p/, /k/, /t/) are short wide\sphinxhyphen{}band bursts.

\end{itemize}


\subsection{Short\sphinxhyphen{}time spectral analysis summary}
\label{\detokenize{Representations/Short-time_analysis:short-time-spectral-analysis-summary}}
\sphinxAtStartPar
We have presented a spectral \sphinxstyleemphasis{analysis} methods for speech.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Spectral analysis can nicely visualise many of the most important
features of speech signals.

\item {} 
\sphinxAtStartPar
The decibel scale corresponds roughly to perceptual sensitivity.

\item {} 
\sphinxAtStartPar
Short\sphinxhyphen{}time Fourier transform is the most common speech analysis method.

\item {} 
\sphinxAtStartPar
Spectrogram is visualizes spectral behaviour over time.

\end{itemize}

\sphinxstepscope


\section{Short\sphinxhyphen{}time processing of speech signals}
\label{\detokenize{Representations/Short-time_processing:short-time-processing-of-speech-signals}}\label{\detokenize{Representations/Short-time_processing::doc}}
\sphinxAtStartPar
In the \DUrole{xref,myst}{short\sphinxhyphen{}time \sphinxstyleemphasis{analysis}} section we already discussed speech signals and analysis methods, and found that we need to split the signal into shorter segments and apply windowing functions. Here we discuss what extra steps we need to consider when we want to \sphinxstyleemphasis{process} signals, that is, when we want to modify the signal.

\sphinxAtStartPar
To process a windowed signal, we thus need the steps:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
windowing

\item {} 
\sphinxAtStartPar
time\sphinxhyphen{}frequency transform such as the DFT (\sphinxstyleemphasis{optional})

\item {} 
\sphinxAtStartPar
apply the desired processing

\item {} 
\sphinxAtStartPar
inverse time\sphinxhyphen{}frequency transform (\sphinxstyleemphasis{when DFT was applied})

\item {} 
\sphinxAtStartPar
reverse of windowing (?).

\end{enumerate}

\sphinxAtStartPar
Here the analysis steps 1\sphinxhyphen{}2 were already discussed and the desired processing is whatever modification to the signal that you might have. The question is thus about the inverse transforms in steps 4 and 5.
Time\sphinxhyphen{}frequency transforms such as the \sphinxhref{https://en.wikipedia.org/wiki/Discrete\_Fourier\_transform}{discrete Fourier transform (DFT)} and the \sphinxhref{https://en.wikipedia.org/wiki/Discrete\_Fourier\_transform}{discrete cosine transform (DCT)} are orthonormal and have well\sphinxhyphen{}known fast algorithms for their inverses. The main challenge is thus the \sphinxstyleemphasis{“reverse of windowing”}, whatever that might be.

\sphinxAtStartPar
The direct approach of just multiplying with the inverse of the windowing function has a problem.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}
\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_processing_2_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Namely, near the ends of the window, the windowing function goes smoothly to zero and thus the inverse of the windowing function approaches infinity. That clearly leads to numerical issues; very small changes in the windowed signal can lead to arbitrarily large samples after inverse windowing.

\sphinxAtStartPar
We thus need a method which does not rely on inverting the windowing function.


\subsection{Overlap\sphinxhyphen{}add}
\label{\detokenize{Representations/Short-time_processing:overlap-add}}
\sphinxAtStartPar
A majority of modern speech processing is based on the overlap\sphinxhyphen{}add method, where the input signal is windowed in overlapping segments, such that when the overlapping parts are added together, we can perfectly reconstruct the original signal. It is like a cross\sphinxhyphen{}fade between subsequent windows.

\sphinxAtStartPar
The most common scenario where overlap\sphinxhyphen{}add cannot be used are applications which require a very low algorithmic delay. For example, stage microphones at concerts and theaters require a low\sphinxhyphen{}delay, such that interaction between people on the stage is not disrupted. Almost always otherwise, the best option is overlap\sphinxhyphen{}add.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_processing_5_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
In the above example, we see two overlapping windows extracted from the original signal. When we add the two together we obtain the “Overlap\sphinxhyphen{}add” signal which, we subtracted from the original, is zero in the whole region of the overlap. At the left and right ends, where we do not have an overlapping window, we do not get perfect reconstruction.


\subsubsection{Windowing and reconstruction}
\label{\detokenize{Representations/Short-time_processing:windowing-and-reconstruction}}
\sphinxAtStartPar
Overlapping segments are added together (fade\sphinxhyphen{}in/fade\sphinxhyphen{}out).
Windowing should be chosen such that reconstruction is exactly  equal to the original.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Let original signal be \(x_n\).

\item {} 
\sphinxAtStartPar
The left and right parts of overlap windows are then \(w_{L,n}\) and \(w_{R,n}\) and the windowed signals are \(w_{L,n}x_n\) and \(w_{R,n}x_n\).

\item {} 
\sphinxAtStartPar
The reconstruction is \(\hat x_n = w_{L,n}x_n + w_{R,n}x_n = (w_{L,n} + w_{R,n})x_n\).

\item {} 
\sphinxAtStartPar
With \(w_{L,n} + w_{R,n} = 1\) we obtain \sphinxstyleemphasis{perfect reconstruction}, \(x_n = \hat x_n\).

\end{itemize}

\sphinxAtStartPar
The window in the above example already adheres to this requirement.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_processing_8_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsubsection{Windowing and processing}
\label{\detokenize{Representations/Short-time_processing:windowing-and-processing}}
\sphinxAtStartPar
The whole point of windowing for processing was that we could modify the windowed signal and that we can then synthesise the modified signal. Suppose \(x':=wx\) is the windowed signal and the modified signal is \(\hat x'\), such that the modification part is \(e=x'-\hat x\). In overlap\sphinxhyphen{}add, we would then take multiple modified windows \(\hat x_k'\) and add them together. We have already seen that the part which corresponds to the original signal \(x'\) will perfectly reconstruct to the original signal. The question is however what happens to the modification part?

\sphinxAtStartPar
With a direct implementation, we would just add the modifications together. There is then no guarantee that consecutive modifications play nicely together and we can, for example, have discontinuities between windows.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_processing_10_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We therefore need to multiply also the modified windows with a windowing function.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_processing_12_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\paragraph{Algorithm ``Overlap\sphinxhyphen{}add’’}
\label{\detokenize{Representations/Short-time_processing:algorithm-overlap-add}}
\sphinxAtStartPar
Let \(w_{in,n}\) and \(w_{out,n}\) be the input and output windowing functions.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The input signal is \(x_n\) is windowed at the input to obtain  \(w_{in,n}x_n\).

\item {} 
\sphinxAtStartPar
The windowed signal is modified with \(e_n\) to obtain \(w_{in,n}x_n+e_n\).

\item {} 
\sphinxAtStartPar
We apply an output window \(w_{out,n}\) to the modified signal to obtain \(w_{out,n}(w_{in,n}x_n+e_n)\).

\item {} 
\sphinxAtStartPar
Add subsequent, overlapping windows together to obtain the output signal.

\end{enumerate}

\sphinxAtStartPar
Then if the output windows go to zero at the border, then the output signal will be continuous.
With no\sphinxhyphen{}modification \(e_n=0\), the output is \(w_{out,n}w_{in,n}x_n\).
In other words, if the left and right parts add up \$\( w_{L,out,n}w_{L,in,n} + w_{R,out,n}w_{R,in,n} = 1 \)\$ then we have perfect reconstruction.

\sphinxAtStartPar
If we the modification is uniform white noise, then the  modification part overlap is \(w_{L,out,n}e_{L,n} + w_{R,out,n}e_{R,n}\).
The energy expectation of the modification is then
\begin{equation*}
\begin{split}  E\left[\left(w_{L,out,n}e_{L,n} + w_{R,out,n}e_{R,n}\right)^2\right]  = \left(w^2_{L,out,n} + w^2_{R,out,n}\right) E[e_n^2]. \end{split}
\end{equation*}
\sphinxAtStartPar
If \(w^2_{L,out,n} + w^2_{R,out,n}=1\) then output energy is uniform.
To fulfil the criteria, we can set the input and output    windows to be the same \(w_{in,n} = w_{out,n}\).

\sphinxAtStartPar
We can then require that (\sphinxstyleemphasis{Princen\sphinxhyphen{}Bradley condition})
\begin{equation*}
\begin{split}  
\boxed{w^2_{L,n} + w^2_{R,n} = 1}. 
\end{split}
\end{equation*}
\sphinxAtStartPar
and that \(w_n\) goes to zero at the borders.
Overlap\sphinxhyphen{}add obtains the following properties:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Perfect reconstruction} – If there is no modification, we can reconstruct the original signal.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Continuous output} – There are no discontinuities.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Uniform noise energy} – Output noise does not have temporal   structure (noise has a smooth energy envelope).

\end{itemize}

\sphinxAtStartPar
One such windowing function is the half\sphinxhyphen{}sine
\begin{equation*}
\begin{split}  
w_n = \sin\left(\frac{\pi n}{N}\right).\qquad\text{(It is the square root of a Hann-window.)}
\end{split}
\end{equation*}
\sphinxAtStartPar
We can readily show that it fulfils the    Prince\sphinxhyphen{}Bradly condition.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Short-time_processing_14_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsubsection{Overlap\sphinxhyphen{}add summary}
\label{\detokenize{Representations/Short-time_processing:overlap-add-summary}}
\sphinxAtStartPar
Overlap\sphinxhyphen{}add is a method for windowing a signal such that we can modify the segments \sphinxstyleemphasis{and} reconstruct the modified signal.

\sphinxAtStartPar
\sphinxstyleemphasis{Algorithm}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Applying windowing function \(w_n\).

\item {} 
\sphinxAtStartPar
Modify/process window with your\sphinxhyphen{}algorithm\sphinxhyphen{}of\sphinxhyphen{}choice.

\item {} 
\sphinxAtStartPar
Applying windowing function \(w_n\) again.

\item {} 
\sphinxAtStartPar
Add overlapping segments together to obtain output signal.

\end{enumerate}

\sphinxAtStartPar
Usually we would perform a time\sphinxhyphen{}frequency transform on the    windowed signal \(w_nx_n\) and perform modifications in the    frequency\sphinxhyphen{}domain. (Almost) all frequency\sphinxhyphen{}domain processing algorithms are based on  overlap\sphinxhyphen{}add.


\subsection{The short\sphinxhyphen{}time Fourier transform (STFT)}
\label{\detokenize{Representations/Short-time_processing:the-short-time-fourier-transform-stft}}
\sphinxAtStartPar
Overlap\sphinxhyphen{}add is typically combined with taking discrete Fourier transforms of the windowed signal, as well as an inverse transforms after processing. This algorithm is known as the \sphinxstyleemphasis{short\sphinxhyphen{}time Fourier transform (STFT)} and it is the most commonly used domain for speech and audio processing. It is so common that often when we talk about a time\sphinxhyphen{}frequency transform in conjunction with processing algorithms, we implicitly mean the STFT.

\sphinxAtStartPar
\sphinxstyleemphasis{Algorithm}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Applying windowing function \(w_n\).

\item {} 
\sphinxAtStartPar
Apply the discrete Fourier transform (DFT) on the windowed signal.

\item {} 
\sphinxAtStartPar
Modify/process window with your\sphinxhyphen{}algorithm\sphinxhyphen{}of\sphinxhyphen{}choice.

\item {} 
\sphinxAtStartPar
Apply the inverse DFT on the modified windows.

\item {} 
\sphinxAtStartPar
Applying windowing function \(w_n\) again.

\item {} 
\sphinxAtStartPar
Add overlapping segments together to obtain output signal.

\end{enumerate}

\sphinxstepscope


\section{Waveform}
\label{\detokenize{Representations/Waveform:waveform}}\label{\detokenize{Representations/Waveform::doc}}
\sphinxAtStartPar
Speech signals are sound signals, defined as pressure variations
travelling through the air. These variations in pressure can be
described as waves and correspondingly they are often called sound
waves. In the current context, we are primarily interested in analysis
and processing of such waveforms in digital systems. We will therefore
always assume that the acoustic speech signals have been captured by a
microphone and converted to a digital form.

\sphinxAtStartPar
A speech signal is then represented by a sequence of numbers \( x_n \)
, which represent the relative air pressure at time\sphinxhyphen{}instant \(
n\in{\mathbb N} \) .  This representation is known as \sphinxhref{https://en.wikipedia.org/wiki/Pulse-code\_modulation}{pulse code
modulation} often
abbreviated as \sphinxstyleemphasis{PCM}. The accuracy of this representation is then
specified by two factors; 1) the sampling frequency (the step in time
between \(n\) and \(n+1\)) and 2) the accuracy and distribution of
amplitudes of \(x_n\).

\sphinxAtStartPar
\sphinxincludegraphics{{1482949661}.png}


\subsection{Sampling rate}
\label{\detokenize{Representations/Waveform:sampling-rate}}
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Sampling\_(signal\_processing)}{Sampling}
is a classic topic of signal processing. Here the most important aspect
is the Nyquist frequency, which is half the sampling rate
\(F_s\) and defines the upper end of the largest bandwidth \(
\left[0, \frac{F_s}2\right] \) which can be uniquely represented.
In other words, if the sampling frequency would be 8000 Hz, then signals
in the frequency range 0 to 4000 Hz can be uniquely described with this
sampling frequency. The AD\sphinxhyphen{}converter would then have to contain a
low\sphinxhyphen{}pass filter which removes any content above the Nyquist frequency.

\sphinxAtStartPar
The most important information in speech signals are the formants, which
reside in the range 300 Hz to 3500 Hz, such that a lower limit for the
sampling rate is around 7 or 8kHz. In fact, first digital speech codecs
like the AMR\sphinxhyphen{}NB use a sampling rate of 8 kHz known as narrow\sphinxhyphen{}band. Some
consonants, especially fricatives like /s/, however contain substantial
energy above 4kHz, whereby narrow\sphinxhyphen{}band is not sufficient for high
quality speech. Most energy however remains below 8kHz such that
wide\sphinxhyphen{}band, that is, a sampling rate of 16 kHz is sufficient for most
purposes. Super\sphinxhyphen{}wide band and full band further correspond,
respectively, to sampling rates of 32 kHz and 44.1 kHz (or 48kHz). The
latter is also the sampling rate used in compact discs (CDs). Such
higher rates are useful when considering also non\sphinxhyphen{}speech signals like
music and generic audio.

\sphinxAtStartPar
Frequency\sphinxhyphen{}range of different bandwidth\sphinxhyphen{}definitions

\sphinxAtStartPar
\sphinxincludegraphics{{175528670}.png}


\subsection{Static demo}
\label{\detokenize{Representations/Waveform:static-demo}}
\sphinxAtStartPar
Sound samples at different bandwidths

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Original (0 to 22050 Hz)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Narrowband (300 Hz to 3.3 kHz)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Wideband (50 Hz to 7 kHz)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Superwideband (50 Hz to 16 kHz)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Fullband (50 Hz to 22 kHz)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{On\sphinxhyphen{}line demo}
\label{\detokenize{Representations/Waveform:on-line-demo}}

\subsubsection{Original sound sample}
\label{\detokenize{Representations/Waveform:original-sound-sample}}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Waveform_5_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsubsection{Resampling}
\label{\detokenize{Representations/Waveform:resampling}}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(IntSlider(value=16000, description=\PYGZsq{}sampling\PYGZus{}rate\PYGZsq{}, max=44100, min=2000, step=500), Outp…
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Accuracy and distribution of steps on the amplitude axis}
\label{\detokenize{Representations/Waveform:accuracy-and-distribution-of-steps-on-the-amplitude-axis}}
\sphinxAtStartPar
In digital representations of a signal you are forced to use a finite
number of steps to describe the amplitude. In practice, we must quantize
the signal to some discrete levels.


\subsubsection{Linear quantization}
\label{\detokenize{Representations/Waveform:linear-quantization}}
\sphinxAtStartPar
Linear quantization with a step size \(\Delta q \) would correspond
to defining the quantized signal as
\begin{equation*}
\begin{split} 
\hat x = \Delta q\,\cdot {\mathrm{round}}(x/\Delta q). 
\end{split}
\end{equation*}
\sphinxAtStartPar
The intermediate representation, \( y={\mathrm{round}}(x/\Delta q),\)
can then be taken to represent, for example, signed 16\sphinxhyphen{}bit integers.
Consequently, the quantization step size  \( \Delta q \) has to be
then chosen such that \(y\) remains in the range  \(
y\in(-2^{15},\,2^{15}] \) to avoid numerical overflow.

\sphinxAtStartPar
The beauty of this approach is that it is very simple to implement. The
drawback is that this approach is sensitive to the choice of the
quantization step size. To make use of the whole range and thus get best
accuracy for \(x\), we should choose the smallest  \( \Delta q \) where
we still remain within the bounds of integers. This is difficult because
the amplitudes of speech signals vary on a large range.

\sphinxAtStartPar
\sphinxincludegraphics{{149882928}.png}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(IntSlider(value=16000, description=\PYGZsq{}sampling\PYGZus{}rate\PYGZsq{}, max=44100, min=2000, step=500), IntS…
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsubsection{Logarithmic quantization and mu\sphinxhyphen{}law}
\label{\detokenize{Representations/Waveform:logarithmic-quantization-and-mu-law}}
\sphinxAtStartPar
To retain equal accuracy for loud and weak signals, we \sphinxstyleemphasis{could} quantize
on an logarithmic scale as
\$\( \hat x = {\mathrm{sign}}(x)\cdot\exp\left[ \Delta
q\cdot\,{\mathrm{round}} \left(\log\left(\|x\|\right)/\Delta
q\right) \right]. \)\$
Such operations which limit the detrimental effects of limited range are
known as \sphinxstyleemphasis{\sphinxhref{https://en.wikipedia.org/wiki/Companding}{companding}}
algorithms.

\sphinxAtStartPar
Here the intermediate representation is  \( y = {\mathrm{round}}
\left(\log\left(\|x\|\right)/\Delta q\right) \) which can be
reconstructed by  \( \hat x = {\mathrm{sign}}(x)\cdot\exp\left[
\Delta q\cdot\,\|y\| \right]. \) A benefit of this approach would
be that we can encode signals on a much larger range and the
quantization accuracy is relative to the signal magnitude.
Unfortunately, very small values cause catastrophic problems. In
particular, for \(x=0\), the intermediate value goes to negative infinity
\( y=-\infty, \) which is not realizable in finite digital systems.

\sphinxAtStartPar
A practical solution to this problem is quantization with the mu\sphinxhyphen{}law
algorithm, which defines a modified logarithm as
\begin{equation*}
\begin{split}
F(x):={\mathrm{sign}}(x)\cdot\,\frac{\log\left(1+\mu\|x\|\right)}{\left(1+\mu\right)}.
\end{split}
\end{equation*}
\sphinxAtStartPar
By replacing the logarithm with \(F(x)\), we retain the properties of the
logarithm for large \(x\), but avoid the problems when \(x\) is small.

\sphinxAtStartPar
\sphinxincludegraphics{{149882926}.png}
\sphinxincludegraphics{{149882927}.png}


\subsection{Wav\sphinxhyphen{}files}
\label{\detokenize{Representations/Waveform:wav-files}}
\sphinxAtStartPar
The most typical format for storing sound signals is the \sphinxhref{https://en.wikipedia.org/wiki/WAV}{wav\sphinxhyphen{}file
format}. It is basically merely a way
to store a time sequence, with typically either 16 or 32 bit accuracy,
as integer, mu\sphinxhyphen{}law or float. Sampling rates can vary in a large range
between 8 and 384 kHz. The files typically have no compression (no
lossless nor lossy coding), such that recording hours of sound can
require a lot of disk space. For example, an hour of mono (single
channel) sound with a sampling rate of 44.1kHz requires 160 MB of disk
space.


\subsection{Adaptive quantization, APCM}
\label{\detokenize{Representations/Waveform:adaptive-quantization-apcm}}\begin{itemize}
\item {} 
\sphinxAtStartPar
To obtain a uniform quantization error during single phones or
sentences, the quantization error has to change slowly over time.

\item {} 
\sphinxAtStartPar
In \sphinxstyleemphasis{adaptive quantization} (adaptive PCM or APCM) the quantization
step size is adapted slowly such that
\begin{itemize}
\item {} 
\sphinxAtStartPar
the available quantization levels cover a sufficient range such
that numerical overflow can be avoided,

\item {} 
\sphinxAtStartPar
the quantization error is stable over time and

\item {} 
\sphinxAtStartPar
as long as the above constraints are fulfilled, quantization
error is minimized.

\end{itemize}

\item {} 
\sphinxAtStartPar
An alternative, equivalent implementation to the change in
quantization step size is to apply an adaptive gain to the input
signal before quantization.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175529208}.png}
\sphinxincludegraphics{{175529206}.png}


\subsubsection{Adaptive quantization with the feed\sphinxhyphen{}forward algorithm using an adaptive quantization step}
\label{\detokenize{Representations/Waveform:adaptive-quantization-with-the-feed-forward-algorithm-using-an-adaptive-quantization-step}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The feed\sphinxhyphen{}forward algorithm requires that in addition to the
quantized signal, also the gain\sphinxhyphen{}coefficients or the quantization
step is transmitted to the recipient.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Transmitting such extra information increases the bit\sphinxhyphen{}rate,
whereby the feed\sphinxhyphen{}forward algorithm is not optimal for
applications which try to minimize transmission rate.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175529207}.png}


\subsubsection{Adaptive quantization with the feed\sphinxhyphen{}forward algorithm using an adaptive gain (compressor)}
\label{\detokenize{Representations/Waveform:adaptive-quantization-with-the-feed-forward-algorithm-using-an-adaptive-gain-compressor}}\begin{itemize}
\item {} 
\sphinxAtStartPar
In \sphinxstyleemphasis{feed\sphinxhyphen{}backward} algorithms the quantization step or
gain\sphinxhyphen{}coefficient is determined from previous samples which are
already quantized.

\item {} 
\sphinxAtStartPar
Since the previous samples are available also at the decoder, the quantization step or gain\sphinxhyphen{}coefficient can be determined also at the decoder without extra transmitted information.

\item {} 
\sphinxAtStartPar
If the signal grows very rapidly, this approach can however not guarantee that there are no numerical overflows, since adaptation is performed only after quantization.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175529205}.png}


\subsubsection{Adaptive quantization with the feed\sphinxhyphen{}backward algorithm using an adaptive quantization step}
\label{\detokenize{Representations/Waveform:adaptive-quantization-with-the-feed-backward-algorithm-using-an-adaptive-quantization-step}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Note that the feed\sphinxhyphen{}forward algorithms all require transmission of the scaling or gain coefficient, which can increase demand on bandwidth and adds to the complexity of the system.

\item {} 
\sphinxAtStartPar
The parallel transmission line can be avoided by predicting those
coefficients from previously transmitted elements, with a
feed\sphinxhyphen{}backward algorithm.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175529204}.png}


\subsubsection{Adaptive quantization with the feed\sphinxhyphen{}backward algorithm using an adaptive gain coefficient}
\label{\detokenize{Representations/Waveform:adaptive-quantization-with-the-feed-backward-algorithm-using-an-adaptive-gain-coefficient}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The feed\sphinxhyphen{}backward algorithm can naturally be applied on gain
adaptation as well.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175529203}.png}


\paragraph{Differential quantization DPCM}
\label{\detokenize{Representations/Waveform:differential-quantization-dpcm}}\begin{itemize}
\item {} 
\sphinxAtStartPar
In \sphinxstyleemphasis{differential quantization} we predict the subsequent sample,
whereby we can quantize only the difference between the prediction
and the actual sample.

\item {} 
\sphinxAtStartPar
If the predictor is simply \( \tilde x_k:=x_{k-1}, \) then the error is \( e_k = x_k - \tilde x_k = x_k - x_{k-1}. \)

\item {} 
\sphinxAtStartPar
The first difference (delta modulation) is the simplest  predictor, which uses the assumption that subsequent samples are highly correlated.

\item {} 
\sphinxAtStartPar
The reconstruction is obtained by reorganization of terms as \( x_k = e_k + x_{k-1}. \)

\item {} 
\sphinxAtStartPar
Observe that the reconstruction is needed at both the encoder and decoder, to feed the predictor.

\item {} 
\sphinxAtStartPar
NB: At this point the flow\sphinxhyphen{}graphs start to get a bit complicated         as there are several feedback loops.

\item {} 
\sphinxAtStartPar
More generally, we can use a predictor \(P\), which predicts a sample
based on a weighted sum of previous samples
\begin{equation*}
\begin{split}
    \tilde x_k = -\sum_{h=1}^M a_h x_{k-h}, 
    \end{split}
\end{equation*}
\sphinxAtStartPar
where the scalars  \( a_h \)  are the predictor parameters.

\item {} 
\sphinxAtStartPar
A feed\sphinxhyphen{}backward would here use the past quantized samples \( \hat
x_k. \)

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175529209}.png}


\subsubsection{Adaptive and differential quantization with feed\sphinxhyphen{}forward}
\label{\detokenize{Representations/Waveform:adaptive-and-differential-quantization-with-feed-forward}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The differential, source\sphinxhyphen{}model based quantization can naturally be combined with adaptive, perception\sphinxhyphen{}based quantization.

\item {} 
\sphinxAtStartPar
The adaptive differential PCM (ADPCM) adaptively predicts the signal and adaptively choosing the quantization step.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175529202}.png}


\subsubsection{Adaptive differential quantization with feed\sphinxhyphen{}backward}
\label{\detokenize{Representations/Waveform:adaptive-differential-quantization-with-feed-backward}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The ADPCM can again, naturally, be implemented as a feed\sphinxhyphen{}backward algorithm as well.

\end{itemize}


\subsubsection{Adaptive differential quantization w/ adaptive predictor}
\label{\detokenize{Representations/Waveform:adaptive-differential-quantization-w-adaptive-predictor}}\begin{itemize}
\item {} 
\sphinxAtStartPar
A differential quantizer can be further improved by letting also the
predictor be adaptive.

\item {} 
\sphinxAtStartPar
The predictor learns adaptively properties of the signal.

\item {} 
\sphinxAtStartPar
The flow\sphinxhyphen{}graph becomes complicated and is omitted here.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175529201}.png}


\subsubsection{Comparison of the SNR of different quantizers (not perceptual)}
\label{\detokenize{Representations/Waveform:comparison-of-the-snr-of-different-quantizers-not-perceptual}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The more bits we can use the better the quality (Duh!).

\item {} 
\sphinxAtStartPar
The more prior information we can use about the signal the better
the efficiency (SNR/bits).
\begin{itemize}
\item {} 
\sphinxAtStartPar
More advanced models (can) improve quality.

\item {} 
\sphinxAtStartPar
More parameters (can) improve quality.

\item {} 
\sphinxAtStartPar
It would naturally also be entirely possible to create
complicated models which do not improve quality, but simple
models can go only so far.

\end{itemize}

\item {} 
\sphinxAtStartPar
The linear prediction \sphinxhyphen{}approach can be extended into a full\sphinxhyphen{}blown
speech production model (see separate chapter).

\item {} 
\sphinxAtStartPar
Note that quality as measured by SNR does not necessarily reflect
perceptual quality.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175529210}.png}
Adapted from {[}\hyperlink{cite.Representations/Waveform:id44}{Noll, 1975}{]}


\subsection{Source modelling in quantization}
\label{\detokenize{Representations/Waveform:source-modelling-in-quantization}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Adaptive quantization is based on our understanding of perception;
we use the knowledge that we prefer slowly changing quantization
errors.
\begin{itemize}
\item {} 
\sphinxAtStartPar
This is a simple \sphinxstyleemphasis{perceptual model}.

\item {} 
\sphinxAtStartPar
Perceptual models are \sphinxstyleemphasis{quality evaluation} models.

\end{itemize}

\item {} 
\sphinxAtStartPar
When we know that the signal is \sphinxstyleemphasis{speech}, we can use that to further
improve quantization.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Models of speech signals are known as \sphinxstyleemphasis{source} models.

\end{itemize}

\item {} 
\sphinxAtStartPar
At its simplest form, we can use the fact that voiced phones are
fairly continuous signals = they have low\sphinxhyphen{}pass character = are
dominated by low\sphinxhyphen{}frequency components.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Samples have a high correlation.

\item {} 
\sphinxAtStartPar
The difference between subsequent samples is much smaller than
the magnitude of samples!

\end{itemize}

\item {} 
\sphinxAtStartPar
The amplitude of the first difference is 41\% of the original.

\item {} 
\sphinxAtStartPar
Uniform quantization of the first difference thus gives a 59\%
reduction in the range which is approx 1 bitsample. At 44kHz that
would be 44kbit/s improvement in bitrate, which is definitely
noticeable.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175529216}.png}


\subsection{Conclusion}
\label{\detokenize{Representations/Waveform:conclusion}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Time\sphinxhyphen{}domain representation of speech signals is simple in
floating\sphinxhyphen{}point processors.
\begin{itemize}
\item {} 
\sphinxAtStartPar
We only need to choose a sampling rate (typically in the range 8
to 48 kHz).

\end{itemize}

\item {} 
\sphinxAtStartPar
For fixed\sphinxhyphen{}point and lower\sphinxhyphen{}bitrate representations, we have several
considerations and options;
\begin{itemize}
\item {} 
\sphinxAtStartPar
We would like the quantization noise to be relative to the
signal magnitude but stable over time for best perceptual
quality.

\item {} 
\sphinxAtStartPar
We can use information about signal properties to improve
efficiency with a source model.

\item {} 
\sphinxAtStartPar
If we want to reduce bit\sphinxhyphen{}rate, then we must make sure that the
required information to decode the signal is available also at
the receiving end.

\end{itemize}

\item {} 
\sphinxAtStartPar
Practical processing algorithms for speech operate on a digital
representation of the acoustic signal.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Accuracy is determined by sampling rate and quantization.

\end{itemize}

\item {} 
\sphinxAtStartPar
Most common (high\sphinxhyphen{}quality) storage format for digital speech and
audio signals is PCM (such as WAV\sphinxhyphen{}files).

\item {} 
\sphinxAtStartPar
Some very basic analysis tools for speech signals include the
autocorrelation and the zero\sphinxhyphen{}crossing rate.

\item {} 
\sphinxAtStartPar
Many classical DSP algorithms, in their flow\sphinxhyphen{}grap representation,
are very much alike modern machine learning methods.

\end{itemize}


\subsection{Refrences}
\label{\detokenize{Representations/Waveform:refrences}}
\sphinxstepscope


\section{Windowing}
\label{\detokenize{Representations/Windowing:windowing}}\label{\detokenize{Representations/Windowing::doc}}
\sphinxAtStartPar
\sphinxincludegraphics{{1482949661}.png}

\sphinxAtStartPar
A spoken sentence is a sequence of phonemes. Speech signals are thus
time\sphinxhyphen{}variant in character. To extract information from a signal, we must
therefore split the signal into sufficiently short segments, such that,
heuristically speaking, each segment contains only one phoneme. In other
words, we want to extract segments which are short enough that the
properties of the speech signal does not have time change within that
segment.

\sphinxAtStartPar
\sphinxstyleemphasis{Windowing} is a classical method in signal processing and it refers to
splitting the input signal into temporal segments. The borders of
segments are then visible as discontinuities, which are incongruent with
the real\sphinxhyphen{}world signal. To reduce the impact of segmenting on the
statistical properties of the signal, we apply windowing to the temporal
segments. Windowing functions are smooth functions which go to zero at
the borders. By multiplying the input signal with a window function, the
windowing function also goes to zero at the border such that the
discontinuity at the border becomes invisible. Windowing does thus
change the signal, but the change is designed such that its effect on
signal statistics is minimized.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Windowing_3_0}.png}

\noindent\sphinxincludegraphics{{Windowing_3_1}.png}

\noindent\sphinxincludegraphics{{Windowing_3_2}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Quick reference}
\label{\detokenize{Representations/Windowing:quick-reference}}
\sphinxAtStartPar
There are two distinct applications of windowing with different
requirements; 1) analysis and 2) processing. In analysis, we only care
about extracting information as accurately as possible given
computational constraints, while in processing applications, we in
addition need the ability to recreate the signal from a sequence of
windows.


\subsubsection{Windowing for \sphinxstyleemphasis{analysis} applications}
\label{\detokenize{Representations/Windowing:windowing-for-analysis-applications}}
\sphinxAtStartPar
This is a classical signal processing topic covered by any basic book on
signal processing. Here we therefore present only the very
basics. Given an input signal \(x_k\), defined for all \(k\), and
a windowing function \(w_k\), defined on a limited range \(k\in[0,L) \) we can extract a window of the signal as
\begin{equation*}
\begin{split} x_{k,n}=x_{n-k}w_n. \end{split}
\end{equation*}
\sphinxAtStartPar
A classical windowing function, the Hann\sphinxhyphen{}window \(
w_n=\left[\sin\left(\pi n/L\right)\right]^2 \) is shown
below.

\sphinxAtStartPar
The main optimization criteria in choosing windowing functions is
spectral distortion. Namely, we would like that the windowed signal
resembles the original signal as much as possible. However, since it is
only a short sample, it cannot be exact. As windowing is multiplication
in the time\sphinxhyphen{}domain (see above equation), it corresponds to convolution
in the frequency domain. By looking at the spectrum of the windowing
function, we can therefore determine how much spreading of peaks in the
frequency will occur when we apply the windowing function.



\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Windowing_6_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsubsection{Windowing for \sphinxstyleemphasis{processing} applications; Overlap\sphinxhyphen{}add}
\label{\detokenize{Representations/Windowing:windowing-for-processing-applications-overlap-add}}
\sphinxAtStartPar
When we intend to modify the windowed signal with some processing, the
most common approach is to use a technique known as overlap\sphinxhyphen{}add. As seen
in the figure below, in overlap\sphinxhyphen{}add, we extract overlapping
windows of the signal, apply some processing, and reconstruct by
windowing a second time and then adding overlapping segments together.

\sphinxAtStartPar
An obvious requirement would then be that if the signal is not modified,
that we could then reconstruct the original signal perfectly; known as
the \sphinxstyleemphasis{perfect reconstruction} property. It is straightforward to
demonstrate that perfect reconstruction is achieved if overlapping
regions of the windowing function add up to unity. Note that here we
need to take into account the windowing is applied twice. That is, we
obtain perfect reconstruction if (Princen\sphinxhyphen{}Bradley criteria)
\begin{equation*}
\begin{split} w_n^2 + w_{n+L/2}^2 = 1.\qquad\text{for}\qquad k\in[0,L/2).
\end{split}
\end{equation*}
\sphinxAtStartPar
Here the squares follow from the fact that windows are applied twice.
Note that subsequent windows are then at a distance of half \(L/2\) the
length of the window.

\sphinxAtStartPar
A classical windowing function which follows the perfect reconstruction
criteria is the half\sphinxhyphen{}sine window, which is actually the square root of
the Hann\sphinxhyphen{}window. However, we have to here take special care that indices
are defined correctly, such that the half\sphinxhyphen{}sine is defined as \(
w_n=\sin\left(\pi (n+0.5)/L\right). \) Observe that the difference
to the Hann\sphinxhyphen{}window is thus the absence of a square. It the follows that,
after squaring, overlapping parts add up to unity.

\sphinxAtStartPar
The length of windows in the figure below is 30 ms, while the
shift between windows is 15 ms. This is known as 50\% overlap and it is
the most common approach, though it is possible to design low\sphinxhyphen{}overlap
windows (useful in low\sphinxhyphen{}delay applications). We can then observe that
analysis of the first window requires that we the signal is at least 30
ms long. \sphinxstyleemphasis{Analysis} of each additional window then requires 15 ms more
signal. That is, for analysis we have
\begin{equation*}
\begin{split} (Analysis\,signal\,length) = (windows-1)\times(step) +
(window\,length). \end{split}
\end{equation*}
\sphinxAtStartPar
However, for reconstruction, we see that we have perfect reconstruction
only in the segment between 15 ms and 60 ms. That is, only those overlap
areas are perfectly reconstructed, where we have access to both the left
and right windows. For reconstruction we then have
\begin{equation*}
\begin{split} (Reconstruction\,signal\,length) = (windows-1)\times(step). \end{split}
\end{equation*}


\sphinxAtStartPar
\sphinxincludegraphics{{148295002}.png}
\sphinxincludegraphics{{148294911}.png}


\subsection{Comprehensive description}
\label{\detokenize{Representations/Windowing:comprehensive-description}}
\sphinxAtStartPar
Specifically, suppose \(x_k\) is the \(k\)th sample of the input
signal. Let \(w_k\) be a windowing function (like the one in the
figure below) such that
\$\( \begin{cases} w_k > 0 & k\in[0,L-1] \\ w_k = 0 & k < 0
\text{ and } k \geq L\\ w_k \rightarrow 0 & \text{near the
borders}. \end{cases} \)\(
The windowed signal of length \)L\( is then
\)\( x'_k = w_k x_k. \)\(
In classical signal processing, the main design criteria for
choosing \)w\_k\$ are related to spectral resolution. Windowing
causes undesirable spreading of frequency components into nearby
frequencies and by choosing the windowing function, we can choose how
much and how far such a components are spread.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Windowing_10_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
In difference to classical signal analysis, speech processing
applications have a range of additional requirements. Most importantly,
speech processing applications are not only analyzing the signals, but
their purpose is to reconstruct the (modified) signal. The figure on the
right illustrates the process. If the signal is not modified, commonly,
our objective is that the signal can be perfectly reconstructed from the
sequence of windows. This is known as the \sphinxstyleemphasis{perfect reconstruction}
property.

\sphinxAtStartPar
In other words, a transform is said to have perfect reconstruction if
the original signal can be recovered perfectly from the transformed
representation.

\sphinxAtStartPar
In application using windowing, perfect reconstruction is achieved with
a process known as \sphinxstyleemphasis{overlap\sphinxhyphen{}add} (sometimes abbreviated as
OLA). 

\sphinxAtStartPar
The basic principle of overlap and add is to apply windowing in
overlapping segments, such that when the windows are later added
together, the original signal is recovered (see Figure below).

\sphinxAtStartPar
\sphinxincludegraphics{{148295002}.png}

\sphinxAtStartPar
As a first approach, let us define window \(h\) as
\begin{equation*}
\begin{split} x_{k,h} = w_{k-Lh/2} x_k. \end{split}
\end{equation*}
\sphinxAtStartPar
Subsequent windows \(x_{k,h-1}\) and \(x_{k,h}\), then have
non\sphinxhyphen{}zero portions which are overlapping (see figure) in
the region \( k\in[Lh/2,\, L(h+1)/2) \) . When we add them
together, we obtain
\begin{equation*}
\begin{split} \begin{split} x_{k,h-1} + x_{k,h} &= w_{k-L(h-1)/2} x_k +
w_{k-Lh/2} x_k \\&= \left(w_{k-L(h-1)/2} + w_{k-Lh/2}\right) x_k.
\end{split} \end{split}
\end{equation*}
\sphinxAtStartPar
It follows that the reconstruction is exactly equal to the
original \(x_{k,h-1} + x_{k,h}=x_k\), iff
\$\( w_{k+L/2} + w_{k} = 1,\qquad\text{for } k\in[0,\,L/2). \)\$
An example of a window which satisfies this requirement is the raised
cosine (or Hann) window, illustrated below and defined as
\begin{equation*}
\begin{split} w_k = \frac12\left[1-\sin\left(\frac{ 2(k+0.5)\pi
}L\right)\right] =
\left[\sin\left(\frac{\pi(k+0.5)}L\right)\right]^2. \end{split}
\end{equation*}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Windowing_14_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Unfortunately, when applying the above windowing in a processing
application, there is a problem. Suppose the windowed
signal \(x_{k,h}\) is modified in some way, for example, the
signal could be quantized and coded for transmission. The receiving
device would then see a modified signal \( \hat x_{k,h} = x_{k,h} +
e_{k,h} \) , where \(e_k,h\) is the modification applied to
window \(h\) and \(e_{k,h}\) is non\sphinxhyphen{}zero only for \( k \in [
Lh/2, L(h+1)/2) \) . The reconstructed signal, for the windows \(h\) and
\(h+1\), would then be (for \( k\in[Lh/2,\,L(h+1)/2) \) )
\begin{equation*}
\begin{split} \hat x_{k,h-1} + \hat x_{k,h} = x_k + e_{k,h-1} + e_{k,h}.
\end{split}
\end{equation*}
\sphinxAtStartPar
The reconstruction error is thus \(e_{k,h-1}+e_{k,h}\).
The problem here is that the modifications, \(e_{k,h-1}\) and
\(e_{k,h}\), appear here without windowing. Consequently, if the
modifications \(e_{k,h}\) are non\sphinxhyphen{}zero near the window borders,
the reconstruction will have discontinuities.

\sphinxAtStartPar
To avoid discontinuities for the modification parts \(e_{k,h}\),
we need to apply windowing also on the output signal. We therefore apply
windowing at both the input and output:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input: \(x_k\)

\item {} 
\sphinxAtStartPar
Analysis windowing:  \( x_{k,h} = w^{\text{in}}_{k-Lh/2} x_k. \)

\item {} 
\sphinxAtStartPar
Processing: \( \hat x_{k,h} = x_{k,h}+e_{k,h}. \)

\item {} 
\sphinxAtStartPar
Synthesis windowing: \( \hat x_{k,h}' = w^{\text{out}}_{k-Lh/2} \hat x_{k,h}. \)

\item {} 
\sphinxAtStartPar
Overlap\sphinxhyphen{}add for the region \( k\in[Lh/2,\,L(h+1)/2):  \hat x'_k = \hat x_{k,h-1}'+\hat x_{k,h}'. \)

\item {} 
\sphinxAtStartPar
Output: \( \hat x'_k. \)

\end{itemize}

\sphinxAtStartPar
The input and output windows are further illustrated in the Figure on
the right.

\sphinxAtStartPar
The output then has
\begin{equation*}
\begin{split} \begin{split} \hat x_{k,h-1}'+\hat x_{k,h} &=
w^{\text{out}}_{k-L(h-1)/2} \hat x_{k,h-1} +
w^{\text{out}}_{k-Lh/2} \hat x_{k,h} \\ &=
w^{\text{out}}_{k-L(h-1)/2}(x_{k,h-1}+e_{k,h-1}) +
w^{\text{out}}_{k-Lh/2} (x_{k,h}+e_{k,h}) \\ &=
w^{\text{out}}_{k-L(h-1)/2}(w_{\text{in},k-L(h-1)/2} x_k
+e_{k,h-1}) \\&\qquad+ w^{\text{out}}_{k-Lh/2}
(w_{\text{in},k-Lh/2} x_k+e_{k,h}) \\ &=
\left(w^{\text{out}}_{k-L(h-1)/2}w^{\text{in}}_{k-L(h-1)/2} +
w^{\text{out}}_{k-Lh/2} w^{\text{in}}_{k-Lh/2}\right) x_k
\\&\qquad +
w^{\text{out}}_{k-L(h-1)/2}e_{k,h-1}+w^{\text{out}}_{k-Lh/2}e_{k,h}.
\end{split} \end{split}
\end{equation*}
\sphinxAtStartPar
We immediately observe that all output errors \(e_{k,h}\) have
been multiplied with windowing functions, whereby discontinuities are
avoided. Moreover, perfect reconstruction is achieved iff
\begin{equation*}
\begin{split} w^{\text{out}}_{k+L/2}w^{\text{in}}_{k+L/2} +
w^{\text{out}}_{k} w^{\text{in}}_{k} =
1,\qquad\text{for } k\in[0,\,L/2). \end{split}
\end{equation*}
\sphinxAtStartPar
This leaves us with the design task of two windowing functions, \(
w^{\textrm{in}}_k \) and \( w^{\textrm{out}}_k \) .

\sphinxAtStartPar
To choose the output window, we can assume that the modifications to the
signal \(e_{k,h}\) are uncorrelated white noise of zero mean and
variance \( \sigma^2 \) . The output error energy is then (for \(
k\in[0,\,L/2) \) )
\begin{equation*}
\begin{split} \begin{split}
E&\left[\left(w^{\text{out}}_{k-L(h-1)/2}e_{k,h-1}+w^{\text{out}}_{k-Lh/2}e_{k,h}\right)^2\right]
\\& =
E\left[\left(w^{\text{out}}_{k-L(h-1)/2}e_{k,h-1}\right)^2\right]
+E\left[\left(w^{\text{out}}_{k-Lh/2}e_{k,h}\right)^2\right]
\\& = \left[\left(w^{\text{out}}_{k-L(h-1)/2}\right)^2
+\left(w^{\text{out}}_{k-Lh/2}\right)^2 \right]\sigma^2.
\end{split} \end{split}
\end{equation*}
\sphinxAtStartPar
Modulations in signal energy are perceptually undesirable, whereby we
can require that
\begin{equation*}
\begin{split} \left(w^{\text{out}}_{k+L/2}\right)^2
+\left(w^{\text{out}}_{k}\right)^2 =
1,\qquad\text{for } k\in[0,\,L/2). \end{split}
\end{equation*}
\sphinxAtStartPar
To simultaneously satisfy constraints on both input and output windows, we set \(w_k=w^{\text{in}}_{k}=w^{\text{out}}_{k}\),
such that our only criteria is
\begin{equation*}
\begin{split} \boxed{w_{k+L/2}^2 + w_{k}^2 =
1,\qquad\text{for } k\in[0,\,L/2).} \end{split}
\end{equation*}
\sphinxAtStartPar
This is known as the Princen\sphinxhyphen{}Bradley condition for overlapping
windows. 

\sphinxAtStartPar
\sphinxincludegraphics{{148295002}.png}
\sphinxincludegraphics{{148294911}.png}

\sphinxAtStartPar
Several windowing functions which satisfy the above criteria are known.
In fact, from any window which satisfies the reconstruction criteria, we
can obtain a window which satisfies the Princen\sphinxhyphen{}Bradley condition by
taking the square root. For example, we have  the half\sphinxhyphen{}sine window
\begin{equation*}
\begin{split} w_k = \begin{cases} \sin\left(\frac{(k+0.5)\pi}{L}\right), &
\textrm{for } 0\leq k  < L\\ 0, &\textrm{otherwise} \end{cases}
\end{split}
\end{equation*}
\sphinxAtStartPar
and the Kaiser\sphinxhyphen{}Bessel\sphinxhyphen{}derived (KBD) window
\begin{equation*}
\begin{split} w_k = \begin{cases} \gamma \sqrt{\sum_{h=0}^k
I_0\left(\pi\alpha \sqrt{1 -
\left(\frac{2h}{L-1}-1\right)^2}\right)}, & \textrm{for } 0\leq k
 < L\\ 0, &\textrm{otherwise}, \end{cases} \end{split}
\end{equation*}
\sphinxAtStartPar
where \(I_0()\) is the zeroth order modified Bessel function of
the first kind and γ is a scalar scaling coefficient chosen such that Princen\sphinxhyphen{}Bradley holds.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Windowing_19_0}.png}

\noindent\sphinxincludegraphics{{Windowing_19_1}.png}

\noindent\sphinxincludegraphics{{Windowing_19_2}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We see that the reconstruction is identical to the original, for the exception of the borders. In this example, we started windowing from the borders, such that the first window has only the right\sphinxhyphen{}hand side window in the overlap\sphinxhyphen{}region. Since the left window is missing, reconstruction does not give perfect output. The same happens at the end of the signal. In practice this is actually desirable, because now the reconstruction does not have discontinuities at its borders. However, if this reconstruction would be combined with other signals, then we would have to stitch their borders together.


\subsubsection{CELP windowing}
\label{\detokenize{Representations/Windowing:celp-windowing}}
\sphinxAtStartPar
Another type of windowing which supports perfect reconstruction is
applied in speech codecs using the code\sphinxhyphen{}excited linear prediction (CELP)
paradigm. Here, temporal statistics of the signal are modeled with a
predictive (IIR) filter and the filter residual is windowed with square
windows. 
In practice, this
approach works only with a computationally complex analysis\sphinxhyphen{}by\sphinxhyphen{}synthesis
methodology, and it has not received much attention outside the speech
coding community.


\subsubsection{Conclusion}
\label{\detokenize{Representations/Windowing:conclusion}}
\sphinxAtStartPar
Windowing with overlap\sphinxhyphen{}add is a basic and commonly used
tool in speech processing. It allows algorithms to modify sections of
the signal such that the modifications do not cause discontinuities to
the signal. A properly designed windowing for overlap\sphinxhyphen{}add does not in
itself cause distortions and the original signal can be perfectly
reconstructed from the windows. The only notable disadvantage of
overlapping windowing is that overlaps cause redundancy, since
information which appears in an overlap region between windows \(k\) and
\(k+1\), will then always be included in computations in both window \(k\)
and \(k+1\). Overlap\sphinxhyphen{}add processing can be modified to remove the
redundancy, by projecting the overlap area into two orthogonal
subspaces. Such methods are known as lapped transforms and are however
beyond the scope of the current
treatise. 

\sphinxAtStartPar
In general, still, I would advise using perfect reconstruction methods
in all speech processing applications (except coding applications where
lapped transforms are preferred). The system is then deterministic in
the sense that all modifications are due to the main processing
algorithm and windowing will never cause surprising side\sphinxhyphen{}effects.

\sphinxstepscope


\section{Signal energy, loudness and decibel}
\label{\detokenize{Representations/Signal_energy_loudness_and_decibel:signal-energy-loudness-and-decibel}}\label{\detokenize{Representations/Signal_energy_loudness_and_decibel::doc}}

\subsection{Signal energy}
\label{\detokenize{Representations/Signal_energy_loudness_and_decibel:signal-energy}}
\sphinxAtStartPar
By signal energy, we usually mean the variance of the signal, which is
the average squared deviation from the mean \(
Energy(x)=var(x)=E[(x-\mu)^2], \) where \( \mu=E[x] \) is the
average of the signal \sphinxstyleemphasis{x}. The variance is a measure of energy if we
interpret \sphinxstyleemphasis{x} as the displacement of a pendulum.

\sphinxAtStartPar
Since the amplitude of an oscillating signal varies through the period
of the oscillation, it does not usually make sense to estimate the
instantaneous energy, but only averaged over some {\hyperref[\detokenize{Representations/Windowing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{window}}}}.
Observe however that the windowing function reduces the average energy
(it multiplies the signal by a quantity smaller than unity), which
introduces a bias that should be corrected if an estimate of the
absolute energy is required. Usually, however, the bias is consistent
throughout a dataset and can be ignored.

\sphinxAtStartPar
Typical alternative energy estimates:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Energy can be calculated over \sphinxstyleemphasis{spectral bands}, often called energy
bands, that is, a range of frequencies of a {\hyperref[\detokenize{Representations/Spectrogram_and_the_STFT::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{time\sphinxhyphen{}frequency
transform}}}}, such as 0 to 1000 Hz, 1000 Hz
to 2000 Hz and so one for 1 kHz bands. Observe that the bands should
be wide enough that they have a “large” number  of frequency
components within them such that the variance can be estimated. Such
a representation is equivalent with an spectral envelope model of
the signal.

\item {} 
\sphinxAtStartPar
In a time\sphinxhyphen{}frequency representation, the energy of a single frequency
component can be estimated over time. That is, we can take the
average energy of a frequency component over several subsequent
frames or windows.

\end{itemize}

\sphinxAtStartPar
Observe that since both of these representations are calculated from
windowed signals, they will be similarly biased as the window itself.


\subsection{Decibel}
\label{\detokenize{Representations/Signal_energy_loudness_and_decibel:decibel}}
\sphinxAtStartPar
A commonly used unit for signal energy is
\sphinxhref{https://en.wikipedia.org/wiki/Decibel}{decibel} (dB). The formula to
convert a signal energy value \( \sigma^2 \) to decibels is
\begin{equation*}
\begin{split} 10\,\log_{10}\sigma^2. \end{split}
\end{equation*}
\sphinxAtStartPar
Decibel is thus a logarithmic measure of energy. Trivially, we can
convert this formula also to \( 20\,\log_{10}\sigma \) such that
it relates signal \sphinxstyleemphasis{magnitude} (the standard deviation) \( \sigma \)
to the decibel scale.

\sphinxAtStartPar
The benefit of using decibels is that signal energies have often a very
large range. By taking the logarithm, we obtain a representation where
for example visualizations are much easier to handle (see illustration
on the right). Moreover, decibels are also closer to human perception of
acoustic energy.


\subsubsection{Energy normalisation, loudness, dBFS and dBov}
\label{\detokenize{Representations/Signal_energy_loudness_and_decibel:energy-normalisation-loudness-dbfs-and-dbov}}
\sphinxAtStartPar
In practically all uses of acoustic data, we need to normalize the
sounds such that they have approximately the same volume or at least a
known volume. For example, consider a television program and
advertisements. Most would feel that it is very annoying if the
advertisements are much louder than the main program (see also \sphinxhref{https://en.wikipedia.org/wiki/Loudness\_war}{loudness
wars}). We thus need to
normalize the advertisement to match volume of the main program.
Normalizing the average energy of the advertisement to match that of the
main program is one crude way of doing that. Observe however that
perception of energy is different across frequency ranges such that
energy and the perceived loudness are not the same thing. To measure
loudness we therefore need to model subjective perception. This is an
involved subject and not discussed further here. Practical applications
however still need some normalization to avoid fundamental problems such
as clipping.

\sphinxAtStartPar
The energy measures decibel to overload, \sphinxstyleemphasis{dBov} and decibel to
full\sphinxhyphen{}scale, \sphinxstyleemphasis{dBFS,} are related to the dynamic range of a signal storage
or transmission format. Suppose for example that the maximum amplitude
that a digital representation in which a signal is represented is
\(x_{ov}\). If we would try to represent a larger amplitude than
that, then the signal would be clipped (distorted). dBov is a measure of
how much below the maximum amplitude (how much below clipping) a signal
is. Suppose \(P_{0}\) is the energy of the maximum\sphinxhyphen{}amplitude
square wave. Then the dBov of a signal with energy \(P\) is defined as
\begin{equation*}
\begin{split} L_{\text{ov}}=10\,\log _{10}\left({\frac
{P}{P_{0}}}\right). \end{split}
\end{equation*}
\sphinxAtStartPar
Since the energy of a sinusoid with maximum amplitude is \(\sqrt{\frac12}\) of the maximum\sphinxhyphen{}amplitude square wave, then its dBov
is \(-3.01\). Observe that dBov values are always negative. dBFS is a
similar

\sphinxAtStartPar
In typical cases, input speech signals are normalized to \(-26\) dBov such
that moderate processing of the signal is unlikely to cause clipping.

\sphinxAtStartPar
\sphinxincludegraphics{{149885445}.png}
\sphinxincludegraphics{{149885446}.png}

\sphinxAtStartPar
The energy (power) of a speech signal spectrum (above) and its logarithm
on the decibel scale (lower).

\sphinxstepscope


\section{Spectrogram and the STFT}
\label{\detokenize{Representations/Spectrogram_and_the_STFT:spectrogram-and-the-stft}}\label{\detokenize{Representations/Spectrogram_and_the_STFT::doc}}
\sphinxAtStartPar
We have intuitive notion of what a high or low pitch means. Pitch refers
to our perception of the frequency of a tonal sound. The
\sphinxhref{https://en.wikipedia.org/wiki/Fourier\_transform}{Fourier \sphinxstyleemphasis{spectrum}} of
a signal reveals such frequency content. This makes the spectrum an
intuitively pleasing domain to work in, because we can visually examine
signals.

\sphinxAtStartPar
In practice, we work with discrete\sphinxhyphen{}time signals, such that the
corresponding time\sphinxhyphen{}frequency transform is the \sphinxhref{https://en.wikipedia.org/wiki/Discrete\_Fourier\_transform}{discrete Fourier
transform}. It
maps a length \(N\) signal \(x_{n}\) into a complex valued frequency
domain representation \(X_{k}\) of \sphinxstyleemphasis{N} coefficients as
\begin{equation*}
\begin{split} X_k = \sum_{n=0}^{N-1}x_n e^{-i2\pi\frac{kn}N}. \end{split}
\end{equation*}
\sphinxAtStartPar
For real\sphinxhyphen{}valued inputs, positive and negative frequency components are
complex conjugates of each other, such that we retain \(N\) unique units
of information. However, since spectra are complex\sphinxhyphen{}valued vectors, it is
difficult to visualize them as such. A first solution would be to plot
the magnitude spectrum \(|X_{k}|\) or power
spectrum \(|X_{k}|^2\). Due to large differences in
the range of different frequencies, unfortunately these representations
do not easily show relevant information.

\sphinxAtStartPar
The log\sphinxhyphen{}spectrum  \( 20\log_{10}\|X_k\| \) is the most common
visualization of spectra and it gives the spectrum in decibels. It is
useful because again, it gives a visualization where sounds can be
easily interpreted.

\sphinxAtStartPar
Since Fourier transforms are covered by basic works in signal
processing, we will here assume that readers are familiar with the basic
properties of these transforms.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{io} \PYG{k+kn}{import} \PYG{n}{wavfile}
\PYG{k+kn}{import} \PYG{n+nn}{scipy}


\PYG{c+c1}{\PYGZsh{} read from storage}
\PYG{n}{filename} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sounds/test.wav}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{fs}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{wavfile}\PYG{o}{.}\PYG{n}{read}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{)}

\PYG{n}{window\PYGZus{}length\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{30}
\PYG{n}{window\PYGZus{}length} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{fs}\PYG{o}{*}\PYG{n}{window\PYGZus{}length\PYGZus{}ms}\PYG{o}{/}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{n} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{n}{window\PYGZus{}length}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{n}{num}\PYG{o}{=}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} windowing function}
\PYG{n}{windowing\PYGZus{}fn} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{o}{*}\PYG{n}{n}\PYG{o}{/}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{c+c1}{\PYGZsh{} sine\PYGZhy{}window}


\PYG{n}{windowpos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{datawin} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{windowpos}\PYG{p}{:}\PYG{p}{(}\PYG{n}{windowpos}\PYG{o}{+}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{datawin} \PYG{o}{=} \PYG{n}{datawin}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{datawin}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} normalize}

\PYG{n}{spectrum} \PYG{o}{=} \PYG{n}{scipy}\PYG{o}{.}\PYG{n}{fft}\PYG{o}{.}\PYG{n}{rfft}\PYG{p}{(}\PYG{n}{datawin}\PYG{o}{*}\PYG{n}{windowing\PYGZus{}fn}\PYG{p}{)}
\PYG{n}{f} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mf}{0.}\PYG{p}{,}\PYG{n}{fs}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{num}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{spectrum}\PYG{p}{)}\PYG{p}{)}


\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{n}\PYG{o}{*}\PYG{l+m+mi}{1000}\PYG{o}{/}\PYG{n}{fs}\PYG{p}{,}\PYG{n}{datawin}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (ms)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Amplitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A window of a signal without a windowing function (i.e. rectangular window)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{10.}\PYG{p}{,}\PYG{l+m+mf}{45.}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.}\PYG{p}{,}\PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}nx = np.concatenate(([\PYGZhy{}1000,0.],n,[window\PYGZus{}length,window\PYGZus{}length+1000]))}
\PYG{c+c1}{\PYGZsh{}datax = np.concatenate(([0.,0.],datawin,[0.,0.]))}
\PYG{c+c1}{\PYGZsh{}plt.plot(nx*1000/fs,datax)}
\PYG{c+c1}{\PYGZsh{}plt.xlabel(\PYGZsq{}Time (ms)\PYGZsq{})}
\PYG{c+c1}{\PYGZsh{}plt.ylabel(\PYGZsq{}Amplitude\PYGZsq{})}
\PYG{c+c1}{\PYGZsh{}plt.title(\PYGZsq{}Signal with a rectangular window looks as if it had a discontinuity at the borders\PYGZsq{})}
\PYG{c+c1}{\PYGZsh{}plt.axis([\PYGZhy{}10.,45.,\PYGZhy{}1.,1.])}
\PYG{c+c1}{\PYGZsh{}plt.tight\PYGZus{}layout()}
\PYG{c+c1}{\PYGZsh{}plt.show()}

\PYG{n}{nx} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1000}\PYG{p}{,}\PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,}\PYG{n}{n}\PYG{p}{,}\PYG{p}{[}\PYG{n}{window\PYGZus{}length}\PYG{p}{,}\PYG{n}{window\PYGZus{}length}\PYG{o}{+}\PYG{l+m+mi}{1000}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{datax} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,}\PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{,}\PYG{n}{datawin}\PYG{o}{*}\PYG{n}{windowing\PYGZus{}fn}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,}\PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{nx}\PYG{o}{*}\PYG{l+m+mi}{1000}\PYG{o}{/}\PYG{n}{fs}\PYG{p}{,}\PYG{n}{datax}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Windowed signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{n}\PYG{o}{*}\PYG{l+m+mi}{1000}\PYG{o}{/}\PYG{n}{fs}\PYG{p}{,}\PYG{n}{windowing\PYGZus{}fn}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Window function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (ms)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Amplitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Signal with a Hann window looks as if it would be continuous}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{10.}\PYG{p}{,}\PYG{l+m+mf}{45.}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.}\PYG{p}{,}\PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{f}\PYG{o}{/}\PYG{l+m+mi}{1000}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrum}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency (kHz)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Absolute Magnitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{The Fourier magnitude spectrum of the window}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{f}\PYG{o}{/}\PYG{l+m+mi}{1000}\PYG{p}{,}\PYG{l+m+mf}{20.}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log10}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrum}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency (kHz)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Log\PYGZhy{}Magnitude (dB)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{The Fourier log\PYGZhy{}spectrum of the window in decibel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Spectrogram_and_the_STFT_1_0}.png}

\noindent\sphinxincludegraphics{{Spectrogram_and_the_STFT_1_1}.png}

\noindent\sphinxincludegraphics{{Spectrogram_and_the_STFT_1_2}.png}

\noindent\sphinxincludegraphics{{Spectrogram_and_the_STFT_1_3}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatSlider(value=1.4048072562358276, description=\PYGZsq{}position\PYGZus{}s\PYGZsq{}, layout=Layout(width=\PYGZsq{}760…
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Speech signals are however non\sphinxhyphen{}stationary signals. If we transform a
spoken sentence to the frequency domain, we obtain a spectrum which is
an average of all phonemes in the sentence, whereas often we would like
to see the spectrum of each individual phoneme separately.

\sphinxAtStartPar
By splitting the signal into shorter segments, we can focus on signal
properties at a particular point in time. Such segmentation was already
discussed in the \sphinxstyleemphasis{{\hyperref[\detokenize{Representations/Windowing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{windowing}}}}} section.

\sphinxAtStartPar
By windowing and taking the discrete Fourier transform (DFT) of each
window, we obtain the \sphinxhref{https://en.wikipedia.org/wiki/Short-time\_Fourier\_transform}{short\sphinxhyphen{}time Fourier
transform}
(STFT) of the signal. Specifically, for an input signal
\(x_{n}\) and window \(w_{n}\), the transform is defined as
\begin{equation*}
\begin{split} STFT\{x_n\}(h,k) = X(h,k) = \sum_{n=0}^{N-1} x_{n+h} w_n
e^{-i2\pi\frac{kn}N}. \end{split}
\end{equation*}
\sphinxAtStartPar
The STFT is one of the most frequently used tools in speech analysis and
processing. It describes the evolution of frequency components over
time. Like the spectrum itself, one of the benefits of STFTs is that its
parameters have a physical and intuitive interpretation.

\sphinxAtStartPar
A further parallel with a spectrum is that the output of the STFT is
complex\sphinxhyphen{}valued, though where the spectrum is a vector, the STFT output
is a matrix. As a consequence, we cannot directly visualize the
complex\sphinxhyphen{}valued output. Instead, STFTs are usually visualized using their
log\sphinxhyphen{}spectra,  \( 20\log_{10}(X(h,k)). \) Such 2 dimensional
log\sphinxhyphen{}spectra can then be visualized with a heat\sphinxhyphen{}map known as
a \sphinxhref{https://en.wikipedia.org/wiki/Spectrogram}{spectrogram}.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{io} \PYG{k+kn}{import} \PYG{n}{wavfile}
\PYG{k+kn}{import} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k}{as} \PYG{n+nn}{ipd}
\PYG{k+kn}{from} \PYG{n+nn}{scipy} \PYG{k+kn}{import} \PYG{n}{signal}

\PYG{c+c1}{\PYGZsh{} read from storage}
\PYG{n}{filename} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sounds/test.wav}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{fs}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{wavfile}\PYG{o}{.}\PYG{n}{read}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} resample to 16kHz for better visualization}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{scipy}\PYG{o}{.}\PYG{n}{signal}\PYG{o}{.}\PYG{n}{resample}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{8000}\PYG{o}{/}\PYG{o}{/}\PYG{n}{fs}\PYG{p}{)}
\PYG{n}{fs} \PYG{o}{=} \PYG{l+m+mi}{16000}

\PYG{n}{ipd}\PYG{o}{.}\PYG{n}{display}\PYG{p}{(}\PYG{n}{ipd}\PYG{o}{.}\PYG{n}{Audio}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,}\PYG{n}{rate}\PYG{o}{=}\PYG{n}{fs}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} window parameters in milliseconds}
\PYG{n}{window\PYGZus{}length\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{30}
\PYG{n}{window\PYGZus{}step\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{5}

\PYG{n}{window\PYGZus{}step} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{fs}\PYG{o}{*}\PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{o}{/}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{window\PYGZus{}length} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{fs}\PYG{o}{*}\PYG{n}{window\PYGZus{}length\PYGZus{}ms}\PYG{o}{/}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{window\PYGZus{}count} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{floor}\PYG{p}{(}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{o}{/}\PYG{n}{window\PYGZus{}step}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} windowing function}
\PYG{n}{windowing\PYGZus{}fn} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{n}{window\PYGZus{}length}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{n}{num}\PYG{o}{=}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{o}{/}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{c+c1}{\PYGZsh{} Hann\PYGZhy{}window}

\PYG{n}{spectrogram\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{window\PYGZus{}length}\PYG{p}{,}\PYG{n}{window\PYGZus{}count}\PYG{p}{]}\PYG{p}{,}\PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{complex}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{window\PYGZus{}ix} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{window\PYGZus{}count}\PYG{p}{)}\PYG{p}{:}    
    \PYG{n}{data\PYGZus{}window} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{multiply}\PYG{p}{(}\PYG{n}{windowing\PYGZus{}fn}\PYG{p}{,}\PYG{n}{data}\PYG{p}{[}\PYG{n}{window\PYGZus{}ix}\PYG{o}{*}\PYG{n}{window\PYGZus{}step}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{window\PYGZus{}ix}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{fft}\PYG{o}{.}\PYG{n}{fft}\PYG{p}{(}\PYG{n}{data\PYGZus{}window}\PYG{p}{)}
    
\PYG{n}{fft\PYGZus{}length} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{p}{(}\PYG{n}{window\PYGZus{}length}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{t} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mf}{0.}\PYG{p}{,}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mf}{1.}\PYG{p}{)}\PYG{o}{/}\PYG{n}{fs}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{311}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,}\PYG{n}{data}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (s)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Amplitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Speech waveform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{312}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log10}\PYG{p}{(}\PYG{l+m+mf}{0.2}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{[}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{fft\PYGZus{}length}\PYG{p}{)}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{n}{origin}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{aspect}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{extent}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{o}{/}\PYG{n}{fs}\PYG{p}{,}\PYG{l+m+mf}{0.}\PYG{p}{,}\PYG{n}{fs}\PYG{o}{/}\PYG{l+m+mi}{2000}\PYG{p}{]}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}plt.xticks(np.arange(0.,len(data)/fs,0.5));}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (s)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}plt.yticks(np.arange(0,fft\PYGZus{}length,fft\PYGZus{}length*10000/fs));}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency (10kHz)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Speech spectrogram}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{313}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log10}\PYG{p}{(}\PYG{l+m+mf}{0.2}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{[}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{fft\PYGZus{}length}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{n}{origin}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{aspect}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{extent}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.}\PYG{p}{,}\PYG{l+m+mf}{.5}\PYG{o}{*}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{o}{/}\PYG{n}{fs}\PYG{p}{,}\PYG{l+m+mf}{0.}\PYG{p}{,}\PYG{n}{fs}\PYG{o}{/}\PYG{l+m+mi}{2000}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (s)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}plt.yticks(np.arange(0,fft\PYGZus{}length,fft\PYGZus{}length*10000/fs));}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency (10kHz)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Speech spectrogram zoomed in to lower frequencies}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Spectrogram_and_the_STFT_4_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
When looking at speech in a spectrogram, many important features of the
signal can be clearly observed:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Horizontal lines in a comb\sphinxhyphen{}structure correspond to the fundamental
frequency.

\item {} 
\sphinxAtStartPar
Vertical lines correspond to abrupt sounds, which are often
characterized as transients. Typical transients in speech are stop
consonants.

\item {} 
\sphinxAtStartPar
Areas which have a lot of energy in the high frequencies (appears as
a lighter colour), correspond to noisy sounds like fricatives.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{ipywidgets} \PYG{k+kn}{import} \PYG{o}{*}
\PYG{k+kn}{import} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k}{as} \PYG{n+nn}{ipd}
\PYG{k+kn}{from} \PYG{n+nn}{ipywidgets} \PYG{k+kn}{import} \PYG{n}{interactive}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{scipy}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{io} \PYG{k+kn}{import} \PYG{n}{wavfile}
\PYG{k+kn}{from} \PYG{n+nn}{scipy} \PYG{k+kn}{import} \PYG{n}{signal}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{n}{filename} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sounds/test.wav}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{fs}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{wavfile}\PYG{o}{.}\PYG{n}{read}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{int16}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}
\PYG{n}{data\PYGZus{}length} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{update}\PYG{p}{(}\PYG{n}{window\PYGZus{}length\PYGZus{}ms}\PYG{o}{=}\PYG{l+m+mf}{32.0}\PYG{p}{,} \PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{o}{=}\PYG{l+m+mf}{16.0}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ipd}\PYG{o}{.}\PYG{n}{clear\PYGZus{}output}\PYG{p}{(}\PYG{n}{wait}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{window\PYGZus{}length} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{window\PYGZus{}length\PYGZus{}ms}\PYG{o}{*}\PYG{n}{fs}\PYG{o}{/}\PYG{l+m+mf}{1000.}\PYG{p}{)}
    \PYG{n}{window\PYGZus{}step} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{o}{*}\PYG{n}{fs}\PYG{o}{/}\PYG{l+m+mf}{1000.}\PYG{p}{)}
    \PYG{n}{window\PYGZus{}count} \PYG{o}{=} \PYG{p}{(}\PYG{n}{data\PYGZus{}length} \PYG{o}{\PYGZhy{}} \PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{o}{/}\PYG{o}{/}\PYG{n}{window\PYGZus{}step} \PYG{o}{+} \PYG{l+m+mi}{1}
    \PYG{n}{dft\PYGZus{}length} \PYG{o}{=} \PYG{p}{(}\PYG{n}{window\PYGZus{}length} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}
    
    \PYG{c+c1}{\PYGZsh{} sine window}
    \PYG{n}{window\PYGZus{}function} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mf}{.5}\PYG{o}{/}\PYG{n}{window\PYGZus{}length}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} 
       
    \PYG{n}{windows} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{window\PYGZus{}length}\PYG{p}{,} \PYG{n}{window\PYGZus{}count}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{window\PYGZus{}count}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{ix} \PYG{o}{=} \PYG{n}{k}\PYG{o}{*}\PYG{n}{window\PYGZus{}step} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{window\PYGZus{}length}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
        \PYG{n}{windows}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{k}\PYG{p}{]} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{ix}\PYG{p}{]}\PYG{o}{*}\PYG{n}{window\PYGZus{}function}

    \PYG{n}{X} \PYG{o}{=} \PYG{n}{scipy}\PYG{o}{.}\PYG{n}{fft}\PYG{o}{.}\PYG{n}{rfft}\PYG{p}{(}\PYG{n}{windows}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{window\PYGZus{}length}\PYG{p}{)}
    
    \PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{l+m+mf}{20.}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log10}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{n}{origin}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{auto}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}    
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{fig}\PYG{o}{.}\PYG{n}{canvas}\PYG{o}{.}\PYG{n}{draw}\PYG{p}{(}\PYG{p}{)}
        
    


\PYG{n}{interactive\PYGZus{}plot} \PYG{o}{=} \PYG{n}{interactive}\PYG{p}{(}\PYG{n}{update}\PYG{p}{,} \PYG{n}{window\PYGZus{}length\PYGZus{}ms}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.0}\PYG{p}{,} \PYG{l+m+mf}{300.0}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{1.}\PYG{p}{,}\PYG{l+m+mf}{50.}\PYG{p}{,}\PYG{l+m+mf}{0.01}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{output} \PYG{o}{=} \PYG{n}{interactive\PYGZus{}plot}\PYG{o}{.}\PYG{n}{children}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{output}\PYG{o}{.}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{height} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{350px}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{interactive\PYGZus{}plot}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatSlider(value=32.0, description=\PYGZsq{}window\PYGZus{}length\PYGZus{}ms\PYGZsq{}, max=300.0, min=2.0, step=0.5), F…
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope


\section{Autocorrelation and autocovariance}
\label{\detokenize{Representations/Autocorrelation_and_autocovariance:autocorrelation-and-autocovariance}}\label{\detokenize{Representations/Autocorrelation_and_autocovariance::doc}}
\sphinxAtStartPar
Look at the speech signal segment below. On a large scale it is
hard to discern a structure, but on a small scale, the signal seems
continuous. Speech signals typically have such structure that samples
near in time to each other are similar in amplitude. Such structure is
often called short\sphinxhyphen{}term temporal structure.

\sphinxAtStartPar
More specifically, samples of the signal are \sphinxstyleemphasis{correlated} with the
preceding and following samples. Such structures are in statistics
measured by covariance and correlation, defined for zero\sphinxhyphen{}mean variables
x and y as
\begin{equation*}
\begin{split} \begin{split} \text{covariance: } & \sigma_{xy} = E[xy] \\
\text{correlation: } & \rho_{xy} =
\frac{E[xy]}{\sqrt{E[x^2]E[y^2]}}, \end{split} \end{split}
\end{equation*}
\sphinxAtStartPar
where \(E[ \cdot ]\) is the expectation operator.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Autocorrelation_and_autocovariance_1_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
For a speech signal \(x_{n}\), where \(n\) is the time\sphinxhyphen{}index, we
would like to measure the correlation between two
time\sphinxhyphen{}indices \(x_{n}\) and \(x_{h}\). Since the structure
which we are interested in appears when \(n\) and \(h\) are near each other,
it is better to measure the correlation
between \(x_{n}\) and \(x_{n-k}\). The scalar \(k\) is known
as the \sphinxstyleemphasis{lag}. Furthermore, we can assume that the correlation is uniform
over all \(n\) within the segment. The self\sphinxhyphen{}correlation and \sphinxhyphen{}covariances,
known as the \sphinxstyleemphasis{autocorrelation} and \sphinxstyleemphasis{autocovariance} are defined as
\begin{equation*}
\begin{split} \begin{split} \text{autocovariance: } & r_{k} =
E_n[x_nx_{n-k}] \\ \text{autocorrelation: } & c_{k} =
\frac{E_n[x_nx_{n-k}]}{E_n[x_n^2]} = \frac{r_k}{r_0}.
\end{split} \end{split}
\end{equation*}
\sphinxAtStartPar
The figure below illustrates the autocovariance of the above
speech signal. We can immediately see that the short\sphinxhyphen{}time correlations
are preserved \sphinxhyphen{} on a small scale, the autocovariance looks similar to
the original speech signal. The oscillating structure is also accurately
preserved.

\sphinxAtStartPar
Because we assume that the signal is stationary, and as a consequence of
the above formulations, we can readily see that autocovarinaces and
\sphinxhyphen{}correlations are symmetric
\begin{equation*}
\begin{split} r_k = E_n[x_nx_{n-k}] = E_n[x_{n+k}x_{n+k-k}] =
E_n[x_{n+k}x_{n}] = r_{-k}. \end{split}
\end{equation*}
\sphinxAtStartPar
This symmetry is clearly visible in the figure below, where the
curve is mirrored around lag 0.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Autocorrelation_and_autocovariance_3_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The above formulas use the expectation operator \(E[\cdot ]\) to define the
autocovariance and \sphinxhyphen{}correlation. It is an abstract tool, which needs to
be replaced by a proper estimator for practical implementations.
Specifically, to estimate the autocovariance from a segment of length
\(N\), we use
\begin{equation*}
\begin{split} r_k \approx \frac1{N-1} \sum_{k=1}^{N-1} x_n x_{n-k}. \end{split}
\end{equation*}
\sphinxAtStartPar
Observe that the speech signal \(x_{n}\) has to be
{\hyperref[\detokenize{Representations/Windowing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{windowed}}}} before using the above formula.

\sphinxAtStartPar
We can also make an on\sphinxhyphen{}line estimate (a.k.a. leaky integrator) of the autocovariance for sample
position \(n\) with lag \(k\) as
\begin{equation*}
\begin{split} \hat r_k(n) := \alpha x_n x_{n-k} + (1-\alpha) \hat r_k(n-1),
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\alpha\in[0,1]\) is a small positive constant which determines how rapidly the
estimate converges.

\sphinxAtStartPar
It is often easier to work with vector notation instead of scalars,
whereby we need the corresponding definitions for autocovariances.
Suppose
\begin{equation*}
\begin{split} x =
\begin{bmatrix}x_0\\x_1\\\vdots\\x_{N-1}\end{bmatrix}. \end{split}
\end{equation*}
\sphinxAtStartPar
We can then define the autocovariance matrix as
\begin{equation*}
\begin{split} R_x := E[x x^T] = \begin{bmatrix}E[x_0^2] & E[x_0x_1] &
\dots & E[x_0x_{N-1}]\\E[x_1x_0] & E[x_1^2] & \dots &
E[x_1x_{N-1}]\\\vdots&\vdots&\ddots&\vdots\\E[x_{N-1}x_0]
& E[x_{N-1}x_1] & \dots & E[x_{N-1}^2]\end{bmatrix} =
\begin{bmatrix}r_0 & r_1 & \dots & r_{N-1}\\ r_1 & r_0 & \dots &
r_{N-2}\\\vdots&\vdots&\ddots&\vdots\\r_{N-1} & r_{N-1} &
\dots & r_0\end{bmatrix}. \end{split}
\end{equation*}
\sphinxAtStartPar
Clearly \(R_{x}\) is thus a symmetric
\sphinxhref{https://en.wikipedia.org/wiki/Toeplitz\_matrix}{Toeplitz} matrix.
Moreover, since it is a product of \(x\) with itself, \(R_{x}\) is
also \sphinxhref{https://en.wikipedia.org/wiki/Definiteness\_of\_a\_matrix}{positive
(semi\sphinxhyphen{})definite}.

\sphinxstepscope


\section{The cepstrum, mel\sphinxhyphen{}cepstrum and mel\sphinxhyphen{}frequency cepstral coefficients (MFCCs)}
\label{\detokenize{Representations/Melcepstrum:the-cepstrum-mel-cepstrum-and-mel-frequency-cepstral-coefficients-mfccs}}\label{\detokenize{Representations/Melcepstrum::doc}}
\sphinxAtStartPar
The spectrogram is a useful representation of speech in the sense that it visualizes effectively many pertinent features of speech signals. In particular, we can observe events over time, changes in fundamental frequency and also some features of the spectral envelope. It however also comes with its drawbacks. It is not a particularly efficient representation in terms of number of coefficients; the spectrum has a large number of coefficients in comparison to the amout of information which we are after. Typically we would like to have information of the formant locations and amplitudes, which could be represented by just a handful of coefficients. Similarly, the fundamental frequency is just one piece of information, but it is hidden in a multitude of frequency components.


\subsection{The cepstrum}
\label{\detokenize{Representations/Melcepstrum:the-cepstrum}}
\sphinxAtStartPar
Many central properties of speech signals are clearly visible as structures in the log\sphinxhyphen{}spectrum, for example, the envelope is the smooth macro\sphinxhyphen{}structure and the fundamental frequency \(F_0\) is a harmonic comb\sphinxhyphen{}structure. Both types of structures can be easily analyzed with a time\sphinxhyphen{}frequency transform. Specifically, since the structures are visible in the log\sphinxhyphen{}spectrum, we should take the discrete Fourier transform or discrete cosine transform of the log\sphinxhyphen{}spectrum. We are thus applying two time\sphinxhyphen{}frequency transform to the original time signal, but with the non\sphinxhyphen{}linear operation, logarithm of the absolute value, in between.

\sphinxAtStartPar
The algorithm is
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Apply analysis windowing to signal

\item {} 
\sphinxAtStartPar
Apply time\sphinxhyphen{}frequency transform (DFT or DCT)

\item {} 
\sphinxAtStartPar
Take the logarithm of the absolute value

\item {} 
\sphinxAtStartPar
Apply second time\sphinxhyphen{}frequency transform

\end{enumerate}

\sphinxAtStartPar
The output after the second time\sphinxhyphen{}frequency transform is known as the \sphinxstyleemphasis{cepstrum}, because it is backwards transform of the \sphinxstyleemphasis{spectrum}. The word\sphinxhyphen{}play was chosen such that it would be funny. Similarly, the x\sphinxhyphen{}axis in the cepstrum is known as the \sphinxstyleemphasis{quefrency}, as corresponding to \sphinxstyleemphasis{frequency}. Since the cepstrum is a result of two consecutive time\sphinxhyphen{}frequncy transforms, the unit in the quefrency will be \sphinxstyleemphasis{seconds}.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}
\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Melcepstrum_3_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Melcepstrum_4_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Features in the Cepstrum}
\label{\detokenize{Representations/Melcepstrum:features-in-the-cepstrum}}
\sphinxAtStartPar
The envelope of the spectrum is a smoothed version, so it should be present in the low part of the cepstrum. The interpretation of the lowest coefficients is however not intuitive. Moreover, observe that the power spectrum can sometimes be zero or very close to zero, such that the log\sphinxhyphen{}spectrum approaches negative infinity. Isolated zeros should not have any effect on the envelope, but in the log\sphinxhyphen{}domain, very large negative values have a large contribution to the cepstrum. The cepstrum, as such, is therefore not very well suited for envelope modelling.

\sphinxAtStartPar
However, the F0 is usually prominently visible as a peak in the cepstrum. Since this domain is similar to the time\sphinxhyphen{}domain, the cepstral peak will be visible at the same quefrency value as the period\sphinxhyphen{}length of the original time signal.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Melcepstrum_6_0}.png}

\noindent\sphinxincludegraphics{{Melcepstrum_6_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We can clearly see the a curve between quefrency 5 and 12 ms undulating up and down over time.
The quefrencies \(q\) can be easily converted to frequencies \(f\) by \(f=1/q\) (but remember to first convert milliseconds to seconds).


\subsection{Down\sphinxhyphen{}sampling the log\sphinxhyphen{}spectrum}
\label{\detokenize{Representations/Melcepstrum:down-sampling-the-log-spectrum}}
\sphinxAtStartPar
The cepstrum is good for extracting envelope and F0\sphinxhyphen{}information, but it is not particularly efficient in the sense that it has a large number of coefficients for a little amount of information. The envelope information is about the slowly\sphinxhyphen{}varying shape of the log\sphinxhyphen{}spectrum, so we could try to extract that by a simple downsampling. However, a problem is that the power\sphinxhyphen{}spectrum can sometimes have arbitrarily small values, which in the log\sphinxhyphen{}spectrum translate to negative near\sphinxhyphen{}infinite values. Any information extraction in the log\sphinxhyphen{}domain would therefore be susceptible for bias to negative infinite.

\sphinxAtStartPar
A solution is to apply smoothing in the power\sphinxhyphen{}spectrum. For example, we could use a FIR\sphinxhyphen{}filter \([0.5,1,0.5]\), or more generally, a triangular shape. In heuristic terms, we just take an average of the power around a certain frequency, such that frequencies near have a larger weight than frequencies far away. The weighting parameters are then chosen in a triangular shape. We calculate the smoothed samples at intervals corresponding to half the width of the triangle, such that the amount of samples obtained corresponds to the amount of smoothing. More rigorously, we can first apply the FIR\sphinxhyphen{}filter and then apply down\sphinxhyphen{}sampling by an appropriate amount, but the end\sphinxhyphen{}result is the same.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Melcepstrum_9_0}.png}

\noindent\sphinxincludegraphics{{Melcepstrum_9_1}.png}

\noindent\sphinxincludegraphics{{Melcepstrum_9_2}.png}

\noindent\sphinxincludegraphics{{Melcepstrum_9_3}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The smoothed representation clearly catches the overall shape of the spectrum, which is the envelope. It achieves this with a low number of coefficient, which means that it is a reasonably efficient model of the envelope. However, a downside is that the information does not reflect the importance of features for humans. Taking the log\sphinxhyphen{}transform does map magnitudes to a perceptual scale, but the frequency scale is still not mapped to a perceptual domain.


\subsection{Mel\sphinxhyphen{}scale}
\label{\detokenize{Representations/Melcepstrum:mel-scale}}
\sphinxAtStartPar
The \sphinxhref{https://en.wikipedia.org/wiki/Mel\_scale}{mel\sphinxhyphen{}scale} is a scale which maps frequencies such that steps between tones align with our perception of steps. That is, for example, the step from X to X+1 mel sounds as large as the step from Y to Y+1 mel. More details in the above link.
We then form a filterbank such that the triangle\sphinxhyphen{}centres are at the frequencies corresponding to equal distance steps on the mel scale.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Melcepstrum_11_0}.png}

\noindent\sphinxincludegraphics{{Melcepstrum_11_1}.png}

\noindent\sphinxincludegraphics{{Melcepstrum_11_2}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The mel\sphinxhyphen{}envelope clearly models lower frequencies accurately, which is also where the all\sphinxhyphen{}important formants reside. That is, accuracy is concentrated on the important part, which is good. Higher frequencies, above 6.5 kHz in particular, are poorly modelled, but there is usually not too much energy anyway, so that is ok.

\sphinxAtStartPar
A remaining issue with the log\sphinxhyphen{}melspectrum is however that neighbouring samples are highly correlated. That is, information is distributed throughout the individual samples. Yet we cannot reduce accuracy more, because then we would start loosing accuracy of the formants.


\subsubsection{The Mel\sphinxhyphen{}Frequency Cepstral Coefficients (MFCCs)}
\label{\detokenize{Representations/Melcepstrum:the-mel-frequency-cepstral-coefficients-mfccs}}
\sphinxAtStartPar
A generic operation for decorrelating sequentially correlated data is the \sphinxhref{https://en.wikipedia.org/wiki/Discrete\_cosine\_transform}{discrete cosine transform (DCT)}. That is, say we have a time\sphinxhyphen{}signal which has correlation over time, by taking the DCT, we obtain the spectrum of the signal, where samples are reasonably uncorrelated (at least when the input vector is a stationary state system and long).

\sphinxAtStartPar
Similarly, we can thus take the DCT of the log\sphinxhyphen{}mel spectrum, which is known as the Mel\sphinxhyphen{}Frequency Cepstral coefficient (MFCC) representation. It has the mel\sphinxhyphen{}frequency mapping, then takes the logarithm and finally the DCT.

\sphinxAtStartPar
The MFCC is an abstract domain, which is not easy to interpret visually. However, since it is designed to correspond to resemble perception in both magnitude and frequency axis, and to be roughly uncorrelated, it is efficient for computation. Observe that this is a perceptual argument, such that the computer simulates how humans perform. Observe that simulating the human is not always appropriate, computers can sometimes do better than humans, but often it focuses attention to the relevant sources of information.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Melcepstrum_13_0}.png}

\noindent\sphinxincludegraphics{{Melcepstrum_13_1}.png}

\noindent\sphinxincludegraphics{{Melcepstrum_13_2}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The envelope is clearly visible in the log\sphinxhyphen{}mel spectrogram as expected. The MFCCs behave also as expected \sphinxhyphen{} not much is visible. Would need to normalize it over time to boost subtle changes to become visible. Such normalization (pre\sphinxhyphen{}whitening) is in fact a standard step for all variables which are used as input to machine learning.


\subsection{Conclusion}
\label{\detokenize{Representations/Melcepstrum:conclusion}}
\sphinxAtStartPar
We have discussed the cepstrum and how it can be used to extract F0 information from a speech signal. We also showed how to extract envelope information, in particular with frequencies mapped to the mel\sphinxhyphen{}scale to correspond with perception of pitch. Finally, we showed how the log\sphinxhyphen{}mel spectrum can be decorrelated to obtain the mel\sphinxhyphen{}frequency cepstral coefficients, the MFCCs. In fact, the main point of this whole exercise was to introduce the MFCCs, because it is the most used analysis method for all of speech and audio, especially as a front\sphinxhyphen{}end for machine learning methods. It is so common that if nothing is mentioned, then the assumption is that everyone will use MFCCs in machine learning.

\sphinxAtStartPar
The reason that MFCCs are used so often is that it works well. It is somehow capable of capturing essential information from speech and audio, even if we do not entirely understand why. There are many particular choices, which could be changed, but which seem to have either no effect at all or reduces the usefulness of the outcome. For example, the mel\sphinxhyphen{}scale is not a well\sphinxhyphen{}motivated choice. Pitch\sphinxhyphen{}perception is a particular thing, whereas the more generic perceptual models would be models of the hair\sphinxhyphen{}cells in the inner ear, approximated by, for example, the \sphinxhref{https://en.wikipedia.org/wiki/Equivalent\_rectangular\_bandwidth}{equivalent rectangular bandwidth (ERB) scale}. The MFCCs also have a historical momentum; they have been used so long already, that we would need a very good reason to move to some other model. There is benefit from having a model which everyone are familiar with.

\sphinxstepscope


\section{Linear prediction}
\label{\detokenize{Representations/Linear_prediction:linear-prediction}}\label{\detokenize{Representations/Linear_prediction::doc}}

\subsection{Definition}
\label{\detokenize{Representations/Linear_prediction:definition}}
\sphinxAtStartPar
Speech is a continuous signal, which means that consecutive samples of
the signal are correlated (see figure on the right). In particular, if
we know a previous sample \(x_{n-1}\), we can make a \sphinxstyleemphasis{prediction}
of the current sample, \( \hat x_n = x_{n-1}, \) such that \( \hat
x_n \approx x_n. \) By using more previous samples we have more
information, which should help us make a better prediction.
Specifically, we can define a predictor which uses \(M\) previous samples
to predict the current sample \(x_{n }\) as
\begin{equation*}
\begin{split} \hat x_n = - \sum_{k=1}^M a_k x_{n-k}. \end{split}
\end{equation*}
\sphinxAtStartPar
This is a \sphinxstyleemphasis{linear predictor} because it takes a linearly weighted sum of
past components to predict the current one.

\sphinxAtStartPar
The error of the prediction, also known as the \sphinxstyleemphasis{prediction residual} is
\begin{equation*}
\begin{split} e_n = x_n - \hat x_n = x_n + \sum_{k=1}^M a_k x_{n-k} =
\sum_{k=0}^M a_k x_{n-k}, \end{split}
\end{equation*}
\sphinxAtStartPar
where \(a_{0}=1\). This explains why the definition \( \hat x_n
\) included a minus sign; when we calculate the residual, the double
negative disappears and we can collate everything into one summation.

\sphinxAtStartPar
A short segment of speech. Notice how consecutive samples are mostly
near each other, which means that consecutive samples are correlated.

\sphinxAtStartPar
\sphinxincludegraphics{{149884842}.png}


\subsection{Vector notation}
\label{\detokenize{Representations/Linear_prediction:vector-notation}}
\sphinxAtStartPar
Using vector notation, we can make the expressions more compact
\begin{equation*}
\begin{split} e = Xa \end{split}
\end{equation*}
\sphinxAtStartPar
where
\begin{equation*}
\begin{split} e =
\begin{bmatrix}e_0\\e_1\\\vdots\\e_{N-1}\end{bmatrix},\qquad
X = \begin{bmatrix}x_0 & x_{-1} & \dots & x_{M} \\x_1 & x_0 &
\dots & x_{M-1} \\ \vdots & \vdots & & \vdots \\ x_{N-1} &
x_{N-2} & \dots & x_{N-M} \end{bmatrix}, \qquad a =
\begin{bmatrix}a_0\\a_1\\\vdots\\a_{M}\end{bmatrix}. \end{split}
\end{equation*}
\sphinxAtStartPar
Here we calculated the residual for a length \(N\) frame of the signal.


\subsection{Parameter estimation}
\label{\detokenize{Representations/Linear_prediction:parameter-estimation}}
\sphinxAtStartPar
Vector \(a\) holds the unknown coefficients of the predictor. To find the
best possible predictor, we can minimize the minimum mean\sphinxhyphen{}square error
(MMSE). The square error is the 2\sphinxhyphen{}norm of the residual, \(
\|e\|^2=e^T e \) . The mean of that error is defined as the
expectation
\begin{equation*}
\begin{split} E\left[|e|^2\right] = E\left[a^T X^T X a\right] = a^T
E\left[X^T X\right] a = a^T R_x a, \end{split}
\end{equation*}
\sphinxAtStartPar
where \( R_x = E\left[X^T X\right] \) and \(
E\left[\cdot\right] \) is the expectation operator. Note that, as
shown in the {\hyperref[\detokenize{Representations/Autocorrelation_and_autocovariance::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{autocorrelation
section}}}}, the matrix
\(R_{x}\), can be usually assumed to have a symmetric
\sphinxhref{https://en.wikipedia.org/wiki/Toeplitz\_matrix}{Toeplitz} structure.

\sphinxAtStartPar
If we would directly minimize the mean\sphinxhyphen{}square error  \(
E\left[\|e\|^2\right], \) then clearly we would obtain the
trivial solution \(a=0\), which is not particularly useful. However that
solution contradicts with the requirement that the first coefficient is
unity, \(a_{0}=1\). In vector notation we can equivalently write
\begin{equation*}
\begin{split} a_0-1=u^T a -1=0,
\qquad\text{where}\,u=\begin{bmatrix}1\\0\\0\\\vdots\\0\end{bmatrix}.
\end{split}
\end{equation*}
\sphinxAtStartPar
The standard method for quadratic minimization with constraints is to
use a \sphinxhref{https://en.wikipedia.org/wiki/Lagrange\_multiplier}{Langrange
multiplier}, λ, such
that the objective function is
\begin{equation*}
\begin{split} \eta(a,\lambda) = a^T R_x a - 2\lambda\left(a^T u - 1\right).
\end{split}
\end{equation*}
\sphinxAtStartPar
This function can be heuristically interpreted such that λ is a free
parameter. Since our objective is to minimize \( a^T R_x a \) if   \(
a^T u - 1 \) is non\sphinxhyphen{}zero, then the objective function can become
arbitrarily large. To allow any value for λ, the constraint must
therefore be zero.

\sphinxAtStartPar
The objective function is then minimized by setting its derivative with
respect to \(a\) to zero
\begin{equation*}
\begin{split} 0 = \frac\partial{\partial a}\eta(a,\lambda) =
\frac\partial{\partial a} \left[a^T R_x a -2\lambda\left(a^T u -
1\right)\right] = 2 R_x a - 2 \lambda u. \end{split}
\end{equation*}
\sphinxAtStartPar
It follows that the optimal predictor coefficients are found by solving
\begin{equation*}
\begin{split} R_x a = \lambda u. \end{split}
\end{equation*}
\sphinxAtStartPar
Since \(R_{x}\), is symmetric and
\sphinxhref{https://en.wikipedia.org/wiki/Toeplitz\_matrix}{Toeplitz}, the above
system of equations can be efficiently solved using the \sphinxhref{https://en.wikipedia.org/wiki/Levinson\_recursion}{Levinson\sphinxhyphen{}Durbin
algorithm} with
algorithmic complexity \(O(M^{2})\). However, note that with
direct solution we obtain \( a':=\frac1\lambda a = R_x^{-1}u \) that
is, instead of \(a\) we get \(a\) scaled with λ. However, since we know that
\(a_{0}=1\), we can find \(a\) by \( a=\lambda a' =
\frac{a'}{a'_0}. \)


\subsection{Spectral properties}
\label{\detokenize{Representations/Linear_prediction:spectral-properties}}
\sphinxAtStartPar
Linear prediction is usually used to predict the current sample of a
time\sphinxhyphen{}domain signal \(x_{n}\). The usefulness of linear prediction
however becomes evident by studying its Fourier spectrum Specifically,
since \(e=Xa\), the corresponding Z\sphinxhyphen{}domain representation is
\begin{equation*}
\begin{split} E(z) = X(z)A(z)\qquad\Rightarrow\qquad X(z)=\frac{E(z)}{A(z)},
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(E(z)\), \(X(z)\), and \(A(z)\), are the Z\sphinxhyphen{}transforms of
\(e_{n}\), \(x_{n}\) and \(a_{n}\), respectively. The
residual \(E(z)\) is white\sphinxhyphen{}noise, whereby the inverse \(A(z)^{-1}\),
must follow the shape of \(X(z)\).

\sphinxAtStartPar
In other words, the linear predictor models the macro\sphinxhyphen{}shape or
\sphinxstyleemphasis{envelope} of the spectrum.


\subsection{Physiological interpretation and model order}
\label{\detokenize{Representations/Linear_prediction:physiological-interpretation-and-model-order}}
\sphinxAtStartPar
Linear prediction has a surprising connection with physical modelling of
\DUrole{xref,myst}{speech production}. Namely,
a linear predictive model is equivalent with a \sphinxstyleemphasis{tube\sphinxhyphen{}model of the vocal
tract} (see figure on the right). A useful consequence is that from the
acoustic properties of such a tube\sphinxhyphen{}model, we can derive a relationship
between the physical length of the vocal tract \(L\) and the number of
parameters \(M\) of the corresponding linear predictor as
\begin{equation*}
\begin{split} M = \frac{2f_sL}c, \end{split}
\end{equation*}
\sphinxAtStartPar
where \(f_{s}\) is the sampling frequency and \(c\) is the speed of
sound. With an air\sphinxhyphen{}temperature of 35 C, the speed of sound is
\(c\)=350m/s. The mean length of vocal tracts for females and males are
approximately 14.1 and 16.9 cm. We can then choose to
overestimate \(L\)=0.17m. At a sampling frequency of 16kHz, this gives 
\( M\approx 17 \) . The linear predictor will catch also features of
the glottal oscillation and lip radiation, such that a useful
approximation is \( M\approx
{\text{round}}\left(1.25\frac{f_s}{1000}\right) \) . For different
sampling rates we then get the number of parameters \(M\) as


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(f_{s}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
M
\\
\hline
\sphinxAtStartPar
8 kHz
&
\sphinxAtStartPar
10
\\
\hline
\sphinxAtStartPar
12.8 kHz
&
\sphinxAtStartPar
16
\\
\hline
\sphinxAtStartPar
16 kHz
&
\sphinxAtStartPar
20
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Observe however that even if a tube\sphinxhyphen{}model is equivalent with a linear
predictor, the relationship is non\sphinxhyphen{}linear and highly sensitive to small
errors. Moreover, when estimating linear predictive models from speech,
in addition to features of the vocal tract, we will also capture
features of glottal oscillation and lip\sphinxhyphen{}radiation It is therefore very
difficult to estimate meaningful tube\sphinxhyphen{}model parameters from speech. A
related sub\sphinxhyphen{}field of speech analysis is \DUrole{xref,myst}{glottal inverse
filtering}, which attempts to estimate the
glottal source from the acoustic signal. A necessary step in such
inverse filtering is to estimate the acoustic effect of the vocal tract,
that is, it is necessary to estimate the tube model.

\sphinxAtStartPar
A tube model of the vocal tract consisting of constant\sphinxhyphen{}radius
tube\sphinxhyphen{}segments

\sphinxAtStartPar
\sphinxincludegraphics{{1498892011}.png}


\subsection{Uses in speech coding}
\label{\detokenize{Representations/Linear_prediction:uses-in-speech-coding}}
\sphinxAtStartPar
Linear prediction has been highly influential especially in early speech
coders. In fact, the dominant speech coding method is \DUrole{xref,myst}{code\sphinxhyphen{}excited
linear prediction (CELP)}, which
is based on linear prediction.


\subsection{Alternative representations (advanced topic)}
\label{\detokenize{Representations/Linear_prediction:alternative-representations-advanced-topic}}
\sphinxAtStartPar
Suppose scalars \(a_{m,k}\), are the coefficients of an \(M\)th
order linear predictor. Coefficients of consecutive orders \(M\) and \(M+1\)
are then related as
\begin{equation*}
\begin{split} a_{M+1,k} = a_{M,k} + \gamma_{M+1} a_{M,M+1-k}, \end{split}
\end{equation*}
\sphinxAtStartPar
where the real valued scalar \( \gamma_{M}\in(-1,+1) \) is the
\(M\)th \sphinxhref{https://en.wikipedia.org/wiki/Reflection\_coefficient}{reflection
coefficient}. This
formulation is the basis for the \sphinxhref{https://en.wikipedia.org/wiki/Levinson\_recursion}{Levinson\sphinxhyphen{}Durbin
algorithm} which can
be used to solve the linear predictive coefficients. In a physical
sense, reflection coefficients describe the amount of the acoustic wave
which is reflected back in each junction of the tube\sphinxhyphen{}model. In other
words, there is a relationship between the \sphinxstyleemphasis{cross\sphinxhyphen{}sectional areas}
\(S_{k}\) of each tube\sphinxhyphen{}segment and the reflection coefficients as
\begin{equation*}
\begin{split} \gamma_k = \frac{S_k - S_{k+1}}{S_k + S_{k+1}}. \end{split}
\end{equation*}
\sphinxAtStartPar
Furthermore, the logarithmic ratio of cross\sphinxhyphen{}sectional areas, also known
as the \sphinxhref{https://en.wikipedia.org/wiki/Log\_area\_ratio}{\sphinxstyleemphasis{log\sphinxhyphen{}area
ratios}}, are defined as
\begin{equation*}
\begin{split} A_k = \log\frac{S_k}{S_{k+1}} =
\log\frac{1-\gamma_k}{1+\gamma_k}. \end{split}
\end{equation*}
\sphinxAtStartPar
This form has been used in coding of linear predictive models, but is
today mostly of historical interest.

\sphinxstepscope


\section{Fundamental frequency (F0)}
\label{\detokenize{Representations/Fundamental_frequency_F0:fundamental-frequency-f0}}\label{\detokenize{Representations/Fundamental_frequency_F0::doc}}
\sphinxAtStartPar
The fundamental frequency of a speech signal, often denoted by F0 or
\(F_{0}\), refers to the approximate frequency of the
(quasi\sphinxhyphen{})periodic structure of voiced speech signals. The oscillation
originates from the vocal folds, which oscillate in the airflow when
appropriately tensed. The fundamental frequency is defined as the
average number of oscillations per second and expressed in Hertz. Since
the oscillation originates from an organic structure, it is not exactly
periodic but contains significant fluctuations. In particular, amount of
variation in period length and amplitude are known respectively as
\sphinxstyleemphasis{jitter} and \sphinxstyleemphasis{shimmer}. Moreover, the F0 is typically not stationary,
but changes constantly within a sentence. In fact, the F0 can be used
for expressive purposes to signify, for example, emphasis and questions.

\sphinxAtStartPar
Typically fundamental frequencies lie roughly in the range \sphinxstyleemphasis{80} to \sphinxstyleemphasis{450
Hz}, where males have lower voices than females and children. The F0 of
an individual speaker depends primarily on the length of the vocal
folds, which is in turn correlated with overall body size. Cultural and
stylistic aspects of speech naturally have also a large impact.

\sphinxAtStartPar
The fundamental frequency is closely related to \sphinxstyleemphasis{pitch}, which is
defined as our perception of fundamental frequency. That is, the F0
describes the actual physical phenomenon, whereas pitch describes how
our ears and brains interpret the signal, in terms of periodicity. For
example, a voice signal could have an F0 of 100 Hz. If we then apply a
high\sphinxhyphen{}pass filter to remove all signal components below 450 Hz, then that
would remove the actual fundamental frequency. The lowest remaining
periodic component would be 500 Hz, which correspond to the fifth
harmonic of the original F0. However, a human listener would then
typically still perceive a pitch of 100 Hz, even if it does not exist
anymore. The brain somehow reconstructs the fundamental from the upper
harmonics. This well\sphinxhyphen{}known phenomenon is however still not completely
understood.

\sphinxAtStartPar
A speech signal with a fundamental frequency of approximately F0=93Hz.

\sphinxAtStartPar
\sphinxincludegraphics{{175515681}.png}

\sphinxAtStartPar
The spectrum of a speech signal with a fundamental frequency of
approximately F0=93Hz (original) and a high\sphinxhyphen{}pass filtered version of it
such that the fundamental frequency has been removed (high\sphinxhyphen{}pass
filtered).

\sphinxAtStartPar
\sphinxincludegraphics{{175515679}.png}

\sphinxAtStartPar
A speech signal with a fundamental frequency of approximately F0=93Hz

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
A high\sphinxhyphen{}pass
filtered version of it such that the fundamental frequency has been
removed

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
If \(F_{0}\) is the fundamental frequency, then the length of a
single period in seconds is
\begin{equation*}
\begin{split} T=\frac{1}{F_0}. \end{split}
\end{equation*}
\sphinxAtStartPar
The speech waveform thus repeats itself after every \(T\) seconds.

\sphinxAtStartPar
A simple way of modelling the fundamental frequency is to repeat the
signal after a delay of \(T\) seconds. If a signal is sampled with a
sampling rate of \(F_{s}\), then the signal repeats after a delay
of \(L\) samples where
\begin{equation*}
\begin{split} L = F_s T = \frac{F_s}{F_0}. \end{split}
\end{equation*}
\sphinxAtStartPar
A signal \(x_{n}\) then approximately repeats itself such that
\begin{equation*}
\begin{split} x_n \approx x\_{n-L} \approx x\_{n-2L} \approx x\_{n-3L}. \end{split}
\end{equation*}
\sphinxAtStartPar
In the Z\sphinxhyphen{}domain this can be modelled by an IIR\sphinxhyphen{}filter as
\begin{equation*}
\begin{split} B(z) = 1 - \gamma_L z^{-L}, \end{split}
\end{equation*}
\sphinxAtStartPar
where the scalar \( 0\leq\gamma_L\leq 1 \) scales with the accuracy
of the period. The Z\sphinxhyphen{}transform of the signal \(x_{n}\) can then be
written as \( X(z)=B^{-1}(z) E(z), \) where \(E(z)\) is the Z\sphinxhyphen{}transform
of a single period.

\sphinxAtStartPar
Segment of a speech signal, with the period length \(L\), and fundamental
frequency \(F_0=1/L\).
\sphinxincludegraphics{{149891410}.png}

\sphinxAtStartPar
Spectrum of speech signal with the fundamental frequency \(F_{0}\)
and harmonics \(kF_{0}\), as well as the
formants \sphinxstyleemphasis{F1}, \sphinxstyleemphasis{F2}, \sphinxstyleemphasis{F3}… Notice how the harmonics form a regular
comb\sphinxhyphen{}structure.

\sphinxAtStartPar
\sphinxincludegraphics{{175515678}.png}

\sphinxAtStartPar
The magnitude spectrum of \(B^{-1}(z)\), has then a periodic
comb\sphinxhyphen{}structure. That is, the magnitude spectrum has peaks at \( k\,F_0
\) , for integer \(k\).
For a discussion about the fundamental frequency in the cepstral
domain, see \DUrole{xref,myst}{Cepstrum and MFCC}.

\sphinxAtStartPar
Spectrum of fundamental frequency model \(B^{-1}(z)\), showing the
characteristic comb\sphinxhyphen{}structure with harmonic peaks appearing at integer
multiples of \(F0\).

\sphinxAtStartPar
\sphinxincludegraphics{{149891452}.png}

\sphinxstepscope


\section{Zero\sphinxhyphen{}crossing rate}
\label{\detokenize{Representations/Zero-crossing_rate:zero-crossing-rate}}\label{\detokenize{Representations/Zero-crossing_rate::doc}}
\sphinxAtStartPar
By looking at different speech and audio waveforms, we can see that
depending on the content, they vary a lot in their smoothness. For
example, voiced speech sounds are more smooth than unvoiced ones.
Smoothness is thus a informative characteristic of the signal.

\sphinxAtStartPar
A very simple way for measuring smoothness of a signal is to calculate
the number of zero\sphinxhyphen{}crossing within a segment of that signal. A voice
signal oscillates slowly \sphinxhyphen{} for example, a 100 Hz signal will cross zero
100 per second \sphinxhyphen{} whereas an unvoiced fricative can have 3000 zero
crossing per second. An implementation of the zero\sphinxhyphen{}crossing for a signal \(x_h\) at window \(k\) is
\begin{equation*}
\begin{split}
ZCR_k = \sum_{h=kM}^{kM+N} \left|{\mathrm{sign}}(x_h)-{\mathrm{sign}}(x_{h-1})\right|,
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(M\) is the step between analysis windows and \(N\) the analysis window length.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Zero-crossing_rate_1_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatSlider(value=1.4048072562358276, description=\PYGZsq{}position\PYGZus{}s\PYGZsq{}, layout=Layout(width=\PYGZsq{}760…
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
To calculate of the zero\sphinxhyphen{}crossing rate of a signal you need to compare
the sign of each pair of consecutive samples. In other words, for a
length \(N\) signal you need \(O(N)\) operations. Such calculations are also
extremely simple to implement, which makes the zero\sphinxhyphen{}crossing rate an
attractive measure for low\sphinxhyphen{}complexity applications. However, there are
also many drawbacks with the zero\sphinxhyphen{}crossing rate:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The number of zero\sphinxhyphen{}crossings in a segment is an integer number. A
continuous\sphinxhyphen{}valued measure would allow more detailed analysis.

\item {} 
\sphinxAtStartPar
Measure is applicable only on longer segments of the signal, since
short segments might not have any or just a few zero crossings.

\item {} 
\sphinxAtStartPar
To make the measure consistent, we must assume that the signal is
zero\sphinxhyphen{}mean. You should therefore subtract the mean of each segment
before calculating the zero\sphinxhyphen{}crossings rate.

\end{itemize}

\sphinxAtStartPar
An alternative to the zero\sphinxhyphen{}crossing rate is to calculate the
{\hyperref[\detokenize{Representations/Autocorrelation_and_autocovariance::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{autocorrelation}}}} at lag\sphinxhyphen{}1. It can
be estimated also from short segments, it is continuous\sphinxhyphen{}valued and
arithmetic complexity is also \(O(N)\).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Zero-crossing_rate_4_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxstepscope


\section{Deltas and Delta\sphinxhyphen{}deltas}
\label{\detokenize{Representations/Deltas_and_Delta-deltas:deltas-and-delta-deltas}}\label{\detokenize{Representations/Deltas_and_Delta-deltas::doc}}
\sphinxAtStartPar
In \DUrole{xref,myst}{recognition tasks}, such as
phoneme recognition or voice activity detection, a classic input feature
are the \DUrole{xref,myst}{mel\sphinxhyphen{}frequency cepstral coefficients}
(MFCCs). They describes the instantaneous, spectral envelope shape of
the speech signal. However, speech signals are time\sphinxhyphen{}variant signals and
in a constant flux. Though we describe speech in linguistics as
concatenated sequences of phonemes, the acoustical signal is more
accurately described as a sequence of transitions between phonemes.

\sphinxAtStartPar
The same observation applies to other features of speech like the
\DUrole{xref,myst}{fundamental frequency (F0)}, which describes
an instantaneous value. However, it is often more informative to analyse
the overall shape of the F0 track, than the absolute value. For example
emphasis in a sentence is often encoded with a distinct high\sphinxhyphen{}low
contrast in F0 and questions have in many languages a characteristic
low\sphinxhyphen{}high F0 contour.

\sphinxAtStartPar
A common method for extracting information about such transitions is to
determine the first difference of signal features, known as the \sphinxstyleemphasis{delta}
of a feature. Specifically, for a feature \(f_k\), at
time\sphinxhyphen{}instant \sphinxstyleemphasis{k}, the corresponding delta is defined as
\begin{equation*}
\begin{split} \Delta_k = f_k - f_{k-1}. \end{split}
\end{equation*}
\sphinxAtStartPar
The second difference, known as the delta\sphinxhyphen{}delta, is correspondingly
\begin{equation*}
\begin{split} \Delta\Delta_k = \Delta_k - \Delta_{k-1}. \end{split}
\end{equation*}
\sphinxAtStartPar
Common short\sphinxhyphen{}hand notations for the deltas and delta\sphinxhyphen{}deltas are,
respectively, \( \Delta \) and \( \Delta\Delta \) \sphinxhyphen{}features.
Features in a recognition engine are then typically appended by their
\( \Delta \) and \( \Delta\Delta \) \sphinxhyphen{}features to triple the
number of features with a very small computational overhead.

\sphinxAtStartPar
A trivial observation/interpretation of the delta and delta\sphinxhyphen{}delta
features is that they approximate first and second derivatives of the
signal. As estimates of the derivatives, they are not particularly
accurate, but their simplicity probably makes up for that. The issue
with accuracy is that differentiators tend to amplify white noise,
whereas the desired signal remains unchanged. Consequently, the output
is more noisy than the original signal. Differentiation is applied twice
in the delta\sphinxhyphen{}delta feature such that issues with noise are also
accumulated.

\sphinxAtStartPar
Note that the delta\sphinxhyphen{}features are linear transforms of the input
features, such that if they are combined with a linear layer in a
subsequent neural network, then in principle, the two consecutive linear
layers are redundant. However, using delta\sphinxhyphen{}features can still provide a
benefit in convergence.

\sphinxAtStartPar
In any case, delta\sphinxhyphen{} and delta\sphinxhyphen{}delta features are a classic component of
machine learning algorithms. They are successful because they are very
simple to calculate and provide often a clear benefit over the
instantaneous features.

\sphinxstepscope


\section{Pitch\sphinxhyphen{}Synchoronous Overlap\sphinxhyphen{}Add (PSOLA)}
\label{\detokenize{Representations/Pitch-Synchoronous_Overlap-Add_PSOLA:pitch-synchoronous-overlap-add-psola}}\label{\detokenize{Representations/Pitch-Synchoronous_Overlap-Add_PSOLA::doc}}
\sphinxAtStartPar
Many speech applications require the ability to modify the fundamental
frequency. For a classic but marginal application, think of the
\sphinxhref{https://en.wikipedia.org/wiki/Auto-Tune}{auto\sphinxhyphen{}tune} function often used
in post\sphinxhyphen{}processing of singing voices. With such tools it is possible to
change the fundamental frequency of a speaker’s or singer’s voice
without changing the phoneme or timbre of the sound. One of the more
popular tools developed for this purpose is pitch\sphinxhyphen{}synchronous
overlap\sphinxhyphen{}add (PSOLA). Like the name suggests, it is closely related to
the overlap\sphinxhyphen{}add method used in the {\hyperref[\detokenize{Representations/Spectrogram_and_the_STFT::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{short\sphinxhyphen{}time Fourier
transform}}}} algorithm. It allows changing the
pitch of a speech sound without modifying or with only minor influence
on other characteristics of the signal, such as vowel\sphinxhyphen{}identity. In
addition to auto\sphinxhyphen{}tune, an important application of PSOLA is \sphinxhref{https://en.wikipedia.org/wiki/Speech\_synthesis}{speech
synthesis}, where we
want to be able generate speech with any reasonable pitch contour. Voice
conversion is another application, where the objective is to convert the
speech of one person, such that it sounds like speech of another person.

\sphinxAtStartPar
Illustration of the PSOLA process; find period length and maximum peak
in each period.

\sphinxAtStartPar
\sphinxincludegraphics{{175514551}.png}

\sphinxAtStartPar
The basic idea of PSOLA is to decompose the signal into individual
pitch\sphinxhyphen{}periods, such that we can move the pitch\sphinxhyphen{}periods to change the
effective length of those periods. That is, the fundamental frequency of
a signal is expressed as a periodic structure of the time\sphinxhyphen{}signal. If we
cut the signal into segment corresponding to the length of such periodic
structures, then we can shift their positions as desired and then add
them back together, like in the overlap\sphinxhyphen{}add process (see STFT). Since
short\sphinxhyphen{}term correlations in the signal are not changed, that is, signal
inside the windows/segments is not changed, then the spectral envelope
of the signal is not changed.

\sphinxAtStartPar
PSOLA analysis windowing, time\sphinxhyphen{}shift and synthesis windowing

\sphinxAtStartPar
\sphinxincludegraphics{{175514552}.png}

\sphinxAtStartPar
To illustrate the principle, consider the following basic algorithm:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Estimate the fundamental frequency contour of a speech sample.

\item {} 
\sphinxAtStartPar
Find pitch periods of the speech sample, for example by identifying
the largest peak in each period.

\item {} 
\sphinxAtStartPar
Extract windows of the speech signal covering \sphinxstyleemphasis{two} pitch periods.
Apply a {\hyperref[\detokenize{Representations/Windowing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{windowing function}}}} with perfect reconstruction.
(Observe: Perfect reconstruction should apply for each period, so we
construct half\sphinxhyphen{}length windows for each period. Conversely, the left
and right parts of windows can be of different length.)

\item {} 
\sphinxAtStartPar
Shift windows to match the desired pitch\sphinxhyphen{}period length.

\end{enumerate}

\sphinxAtStartPar
In the sound examples on the right, the pitch period lengths are
adjusted by a fixed multiplier to increase or decrease the fundamental
frequency. Observe that the implementation is not perfectly tuned such
that the output sound has some audible distortions.

\sphinxAtStartPar
Sound examples with varying multiplier on distance between pitch periods

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Multiplier 0.6
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Multiplier 0.8
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Multiplier 0.9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Multiplier 1.0 (original)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Multiplier 1.1
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Multiplier 1.2
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Multiplier 1.4
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxstepscope


\section{Jitter and shimmer}
\label{\detokenize{Representations/Jitter_and_shimmer:jitter-and-shimmer}}\label{\detokenize{Representations/Jitter_and_shimmer::doc}}
\sphinxAtStartPar
The speech production system is not a rigid, mechanical machine, but
composed of an assortment of soft\sphinxhyphen{}tissue components. Therefore, although
parts of a speech signal might seem stationary, there are always small
fluctuations in it, as vocal fold oscillation is not exactly periodic.
Variations in signal frequency and amplitude are called jitter and
shimmer, respectively. Jitter and shimmer are acoustic characteristics
of voice signals, and they are caused by irregular vocal fold vibration.
They are perceived as roughness, breathiness, or hoarseness in a
speaker’s voice. All natural speech contains some level of jitter and
shimmer, but measuring them is a common way to detect voice pathologies.
Personal habits such as smoking or alcohol consumption might increase
the level of jitter and shimmer in voice. However, many other factors
can have an effect as well, such as loudness of voice, language, or
gender. As jitter and shimmer represent individual voice characteristics
that humans might use to recognize familiar voices, these measures could
even be useful for speaker recognition systems.

\sphinxAtStartPar
There are several different ways to measure jitter and shimmer. For
instance, when detecting voice disorders, they are measured as
percentages of the average period, where values above certain thresholds
are potentially related to pathological voices. Jitter and shimmer are
most clearly detected from long, sustained vowels.

\sphinxAtStartPar
A commonly used jitter value is the absolute jitter. This measure
expresses the average absolute difference between consecutive periods.
\begin{equation*}
\begin{split} Jitter(absolute) = \frac{1}{N-1}\sum_{i=1}^{N-1}\|T_i-T_{i+1}\|
\end{split}
\end{equation*}
\sphinxAtStartPar
where Ti are the extracted F0 period lengths and N is the number of
extracted F0 periods.

\sphinxAtStartPar
When this is divided by the average period, another common measure,
relative jitter, is obtained.
\begin{equation*}
\begin{split} Jitter(relative) =
\frac{\frac{1}{N-1}\sum_{i=1}^{N-1}\|T_i-T_{i+1}\|}{\frac{1}{N}\sum_{i=1}^{N}T_i}
\end{split}
\end{equation*}
\sphinxAtStartPar
where Ti are the extracted F0 period lengths and N is the number of
extracted F0 periods.

\sphinxAtStartPar
A commonly used shimmer value, here Shimmer(dB), expresses the average
absolute base\sphinxhyphen{}10 logarithm of the difference between the amplitudes of
consecutive periods multiplied by 20.
\begin{equation*}
\begin{split} Shimmer(dB) =
\frac{1}{N-1}\sum_{i=1}^{N-1}\|20\log(A_{i+1}/A_i)\| \end{split}
\end{equation*}
\sphinxAtStartPar
where Ai are the extracted peak\sphinxhyphen{}to\sphinxhyphen{}peak amplitude data and N is the
number of extracted fundamental frequency periods.

\sphinxAtStartPar
Relative shimmer expresses the average absolute difference between the
amplitudes of consecutive periods divided by the average amplitude.
\begin{equation*}
\begin{split} Shimmer(relative) =
\frac{\frac{1}{N-1}\sum_{i=1}^{N-1}\|A_i-A_{i+1}\|}{\frac{1}{N}\sum_{i=1}^{N}A_i}
\end{split}
\end{equation*}
\sphinxAtStartPar
where Ai are the extracted peak\sphinxhyphen{}to\sphinxhyphen{}peak amplitude data and N is the
number of extracted fundamental frequency periods.

\sphinxstepscope


\chapter{Pre\sphinxhyphen{}processing}
\label{\detokenize{Pre-processing:pre-processing}}\label{\detokenize{Pre-processing::doc}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Preprocessing/Pre-emphasis::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Pre\sphinxhyphen{}emphasis}}}}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Noise\_gate}{Noise gate} (Wikipedia)

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Dynamic\_range\_compression}{Dynamic Range Compression} (Wikipedia)

\item {} 
\sphinxAtStartPar
\DUrole{xref,myst}{Voice activity detection (VAD)}

\item {} 
\sphinxAtStartPar
Vocal tract length normalization

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Speech_enhancement::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Speech enhancement}}}}

\end{enumerate}

\sphinxstepscope


\section{Pre\sphinxhyphen{}emphasis}
\label{\detokenize{Preprocessing/Pre-emphasis:pre-emphasis}}\label{\detokenize{Preprocessing/Pre-emphasis::doc}}
\sphinxAtStartPar
\sphinxincludegraphics{{149888832}.png}

\sphinxAtStartPar
The figure above illustrates the average magnitude spectrum of a
speech signal. We observe that a majority of the energy is concentrated
in the lower end of the spectrum. In fact, as a linear approximation, we
see that in this particular example, the energy drops at a rate of 2.2
dB/kHz. The exact rate of decrease varies for each speaker and depending
on several factors. A safe and often used assumption is that energy
drops at roughly 2 dB/kHz.

\sphinxAtStartPar
This rapid reduction in energy leads to practical problems in
implementations. For example, if we would implement a discrete Fourier
transform with \sphinxhref{https://en.wikipedia.org/wiki/Fixed-point\_arithmetic}{fixed\sphinxhyphen{}point
arithmetic}, then
the accuracy would be very different in different parts of the spectrum.
Typically the spectrum at 6kHz is 15dB lower than at 0Hz. On a linear
scale 15dB corresponds to a factor of 6. In other words, on a 16\sphinxhyphen{}bit
CPU, if we use the full range of a signed 15\sphinxhyphen{}bit representation for the
lowest frequencies, than we use effectively only 12\sphinxhyphen{}bit range for
frequency components at 6kHz.

\sphinxAtStartPar
A common pre\sphinxhyphen{}processing tool used to compensate for the average spectral
shape is \sphinxstyleemphasis{pre\sphinxhyphen{}emphasis}, which emphasises higher frequencies. Typically,
pre\sphinxhyphen{}emphasis is applied as a time\sphinxhyphen{}domain FIR filter with one free
parameter, for example, in speech coding at a sampling rate of 8kHz or
12.8kHz, we use the pre\sphinxhyphen{}emphasis filter \( P(z)=1-0.68 z^{-1} \) {[}\hyperlink{cite.Transmission/Modified_discrete_cosine_transform_MDCT:id43}{Bäckström \sphinxstyleemphasis{et al.}, 2017}{]}. The
spectrum of this filter is illustrated below. After applying the
filter, the spectrum is more flat and we can apply fixed\sphinxhyphen{}point
arithmetic with a lower accuracy and thus better optimize CPU
consumption.

\sphinxAtStartPar
There are numerous different ways of tuning pre\sphinxhyphen{}emphasis. Firstly,
though the average spectrum is decaying, unvoiced fricatives have
typically \sphinxstyleemphasis{more} energy at the high frequencies. Excessive pre\sphinxhyphen{}emphasis
would therefore cause problems for fricatives. Pre\sphinxhyphen{}emphasis also has an
effect on both perceptual and statistical modelling as well as
estimation of linear predictive models. The best amount of pre\sphinxhyphen{}emphasis
is therefore very much dependent on the application and implementation
details.

\sphinxAtStartPar
\sphinxincludegraphics{{149888831}.png}


\section{References}
\label{\detokenize{Preprocessing/Pre-emphasis:references}}
\sphinxstepscope


\chapter{Modelling tools in speech processing}
\label{\detokenize{Modelling_tools_in_speech_processing:modelling-tools-in-speech-processing}}\label{\detokenize{Modelling_tools_in_speech_processing::doc}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Modelling/Source_modelling_and_perceptual_modelling::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Source and perceptual modelling}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Modelling/Linear_regression::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Linear regression}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Modelling/Sub-space_models::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Sub\sphinxhyphen{}space models}}}}

\item {} 
\sphinxAtStartPar
\DUrole{xref,myst}{Vector quantization (VQ)}

\item {} 
\sphinxAtStartPar
\DUrole{xref,myst}{Gaussian mixture model (GMM)}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Modelling/Neural_networks::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Neural networks}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Modelling/Non-negative_Matrix_and_Tensor_Factorization::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Non\sphinxhyphen{}negative Matrix and Tensor Factorization}}}}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Hidden\_Markov\_model}{Hidden Markov Models} (Wikipedia)

\end{enumerate}

\sphinxstepscope


\section{Source modelling and perceptual modelling}
\label{\detokenize{Modelling/Source_modelling_and_perceptual_modelling:source-modelling-and-perceptual-modelling}}\label{\detokenize{Modelling/Source_modelling_and_perceptual_modelling::doc}}
\sphinxAtStartPar
Speech processing applications use predominantly two types of modelling,
models related to perception and to the source. They are always separate
models on a meta\sphinxhyphen{}level, where we describe the motivations why we use
them. In practical applications, they however have often complicated
interactions, such that it becomes difficult to separate them.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Source models} characterize the objective properties of a signal. A
source model can for example describe the statistical distribution
of speech signals and their characteristics. One such model would be
a model of the \DUrole{xref,myst}{fundamental frequency};
voiced signals have a fundamental frequency and we can specify the
range where fundamental frequencies of speech signals can lie. Other
obvious characteristics of speech signals we can model include the
intensity of speech and how it can change over time, as well as the
spectral envelope, what shapes are possible and how they can change
over time.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Perceptual models} characterize \sphinxstyleemphasis{what} human listeners \sphinxstyleemphasis{can hear}
and \sphinxstyleemphasis{how} much they appreciate different qualities. Perceptual
models thus try to predict how humans evaluate quality. To construct
perceptual models we need to ask human listeners questions like “Can
you hear distortions in this signal?” or “How much is this signal
distorted?”. Based on the responses of human listeners, we can then
make an analysis algorithm, which predicts the answer based on an
analysis of the input signal.

\end{itemize}

\sphinxAtStartPar
In other words, source models explain objectively “\sphinxstyleemphasis{what the world is
like}”, where as perceptual models are evaluation models estimate
subjective preference, “\sphinxstyleemphasis{how good the world is}”.

\sphinxAtStartPar
It is not however always so clear which model is active in which
situation. For example, consider a speech recognizer, to which we feed
speech with a loud background noise. We must decide whether the speech
recognizer would evaluate speech as a human would evaluate, or whether
we want to recognize speech as best we can. It is potentially possible
that the speech recognizer could tolerate noise better than humans, such
that it recognizes speech also when a human would fail. The model is
then objectively evaluating speech content. However, if we want for
example that a humanoid robot behaves like a human, then it should not
understand speech in situations where a human would not.

\sphinxAtStartPar
As another example, consider a noise attenuation task, where our
objective is to restore the original speech from a corrupted sample. We
can then remove noise and only evaluate objectively how close we are to
the original. This is an unambiguous objetive task, where perception
plays no role. More similar to the original is better. However, an
alternative approach is to consider all possible sounds \(y\), and given a
noisy observation \(v\), compute the likelihood of all possible inputs.
Suppose then that our output is \(x\), we can then compute the perceptual
distortion between all possible inputs \(d(x,y)\), and assign weighting to
them according to their likelihoods \(P(y|v)\). Finally, we can
minimize the expected distortion \(\min E[d(x,y) | P(y|v)].\) We thus
take into account all possible true inputs, calculate the perceptual
distortion between the true inputs and the output, and weight them
according to how likely they are. Now, for the same task as before, we
have a perceptual criterion with which we can choose the best output.
Clearly the latter is more complicated, but it is also better motivated,
so we have to choose which one we want to use. The first one is based on
source modelling only (model of speech and noise), while the latter is a
combination of source (likelihood of different speech signals, given a
noisy observation) and perceptual modelling (perceptual distortion
measure).

\sphinxstepscope


\section{Linear regression}
\label{\detokenize{Modelling/Linear_regression:linear-regression}}\label{\detokenize{Modelling/Linear_regression::doc}}

\subsection{Problem definition}
\label{\detokenize{Modelling/Linear_regression:problem-definition}}
\sphinxAtStartPar
In speech processing and elsewhere, a frequently appearing task is to
make a prediction of an unknown vector \sphinxstyleemphasis{y} from available observation
vectors \sphinxstyleemphasis{x}. Specifically, we want to have an estimate \( \hat y =
f(x) \) such that \( \hat y \approx y. \) In particular, we will
focus on \sphinxstyleemphasis{linear estimates} where \( \hat y=f(x):=A^T x, \) and where
\sphinxstyleemphasis{A} is a matrix of parameters. The figure on the right illustrates a
linear model, where the input sample pairs \sphinxstyleemphasis{(x,y)} are modelled by a
linear model  \( \hat y \approx ax. \)

\sphinxAtStartPar
\sphinxincludegraphics{{155465953}.png}


\subsection{The minimum mean square estimate (MMSE)}
\label{\detokenize{Modelling/Linear_regression:the-minimum-mean-square-estimate-mmse}}
\sphinxAtStartPar
Suppose we want to minimise the squared error of our estimate on
average. The estimation error is \( e=y-\hat y \) and the squared
error is the \sphinxstyleemphasis{L\_\{2\}}\sphinxhyphen{}norm of the error, that is, \(
\left\|e\right\|^2 = e^T e \) and its mean can be written as the
expectation \( E\left[\left\|e\right\|^2\right] =
E\left[\left\|y-\hat y\right\|^2\right] =
E\left[\left\|y-A^T x\right\|^2\right]. \) Formally, the
minimum mean square problem can then be written as
\begin{equation*}
\begin{split} \min_A\, E\left[\left\|y-A^T x\right\|^2\right]. \end{split}
\end{equation*}
\sphinxAtStartPar
This can in generally not be directly implemented because we have the
abstract expectation\sphinxhyphen{}operation in the middle.

\sphinxAtStartPar
\sphinxstyleemphasis{(Advanced derivation begins)} To get a computational model, first note
that the error expectation can be written in terms of the mean of a
sample of vector \sphinxstyleemphasis{e\_\{k\}} as
\begin{equation*}
\begin{split} E\left[\left\|e\right\|^2\right] \approx \frac1N
\sum_{k=1}^N \left\|e_k\right\|^2 = \frac1N {\mathrm{tr}}(E^T
E), \end{split}
\end{equation*}
\sphinxAtStartPar
where \( E=\left[e_1,\,e_2,\dotsc,e_N\right] \) and \sphinxstyleemphasis{tr()} is
the matrix trace. To minimize the error energy expectation, we can then
set its \sphinxhref{https://en.wikipedia.org/wiki/Matrix\_calculus}{derivative} to
zero
\begin{equation*}
\begin{split} 0 = \frac{\partial}{\partial A} \frac1N {\mathrm{tr}}(E^T E) =
\frac1N\frac{\partial}{\partial A} {\mathrm{tr}}((Y-A^TX)^T
(Y-A^TX)) = \frac1N(Y-A^T X)X^T \end{split}
\end{equation*}
\sphinxAtStartPar
where the observation matrix is \(
X=\left[x_1,\,x_2,\dotsc,x_N\right] \) and the desired output
matrix is \( Y=\left[y_1,\,y_2,\dotsc,y_N\right] \) . \sphinxstyleemphasis{(End of
advanced derivation)}

\sphinxAtStartPar
It follows that the optimal weight matrix \sphinxstyleemphasis{A} can be solved as
\begin{equation*}
\begin{split} \boxed{A = \left(XX^T\right)^{-1}XY^T = X^\dagger Y^T}, \end{split}
\end{equation*}
\sphinxAtStartPar
where the superscript \( \dagger \) denotes the \sphinxhref{https://en.wikipedia.org/wiki/Moore\%E2\%80\%93Penrose\_pseudoinverse}{Moore\sphinxhyphen{}Penrose
pseudo\sphinxhyphen{}inverse}.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Linear_regression_1_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsubsection{Estimates with a mean parameter}
\label{\detokenize{Modelling/Linear_regression:estimates-with-a-mean-parameter}}
\sphinxAtStartPar
Suppose that instead of an estimate \( \hat y=A^T x \) , we want to
include a mean vector in the estimate as \( \hat y=A^T x + \mu \) .
While it is possible to derive all of the above equations for this
modified model, it is easier to rewrite the model into a similar form as
above with
\begin{equation*}
\begin{split} \hat y=A^T x + \mu = \begin{bmatrix} \mu^T \\ A^T
\end{bmatrix} \begin{bmatrix} 1 \\ x \end{bmatrix} := A'^T x'. \end{split}
\end{equation*}
\sphinxAtStartPar
That is, we can extend \sphinxstyleemphasis{x} by a single 1, (the observation \sphinxstyleemphasis{X} similarly
with a row of constant 1s), and extend \sphinxstyleemphasis{A} to include the mean vector.
With this modifications, the above Moore\sphinxhyphen{}Penrose pseudo\sphinxhyphen{}inverse can
again be used to solve the modified model.


\subsubsection{Estimates with linear equality constraints}
\label{\detokenize{Modelling/Linear_regression:estimates-with-linear-equality-constraints}}
\sphinxAtStartPar
\sphinxstyleemphasis{(Advanced derivation begins)}

\sphinxAtStartPar
In practical situations we often have also linear constraints, such as
\( C^T A = B \) , which is equivalent with \( C^T A - B = 0. \) The
modified programming task is then
\begin{equation*}
\begin{split} \min_A\, E\left[\left\|y-A^T
x\right\|^2\right]\quad\text{such that}\quad C^T A - B = 0. \end{split}
\end{equation*}
\sphinxAtStartPar
For simplicity, let us consider only scalar estimation, where instead of
vector \sphinxstyleemphasis{y}, as well as matrices \sphinxstyleemphasis{A, B} and \sphinxstyleemphasis{C}, respectively, we have
scalar \sphinxstyleemphasis{θ} as well as vector \sphinxstyleemphasis{a, b} and *c *and the optimization problem
is
\begin{equation*}
\begin{split} \min_a\, E\left[\left\|\theta-a^T
x\right\|^2\right]\quad\text{such that}\quad c^T a - b = 0. \end{split}
\end{equation*}
\sphinxAtStartPar
Such constraints can be included into the objective function using the
method of \sphinxhref{https://en.wikipedia.org/wiki/Lagrange\_multiplier}{Lagrange
multipliers} such
that the modified objective function is
\begin{equation*}
\begin{split} \eta(a,g) = E\, \left[\left\\|\theta - a^T
x\right\|^2\right] - 2 \left[ g^T \left(c^T a -
b\right)\right]. \end{split}
\end{equation*}
\sphinxAtStartPar
A heuristic explanation of this objective function is based on the fact
the \sphinxstyleemphasis{g} is a free parameter. Since its value can be anything, then \(
c^T a - b \) must be zero, because otherwise the output value of the
objective function could be anything. That is, when optimizing with
respect to \sphinxstyleemphasis{a}, we find the minimum of the mean square error, while
simultaneously satisfying the constraint.

\sphinxAtStartPar
The objective function can further be rewritten as
\begin{equation*}
\begin{split} \begin{split} \eta(a,g) & = E\, \left[\left(\theta - a^T
x\right)\left(\theta - a^T x\right)^T\right] - 2 \gamma
\left(c^T a - \beta\right) \\& = E\, \left[\theta^2 - 2\theta
x^T a + a^T xx^T a\right] - 2 \gamma \left(c^T a - \beta\right)
\\& = \sigma_\theta^2 + a^T R_x^T a - 2 \gamma \left(c^T a -
\beta\right) \\& = \begin{bmatrix} a^T & \gamma \end{bmatrix}
\begin{bmatrix} R_x & -c \\ -c^T & 0 \end{bmatrix} \begin{bmatrix}
a \\ \gamma \end{bmatrix} -2 \begin{bmatrix} 0 & \beta^T
\end{bmatrix} \begin{bmatrix} a \\ \gamma \end{bmatrix} +
\sigma_\theta^2 \\& = \begin{bmatrix} a^T & \gamma-\beta
\end{bmatrix} \begin{bmatrix} R_x & -c \\ -c^T & 0 \end{bmatrix}
\begin{bmatrix} a \\ \gamma-\beta \end{bmatrix} + \text{constant}
\\& := (a'-\mu)^T R' (a'-\mu) + \text{constant} . \end{split} \end{split}
\end{equation*}
\sphinxAtStartPar
where  \( R_x = E[xx^T] \) and “\sphinxstyleemphasis{constant”} refers to a constant
which does not depend on \sphinxstyleemphasis{a} or γ.

\sphinxAtStartPar
In other words, with a straightforward approach, equality constraints
can in general also be merged into a quadratic form. We can therefore
reduce constrained problems to unconstrained problems which can be
easily solved.

\sphinxAtStartPar
\sphinxstyleemphasis{Inequality} constraints can be reduced to quadratic forms with similar
steps as follows. Suppose we have a task  \( \min f(x)\,\text{such
that}\,x \geq b. \) We would then first solve the unconstrained
problem \( \min f(x) \) and check whether the constraint is
satisfied. If not, then the inequality constraint is “active”, that is,
we must have \sphinxstyleemphasis{x=b}. We can then rewrite the inequality constraint as an
equality constraint and solve the problem as above.

\sphinxAtStartPar
In the case that we have multiple constraints over multiple dimensions,
we can keep merging them one by one until we have an entire
unconstrained programming problem.

\sphinxAtStartPar
\sphinxstyleemphasis{(Advanced derivation ends)}


\subsection{Some applications}
\label{\detokenize{Modelling/Linear_regression:some-applications}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,myst}{Linear prediction} in modelling of the spectral
envelope of speech

\item {} 
\sphinxAtStartPar
Noise attenuation with Wiener filtering

\end{itemize}


\subsection{Discussion}
\label{\detokenize{Modelling/Linear_regression:discussion}}
\sphinxAtStartPar
Linear regression is just about the simplest thing you can do to model
data. If that works then it’s perfect! Especially for estimating
low\sphinxhyphen{}dimensional from high\sphinxhyphen{}dimensional data, linear estimates can be very
useful. In any case, it is always a good approach to start modelling
with the simplest possible model, which usually is a linear model. If
nothing else, that gives a good baseline. The first figure on this page
demonstrates a case where a linear model does do a decent job at
modelling the data.

\sphinxAtStartPar
Naturally there are plenty of situations where linear models are
insufficient, such as when the data
\begin{itemize}
\item {} 
\sphinxAtStartPar
follows non\sphinxhyphen{}linear relationships

\item {} 
\sphinxAtStartPar
has discontinuities or when the data

\item {} 
\sphinxAtStartPar
contains multiple classes with different properties.

\end{itemize}

\sphinxAtStartPar
Moreover, in many cases we are not interested in modelling the average
signal, but to recreate a signal which contains all the complexity of
the original signal. Say, if we want to synthesize speech, then “average
speech” can sound dull. Instead, we would like to reproduce all the
colorfulness and expressiveness of a natural speaker. A model of the
statistical distribution of the signal can then be more appropriate,
such as the \DUrole{xref,myst}{Gaussian mixture model
(GMM)}.

\sphinxAtStartPar
Another related class of models are {\hyperref[\detokenize{Modelling/Sub-space_models::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Sub\sphinxhyphen{}space
models}}}}, where the input signal is modeled in a
lower\sphinxhyphen{}dimensional space such that dimensions related to background noise
are cancelled and the desired speech signal is retained.

\sphinxstepscope


\section{Sub\sphinxhyphen{}space models}
\label{\detokenize{Modelling/Sub-space_models:sub-space-models}}\label{\detokenize{Modelling/Sub-space_models::doc}}
\sphinxAtStartPar
In many cases, we can assume that signals are low\sphinxhyphen{}dimensional in the
sense that a high\sphinxhyphen{}dimensional observation  \( y\in{\mathbb
R}^{N\times 1} \) can be completely explained by a low\sphinxhyphen{}dimensional
representation  \( x\in{\mathbb R}^{M\times 1} \) such that with a
matrix  \( A\in{\mathbb R}^{N\times M} \) we have \( y = Ax \)
with \(N\>M\). This signal thus spans only a \(M\)\sphinxhyphen{}dimensional \sphinxstyleemphasis{sub\sphinxhyphen{}space}
of the whole \(N\)\sphinxhyphen{}dimensional space.


\subsection{Application with known sub\sphinxhyphen{}space}
\label{\detokenize{Modelling/Sub-space_models:application-with-known-sub-space}}
\sphinxAtStartPar
This representation comes in handy for example if we assume that we have
only a noisy observation of \(y\). The desired signal lies in a sub\sphinxhyphen{}space,
so all the other dimensions have only noise in them and we can remove
them. We thus only need a mapping from the whole space to the sub\sphinxhyphen{}space.
It turns out that such a mapping is a projection to the sub\sphinxhyphen{}space
spanned by matrix \(A\). In fact, the minimum mean square error (see
{\hyperref[\detokenize{Modelling/Linear_regression::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Linear regression}}}}) solution is exactly the
\sphinxhref{https://en.wikipedia.org/wiki/Moore\%E2\%80\%93Penrose\_inverse}{Moore\sphinxhyphen{}Penrose
pseudo\sphinxhyphen{}inverse}.
However, the downside with this approach is that here the matrix A needs
to be known in advance such that the pseudo\sphinxhyphen{}inverse can be formed.


\subsection{Estimation of unknown sub\sphinxhyphen{}space}
\label{\detokenize{Modelling/Sub-space_models:estimation-of-unknown-sub-space}}
\sphinxAtStartPar
In practical cases it is rather unusual that we would have access to the
matrix \(A\), but instead, it must be estimated from available data. A
typical approach is based on a \sphinxhref{https://en.wikipedia.org/wiki/Singular\_value\_decomposition}{singular value decomposition
(SVD)} or
the \sphinxhref{https://en.wikipedia.org/wiki/Eigendecomposition\_of\_a\_matrix}{eigenvalue
decomposition}.
In short, we first estimate the covariance matrix of the signal and then
decompose it into uncorrelated components with the singular value
decomposition. Often, a small set of singular values make up most of the
energy of the whole signal. Thus if we discard the smallest singular
values, we do not loose much of the energy, but have a signal of a much
lower dimensionality. The singular value decomposition thus takes the
role of the sub\sphinxhyphen{}space mapping matrix \(A\), and we can apply the model as
described above.


\subsection{Discussion}
\label{\detokenize{Modelling/Sub-space_models:discussion}}
\sphinxAtStartPar
Sub\sphinxhyphen{}space models are theoretically appealing models, since their
analysis is straightforward. In terms of speech signals, the difficulty
lies in finding a representation which is actually low\sphinxhyphen{}rank. In other
words, it is not immediately clear in which domain we can apply analysis
such that speech signals can efficiently modelled by low\sphinxhyphen{}rank models.

\sphinxstepscope


\section{Vector quantization (VQ)}
\label{\detokenize{Modelling/Vector_quantization_VQ:vector-quantization-vq}}\label{\detokenize{Modelling/Vector_quantization_VQ::doc}}
\sphinxAtStartPar
Suppose you have recorded sounds at different locations and want to
categorize them into similar groups. In other words, you have a
stochastic vector \(x\) which you want to characterize with a simple
description. For example, categories could correspond to office, street,
hallway and cafeteria. A classic way for this task is to choose template
vectors \(c_{k}\), which represents a typical sound in each
environment \(k\). To categorize the sounds, you then find that template
vector which is closest to your recording \(x\). In mathematical notation,
you search for a \(k^{^*}\) by
\begin{equation*}
\begin{split} k^* = \arg\min_k \|x-c_k\|^2. \end{split}
\end{equation*}
\sphinxAtStartPar
The above expression thus calculates the squared error between \(x \) and
each of the vectors \(c_{k}\) and chooses the index \(k \) of the
vector with the smallest error. The vectors \(c_{k}\) then
represent a codebook and the vector \(x\) is quantized to
\(c_{k^*}\). This is the basic idea behind \sphinxstyleemphasis{vector quantization,}
which is also known as \sphinxstyleemphasis{k\sphinxhyphen{}means}.

\sphinxAtStartPar
A illustration of a simple vector codebook is shown on the right. The
input data is a Gaussian distribution shown with grey dots and the
codebook vectors \(c_{k}\) with red circles. For each input vector
we thus search for the nearest codebook vector and the borders of the
regions where input vectors are assigned to a particular codebook vector
are illustrated with blue lines. These regions are known as \sphinxhref{https://en.wikipedia.org/wiki/Voronoi\_diagram}{Voronoi
regions} and the blue
lines are the decision\sphinxhyphen{}boundaries between codebook vectors.

\sphinxAtStartPar
\sphinxincludegraphics{{175511825}.png}

\sphinxAtStartPar
Example of a codebook for a 2D Gaussian with 16 code vectors.


\subsection{Metric for codebook quality}
\label{\detokenize{Modelling/Vector_quantization_VQ:metric-for-codebook-quality}}
\sphinxAtStartPar
Suppose then that you have a large collection of
vectors \(x_{h}\), and you want to find out how well this codebook
represents the input data. The expectation of the squared error is
approximately the mean over your data, such that
\begin{equation*}
\begin{split} E_h\left[ \min_k \|x_h-c_k\|^2 \right] \approx \frac 1N
\sum_{h=1}^N \min_k \|x_h-c_k\|^2, \end{split}
\end{equation*}
\sphinxAtStartPar
where \(E[ ]\) is the expectation operator and \(N\) is the number of
input vectors \(x_{h}\). Above, we thus find the codebook vector
which is closest to \(x_{h}\), find its squared error and take the
expectation over all possible inputs. This is approximately equal to the
mean of those squared errors over a set of input vectors.

\sphinxAtStartPar
To find the best set of codebook vectors \(c_{k}\), we then need
to minimize the mean squared error as
\begin{equation*}
\begin{split} \{c_k^*\} := \arg\min_{\{c_k\}}\, E_h\left[ \min_k
\|x_h-c_k\|^2 \right]  \end{split}
\end{equation*}
\sphinxAtStartPar
or more specifically, for a dataset as
\begin{equation*}
\begin{split} \{c_k^*\} := \arg\min_{\{c_k\}} \sum_{h=1}^N \min_k
\|x_h-c_k\|^2. \end{split}
\end{equation*}
\sphinxAtStartPar
Unfortunately we do not have an analytic solution for this optimization
problem, but have to use numerical, iterative methods.


\subsection{Codebook optimization}
\label{\detokenize{Modelling/Vector_quantization_VQ:codebook-optimization}}

\subsubsection{Expectation maximization (EM)}
\label{\detokenize{Modelling/Vector_quantization_VQ:expectation-maximization-em}}
\sphinxAtStartPar
Classical methods for finding the best codebook are derivatives of
expectation maximization (EM), which is based on two alternating steps:

\sphinxAtStartPar
\sphinxstyleemphasis{Expectation Maximation (EM)} algorithm:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
For every vector \(x_{h}\) in a large database, find the best
codebook vector \(c_{k}\).

\item {} 
\sphinxAtStartPar
For every codebook vector \(c_{k}\);
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
Find all vectors \(x_{h}\) assigned to that codevector.

\item {} 
\sphinxAtStartPar
Calculate mean of those vectors.

\item {} 
\sphinxAtStartPar
Assign the mean as a new value for the codevector.

\end{enumerate}

\item {} 
\sphinxAtStartPar
If converged then stop, otherwise go to 1.

\end{enumerate}

\sphinxAtStartPar
This algorithm is guaranteed to give a codebook at every step which is
\sphinxstyleemphasis{not worse} than the previous codebook. That is, at each iteration will
improve until it finds a local minimum, where it stops changing. The
reason is that each step in the iteration finds a partial best\sphinxhyphen{}solution.
In the first step, we find the best matching codebook vectors for each
data vectors \(x_{h}\). In the second step, we find the
within\sphinxhyphen{}category mean. That is, the new mean is more accurate than the
previous codevector in that it reduces the average squared error. If the
mean is equal to the previous codevector, then there is no improvement.

\sphinxAtStartPar
As noted above, this algorithm is the basis to most vector quantization
codebook optimization algorithms. There are a multiple reasons why this
simple algorithm is usually not sufficient alone. Most importantly, the
above algorithm is slow to converge to a stable solution \sphinxstyleemphasis{and} it often
finds a local minimum instead of a global minimum.

\sphinxAtStartPar
To improve performance, we can apply several heuristic approaches. For
example, we can start with a small codebook \( \{ c_k \}_{k=1}^K \)
of \(K\) elements and optimize it with the EM algorithm. We then split the
codebook into two, offset by a small delta \(d\), such that \(
\|d\|<\epsilon \) and make the new codebook \( \{ \hat c_k
\}_{k=1}^{2K} := \{ c_k,\, c_k+d \}_{k=1}^K \) of 2\(K\) elements.
We then rerun the EM algorithm on the new codebook. The codebook thus
doubles in size at every iteration and we continue until we have the
desired codebook size.

\sphinxAtStartPar
The advantage of this approach is that it focuses attention to the big
bulk of datapoints \(x_{k}\), and ignores outliers. The outcome is
then expected to be more stable and the likelihood of converging to a
local minimum is smaller. The downside is that with this approach it is
then more difficult to find small separated islands. That is, because
the initial codebook is near the center of the whole mass of datapoints,
adding a small delta to the codebook vectors keeps the new codevectors
near the center\sphinxhyphen{}of\sphinxhyphen{}mass.

\sphinxAtStartPar
Conversely, we can start with a large codebook, say treat the whole
input database \(x_{k}\) as a codebook. We can then iteratively
merge pairs of points which are close to each other, until the codebook
is reduced to the desired size. Needless to say, this will be a slow
process if the database is large, but will be very efficient in finding
separated islands of points.

\sphinxAtStartPar
In any case, optimization of vector codebooks is a difficult task and we
have no practical algorithms which would be guaranteed to find the
global optimum. Like in many other machine learning problems, optimizing
the codebook is very much about learning to know your data. You should
first use one algorithm and then analyse the output to find out what can
be improved, and keep repeating this optimization and analyse process
until the output is sufficiently good.


\subsubsection{Optimization with machine learning platforms}
\label{\detokenize{Modelling/Vector_quantization_VQ:optimization-with-machine-learning-platforms}}
\sphinxAtStartPar
A modern approach to modelling is {\hyperref[\detokenize{Modelling/Neural_networks::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{machine learning}}}}, where complex phenomena are modelled with neural networks. Typically they are trained with \sphinxhref{https://en.wikipedia.org/wiki/Gradient\_descent}{gradient\sphinxhyphen{}descent} type methods, where parameters are iteratively nudged towards the minimum, by following the steepest gradient. Since such gradients can be automatically derived on machine learning platforms (using the \sphinxhref{https://en.wikipedia.org/wiki/Chain\_rule}{chain rule}), they can be applied on very complex models. Consequently, they have become very popular and succesful.

\sphinxAtStartPar
The same type of training can be readily applied to vector quantizers as well. However, there is a practical problem with this approach. Estimation of the gradients of the parameters with the chain\sphinxhyphen{}rule requires that \sphinxstyleemphasis{all} intermediate gradients are non\sphinxhyphen{}zero. Quantizers are however piece\sphinxhyphen{}wise constant such that their gradients are uniformly zero, thus disabling the chain rule and gradient descent for all parameters which lie behind the quantizer in the computational graph. A practical solution is known as pass\sphinxhyphen{}through, where gradients are passed unchanged through the quantizer. This approximation is simple to implement and provides often adequate performance.

\sphinxAtStartPar
While this approach converges slower than the EM\sphinxhyphen{}algorithm, it is often beneficial since it allows optimization of an entire system to a single goal. That is, if we have several modules in a system, it is beneficial if their joint interaction is taken into account in the optimization. If modules are optimized independently, it is difficult to anticpate all interactions and the system performance can remain far from optimal.


\subsection{Algorithmic complexity}
\label{\detokenize{Modelling/Vector_quantization_VQ:algorithmic-complexity}}
\sphinxAtStartPar
Vector quantization is manageable for relatively small codebooks of,
say, \(K=32\) codevectors. That corresponds to 5 bits of information. For
many applications, that does not give sufficient accuracy \sphinxhyphen{} the mean
squared error is too large. For example, the linear predictive models in
speech coding could be quantized with 30 bits, which corresponds to \(
K=2^{30}\approx 10^9 \) codevectors. To find the best codevector for a
vector \(x\) of length \(N=16\), we would then need to calculate the
distance between every codebook vector and \(x\), which amounts to
approximately \( 16\times10^9= 1.6\times10^{10} \) operations. That
is infeasible in on\sphinxhyphen{}line applications on mobile devices. Instead, we
need to find a simpler method which retains the best aspects of the
algorithm, but reduces algorithmic complexity.

\sphinxAtStartPar
A heuristic approach is to use successive codebooks, where at each
iteration, we quantize the error of the last iteration. That is, let’s
say that on the first iteration we have 8 bits, corresponding to a
codebook \(c_{k}\) of \(K=256\) vectors. We find the best matching
codevector \(c_{k^*}\) and calculate the residual \(
x':=x-c_{k^*} \) . In the second stage, we would then find the best
matching vector for \(x'\) from a second codebook \(c_{k}'\). We can
add as many layers of codebooks as we want until the desired number of
bits has been consumed. This approach is known as a \sphinxstyleemphasis{multi\sphinxhyphen{}stage vector
quantizer}.

\sphinxAtStartPar
Where ordinary vector quantization can find the optimal solution, split
vector quantization generally does not give a global optimum. It does
give good solutions, though, but with an algorithmic complexity which
very much lower than ordinary vector quantization. For example, in the
above example of 30 bits, we could assign three consecutive layers of
codebooks with 10 bits / \(K=1024\) each, such that the overall complexity
is \( 3\times 16\times 2^{10} \approx 5\times10^4, \) which gives
an improvement with a factor of \( 3.5\times10^5. \) Given that the
reduction in accuracy is manageable, this is a major improvement in
complexity.


\subsection{Applications}
\label{\detokenize{Modelling/Vector_quantization_VQ:applications}}
\sphinxAtStartPar
Probably the most important application where vector quantization is
used in speech processing, is \DUrole{xref,myst}{speech
coding} with \DUrole{xref,myst}{Code\sphinxhyphen{}excited
linear prediction (CELP)}, where
\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,myst}{linear predictive coefficients (LPC)} are
transformed to line spectral frequencies (LSFs), which are often
encoded with multi\sphinxhyphen{}stage vector quantizers.

\item {} 
\sphinxAtStartPar
gains (signal energy) of the residual and long term prediction are
jointly encoded with a single stage vector quantizer.

\end{itemize}

\sphinxAtStartPar
Other typical applications include
\begin{itemize}
\item {} 
\sphinxAtStartPar
In optimization of \DUrole{xref,myst}{Gaussian mixture models
(GMMs)}, it is useful to use vector
quantization to find a first\sphinxhyphen{}guess of the means of each mixture.

\end{itemize}


\subsection{Discussion}
\label{\detokenize{Modelling/Vector_quantization_VQ:discussion}}
\sphinxAtStartPar
The benefit of vector quantization is that it is a simple algorithm
which gives high accuracy. In fact, for quantizing complicated data,
vector quantization is (in theory) optimal in fixed\sphinxhyphen{}rate coding
applications. It is simple in the sense that an experienced engineer can
implement it in a matter of hours. Downsides with vector quantization
include
\begin{itemize}
\item {} 
\sphinxAtStartPar
Complexity; for accurate quantization you need prohibitively large
codebooks. The method therefore does not scale up nicely to big
problems.

\item {} 
\sphinxAtStartPar
Difficult optimization;
\begin{itemize}
\item {} 
\sphinxAtStartPar
Training data; The amount of data needed to optimize a vector
codebook is large. Each codebook vector must be assigned to a
large number of data vectors, such that calculation of the mean
(in the EM algorithm) is meaningful.

\item {} 
\sphinxAtStartPar
Convergence; we have no assurance that optimization algorithms
find the global optimum and we have no assurance that local
minima are “good enough”.

\end{itemize}

\item {} 
\sphinxAtStartPar
Lack of flexibility; the codebook has a fixed size. If we would like
to use codebooks of different sizes, for example, if we want to
transmit data with a variable bit\sphinxhyphen{}rate, then we have to optimize and
store a large codebook for \sphinxstyleemphasis{every possible bitrate}.

\item {} 
\sphinxAtStartPar
Blindness to inherent structures; this model describes data with a
codebook, without any deeper understanding of what the data looks
like within each category. For example, say we have two classes,
speech and non\sphinxhyphen{}speech. Even if speech is very flexible, the
non\sphinxhyphen{}speech class is much, much larger. Speech is a very small subset
of all possible sounds. Therefore, the within\sphinxhyphen{}class variance will be
much larger in the non\sphinxhyphen{}speech class. Consequently, the accuracy in
the non\sphinxhyphen{}speech class would be much lower.As a consequence, we would be tempted to increase the number of
codevectors such that we get uniform accuracy in both classes. But
then we loose the correspondence between codevectors and natural
descriptions of the signal.

\end{itemize}

\sphinxstepscope


\section{Gaussian mixture model (GMM)}
\label{\detokenize{Modelling/Gaussian_mixture_model_GMM:gaussian-mixture-model-gmm}}\label{\detokenize{Modelling/Gaussian_mixture_model_GMM::doc}}

\subsection{Motivation}
\label{\detokenize{Modelling/Gaussian_mixture_model_GMM:motivation}}
\sphinxAtStartPar
Where approaches such as {\hyperref[\detokenize{Modelling/Linear_regression::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{linear regression}}}}
and {\hyperref[\detokenize{Modelling/Sub-space_models::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{sub\sphinxhyphen{}space models}}}} are based on reducing the
dimensionality of a signal to capture the essential information in a
signal, in many cases we want to model the full range of possible
signals. For that purpose we can design models the \sphinxstyleemphasis{statistical
distribution} of the signal. For example, it is possible to model a
signal as a \sphinxhref{https://en.wikipedia.org/wiki/Gaussian\_process}{Gaussian
process}, where every
observation has a (multivariate) Gaussian (normal) distribution.

\sphinxAtStartPar
Speech signals however feature much more structure than simple Gaussian
processes. For example voiced signals are very different from unvoiced
signals, and within both voiced and unvoiced signals we have a multitude
of distinct groups of utterances whose statistical characteristics are
clearly different. Modelling them all with a Gaussian process would
ignore such structures and the model would therefore be inefficient.

\sphinxAtStartPar
\sphinxstyleemphasis{\sphinxhref{https://en.wikipedia.org/wiki/Mixture\_model}{Mixture models}} is a
type of models, where we assume that the signal under study consists of
several distinct classes, where each class has its own unique
statistical model. That is, for example the statistics of voiced sounds
is clearly different from those of unvoiced sounds. We model each class
with its own distribution and their joint distribution is the weighted
sum of the class distributions. The weights of each distribution
correspond to the frequency with which they appear in the signal. So if
unvoiced signals would in some hypothetical language constitute 30\% of
all speech sounds, then the weight of the unvoiced class would be 0.3.

\sphinxAtStartPar
The most typical mixture model structure uses Gaussian (normal)
distributions for each of the classes, so that the whole model is known
as a \sphinxstyleemphasis{Gaussian mixture model} (GMM). Depending on application the
class\sphinxhyphen{}distributions can obviously take other forms than Gaussian, for
example a Beta mixture model could be used if the individual classes
follow the Beta distribution. In this document we however focus on
Gaussian mixture models because it is most common among mixture models
and demonstrates application in an accessible way.


\subsection{Model definition}
\label{\detokenize{Modelling/Gaussian_mixture_model_GMM:model-definition}}
\sphinxAtStartPar
The \sphinxhref{https://en.wikipedia.org/wiki/Multivariate\_normal\_distribution}{multivariate normal
distribution}
for a variable \(x\) is defined as
\begin{equation*}
\begin{split} f\left(x;\Sigma,\mu\right) =
\frac{1}{\sqrt{\left(2\pi\right)^N \|\Sigma\|}}
\exp\left[-\frac12 (x-\mu)^T \Sigma^{-1}(x-\mu)\right], \end{split}
\end{equation*}
\sphinxAtStartPar
where \( \Sigma \) and  \( \mu \) are the covariance and mean of
the process, respectively, with \(N\) dimensions. In other words, this is
the familiar Gaussian process for vectors \(x\).

\sphinxAtStartPar
Suppose then that we have \(K\) classes in the signal, where each class
has its own covariance and mean \( \Sigma_k \) and  \( \mu_k. \)
The \sphinxstyleemphasis{Gaussian mixture model} is then defined as
\begin{equation*}
\begin{split} \boxed{f\left(x\right) = \sum_{k=1}^K \alpha_k f\left(x;
\Sigma_k,\mu_k\right),} \end{split}
\end{equation*}
\sphinxAtStartPar
where the weights \( \alpha_k \) add up to unity \( \sum_{k=1}^K
\alpha_k=1. \)


\subsection{Applications}
\label{\detokenize{Modelling/Gaussian_mixture_model_GMM:applications}}\begin{itemize}
\item {} 
\sphinxAtStartPar
In recognition/classification applications, we can, for example,
model a system which has two distinct states (like speech and noise)
and train a GMM with mixture components matching those states. When
receiving a microphone signal, we can then determine the likelihood
of each mixture component and thus obtain the likelihood that the
signal is speech or noise.

\item {} 
\sphinxAtStartPar
In \DUrole{xref,myst}{transmission
applications}, our
objective is to model the signal such that we can transmit likely
signals with a small amount of bits and unlikely signals with a
large number of bits. If we train a GMM on a speech database, we can
determine which signals are speech\sphinxhyphen{}like, such that those can be
transmitted with a low number of bits.

\end{itemize}

\sphinxstepscope


\section{Neural networks}
\label{\detokenize{Modelling/Neural_networks:neural-networks}}\label{\detokenize{Modelling/Neural_networks::doc}}

\subsection{Introduction}
\label{\detokenize{Modelling/Neural_networks:introduction}}
\sphinxAtStartPar
An Artificial Neural Network (ANN) is a mathematical model that tries to simulate the structure and functionalities of biological neural networks. Basic building block of every artificial neural network is artificial neuron, that is, a simple mathematical model
(function).  Artificial neuron is a basic building block of every
artificial neural network. Its design and functionalities are derived from observation of a biological neuron that is basic building block of biological neural networks (systems) which includes the brain, spinal
cord and peripheral ganglia.

\sphinxAtStartPar
Computational modelling using neural networks was started in the 1940’s
and has gone through several waves of innovation and subsequent decline.
The recent boom of deep neural networks (DNNs) \sphinxhyphen{} which roughly means
that the network has more layers of non\sphinxhyphen{}linearities than previous
models \sphinxhyphen{} happened probably due to the increase in available
computational power and availability of large data\sphinxhyphen{}sets. It has fuelled
a flurry of incremental innovation in all directions, and in many
modelling tasks, deep neural networks currently give much better results
than any competing method.

\sphinxAtStartPar
Despite its successes, application of DNNs in speech processing does not
come without its fair share of problems. For example,
\begin{itemize}
\item {} 
\sphinxAtStartPar
Training DNNs for a specific task on a particular set of data often
does not increase our understanding of the problem. It is a black
box. How are we to know whether the model is reliable? A trained
speech recognizer on a language A does not teach much about speech
recognition for language B (though the process of designing a
recognizers does teach us about languages).

\item {} 
\sphinxAtStartPar
Training of DNNs is sensitive to the data on which it is trained on.
Models can for example be susceptible to hidden biases, such that
performance degrades for particular under\sphinxhyphen{}represented groups of
people.

\item {} 
\sphinxAtStartPar
A trained DNN is “a solution” to a particular problem, but it does
not directly give us information about how good of a solution it is.
For example, if a data\sphinxhyphen{}set represents a circle in the 2D\sphinxhyphen{}plane, then
it is possible to accurately model that data\sphinxhyphen{}set with a neural
network where the non\sphinxhyphen{}linearities are sigmoids. The neural network
just has to be large enough and it can do the job. However, the
network is then several orders of magnitude more complex than the
equation of the circle. That is, though training of the network was
successful, and the model is relatively accurate, it gives no
indication if the complexity of the network is of similar scale as
the complexity of the problem.

\end{itemize}

\sphinxAtStartPar
To combat such problems, a recent trend in model design has been to
return to classical design paradigms, where models are based on thorough
understanding of the problem. The parameters of such models are then
trained using methods from machine learning.


\subsection{Network structures}
\label{\detokenize{Modelling/Neural_networks:network-structures}}
\sphinxAtStartPar
Neural networks are, in principle, built from simple building blocks,
where the most common type of building block is
\begin{equation*}
\begin{split} y = f(A x + b) \end{split}
\end{equation*}
\sphinxAtStartPar
where \sphinxstyleemphasis{x} is an input vector, matrix \sphinxstyleemphasis{A} and vector \sphinxstyleemphasis{b} are constants,
\sphinxstyleemphasis{f} is a non\sphinxhyphen{}linear function such as the element\sphinxhyphen{}wise sigmoid, and \sphinxstyleemphasis{y}
is the output. This is often referred to as a \sphinxstyleemphasis{layer}. Layers can then
be stacked after each other such that, for example, a three layer
network would be
\begin{equation*}
\begin{split} \begin{matrix} y_1 &= f(A_1 x + b_1)\\ y_2 &= f(A_2 y_1 +
b_3)\\ y_{out} &= f(A_3 y_2 + b_3). \end{matrix} \end{split}
\end{equation*}

\subsubsection{Deep Neural Networks (DNNs)}
\label{\detokenize{Modelling/Neural_networks:deep-neural-networks-dnns}}
\sphinxAtStartPar
Deep Neural Network (DNNs) are an artificial neural network (ANN) with
multiple layers between the input and output layers. Many experts define
deep neural networks as networks that have an input layer, an output
layer and at least one hidden layer in between. Each layer performs
specific types of sorting and ordering in a process that some refer to
as “feature hierarchy.” One of the key uses of these sophisticated
neural networks is dealing with unlabeled or unstructured data. The
phrase “deep learning” is also used to describe these deep neural
networks, as deep learning represents a specific form of machine
learning where technologies using aspects of artificial intelligence
seek to classify and order information in ways that go beyond simple
input/output protocols.


\subsubsection{Basics of Neural Networks}
\label{\detokenize{Modelling/Neural_networks:basics-of-neural-networks}}
\sphinxAtStartPar
Neurons: It forms the basic structure of a neural network. When we get
the information, we process it and then we generate an output.
Similarly, a neuron receives an input, processes it and generates an
output which is either sent to other neurons for further processing or
it is the final output.

\sphinxAtStartPar
Weights: When an input enters a neuron, it is multiplied by a weight.
Initially, the weights are initialized and they are updated during the
model training process. When the training is over, the neural network
assigns a higher weight value to the input it considers more important
as compared to the ones which are considered less important.

\sphinxAtStartPar
Bias: In addition to the weights, another linear component is applied to
the input, called as the bias. It is added to the result of weight
multiplication to the input. The bias is basically added to change the
range of the weight multiplied input.


\subsubsection{Activation Functions:}
\label{\detokenize{Modelling/Neural_networks:activation-functions}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Sigmoid: It allows a reduction in extreme or atypical values in
valid data without eliminating them: it converts independent
variables of almost infinite range into simple probabilities between
0 and 1. Most of its output will be very close to the extremes of 0
or 1.
\$\( sigmoid(x) = \frac{1}{1+e^{-x}} \)\$

\end{enumerate}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mf}{1.} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}sigmoid(x)\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Neural_networks_4_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
ReLU(Rectified Linear Unit): It has output 0 if the input is less
than 0, and raw output otherwise. That is, if the input is greater
than 0, the output is equal to the input. The operation of ReLU is
closer to the way our biological neurons work.
\$\( ReLU(x) = \max(x,0) = \begin{cases} x, & x>0\\ 0, & \text{otherwise}.\end{cases} \)\$

\end{enumerate}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{*}\PYG{n}{x}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}ReLU(x)\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Neural_networks_6_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Softmax\_function}{Softmax}: A modification
of the regular \(\max\) over a vector, such that it has a continuous derivative
everywhere. That is, it maps the
output to the range \([0,1]\) and simultaenously ensures
that the total sum is 1. The output of Softmax is therefore
a probability distribution.
\$\(  SoftMax (\mathbf {x_k} )=\frac {e^{x_k}}{\sum _{j=1}^{K}e^{x_j}}.\)\$

\end{enumerate}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Input / Output / Hidden Layer : The input layer receives the input
and is the first layer of the network. The output layer is the one which
generates the output and is the final layer of the network. The
processing layers are the hidden layers within the network. These hidden
layers are the ones which perform specific tasks on the incoming data
and pass on the output generated by them to the next layer. The input
and output layers are the ones visible to us, while are the intermediate
layers are hidden.

\item {} 
\sphinxAtStartPar
MLP (Multi Layer perceptron): In the simplest network, we would have
an input layer, a hidden layer and an output layer. Each layer has
multiple neurons and all the neurons in each layer are connected to all
the neurons in the next layer. These networks can also be called as
fully connected networks.

\item {} 
\sphinxAtStartPar
Cost or loss function: When we train a network, its main objective is to  to
predict the output as close as possible to the actual value. Hence, the
cost/loss function is used to measure this accuracy. The cost or loss
function penalizes the network when it makes errors. The main objective
while running the network is to increase the prediction accuracy and to
reduce the error, thus minimizing the cost function.

\item {} 
\sphinxAtStartPar
Gradient Descent: It is an optimization algorithm used to minimize
some function by iteratively moving in the direction of steepest descent
as defined by the negative of the gradient.

\item {} 
\sphinxAtStartPar
Learning Rate: The learning rate is a hyperparameter that controls
how much to change the model in response to the estimated error each
time the model weights are updated. Choosing the learning rate is
challenging as a value too small may result in a long training process
that could get stuck, whereas a value too large may result in learning a
sub\sphinxhyphen{}optimal set of weights too fast or an unstable training process. The
learning rate may be the most important hyperparameter when configuring
neural network. Therefore, it is important  to know how to investigate
the effects of the learning rate on model performance and to build an
intuition about the dynamics of the learning rate on model behavior.

\item {} 
\sphinxAtStartPar
Backpropagation: When we define a neural network, we assign random
weights and bias values to our nodes. Once we have received the output
for a single iteration, we can calculate the error of the network. This
error is then fed back to the network along with the gradient of the
cost function to update the weights of the network. These weights are
then updated so that the errors in the subsequent iterations is reduced.
This updating of weights using the gradient of the cost function is
known as back\sphinxhyphen{}propagation. In back\sphinxhyphen{}propagation the movement of the
network is backwards, the error along with the gradient flows back from
the out layer through the hidden layers and the weights are updated.

\item {} 
\sphinxAtStartPar
Batches: While training a neural network, instead of sending the
entire input in one go, we divide in input into several chunks of equal
size randomly. Training the data on batches makes the model more
generalized as compared to the model built when the entire data set is
fed to the network in one go.

\item {} 
\sphinxAtStartPar
Epochs: An epoch is is a single training iteration of all batches
in both forward and back propagation. Thus, 1 epoch is a single forward
and backward pass of the entire input data. Although it is highly likely
that more number of epochs would show higher accuracy of the network, it
would also take longer for the network to converge. You should also take
into account that if the number of epochs are too high, the network
might over\sphinxhyphen{}fit.

\item {} 
\sphinxAtStartPar
Dropout: It is a regularization technique to prevent over\sphinxhyphen{}fitting
of the network. While training, a number of neurons in the hidden layer
are randomly dropped.

\item {} 
\sphinxAtStartPar
Batch Normalization: It normalizes the input layer by adjusting and
scaling the activations. For example, if we have features from 0 to 1
and some from 1 to 1000, we should normalize them to speed up learning.

\end{enumerate}

\sphinxAtStartPar
Classification of Neural Networks
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Shallow neural network}: It has only one hidden layer between the
input and output.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deep neural network}: it has more than one layer.

\end{enumerate}

\sphinxAtStartPar
Types of Deep Learning Networks

\sphinxAtStartPar

\begin{itemize}
\item {} 
\sphinxAtStartPar
Feed forward neural networks: They are artificial neural networks
where the connections between units do not form a cycle. Feedforward
neural networks were the first type of artificial neural network
invented and are simpler than their counterpart, recurrent neural
networks (see below ). They are called feedforward because
information only travels forward in the network.  Firstly, it goes
through the input nodes, then through the hidden nodes and finally
through the output nodes.

\end{itemize}


\subsubsection{Convolutional neural networks}
\label{\detokenize{Modelling/Neural_networks:convolutional-neural-networks}}
\sphinxAtStartPar
CNN was first proposed by {[}1{]}.  It has first been successfully used by
{[}2{]}  for handwritten digit classification problem. It is currently the
most popular neural network model being used for image classification
problem. The advantages of CNNs over DNNs include CNNs are highly
optimized for processing 2D and 3D images, and are effective to learn
and extract abstractions of 2D features. In addition to these, CNNs have
significantly fewer parameters than a fully connected network of similar
size.

\sphinxAtStartPar
Figure 4.1. shows the overall architecture of CNNs. CNNs consists mainly
of two  parts: feature extractors and classifier. In the feature
extraction module, each layer of the network receives the output from
its immediate previous layer as its input and passes its output as the
input to the next layer. The CNN architecture mainly consists of three
types of layers: convolution, pooling, and classification.

\sphinxAtStartPar
Figure 4.1. An overall architecture of the Convolutional Neural Network.

\sphinxAtStartPar
The main layers in Convolutional Neural Networks are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Convolutional layer: A “filter” passes over the image, scanning a
few pixels at a time and creating a feature map that predicts the
class to which each feature belongs. Thus, in this layer, feature
maps from previous layers are convolved with learnable kernels. The
output of the kernels goes through a linear or non\sphinxhyphen{}linear activation
function, such as sigmoid, hyperbolic tangent, Softmax, rectified
linear, and identity functions) to form the output feature maps.
Each of the output feature maps can be combined with more than one
input feature map.

\item {} 
\sphinxAtStartPar
Pooling layer: It reduces the amount of information in each feature
obtained in the convolutional layer while maintaining the most
important information (there are usually several rounds of
convolution and pooling).

\item {} 
\sphinxAtStartPar
Fully connected input layer (flatten): It takes the output of the
previous layers, “flattens” them and turns them into a single vector
that can be an input for the next stage.

\item {} 
\sphinxAtStartPar
The first fully connected layer: It takes the inputs from the
feature analysis and applies weights to predict the correct label.

\item {} 
\sphinxAtStartPar
Fully connected output layer: It gives the final probabilities for
each label.

\end{itemize}

\sphinxAtStartPar
Higher\sphinxhyphen{}level features are derived from features propagated from lower
level layers. As the features propagate to the highest layer or level,
the dimensions of features are reduced based on the size of the kernel
for the convolution and pooling operations respectively. However, the
number of feature maps usually increase for representing better features
of the input images for ensuring classification accuracy. The output of
the last layer of the CNN is used as the input to a fully connected
network. Feed\sphinxhyphen{}forward neural networks are used as the classification
layer since they provide better performance.

\sphinxAtStartPar
Figure 4.2 Popular CNN architectures. The Figure is taken from
\sphinxurl{https://www.aismartz.com/blog/cnn-architectures/}.

\sphinxAtStartPar
Applications of CNNs:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Image Processing and Computer Vision: CNNs have been successfully
used in different image classification tasks {[}7–11{]}.

\item {} 
\sphinxAtStartPar
Speech Processing: CNNs have been successfully used in different
speech processing applications such as speech enhancement {[}8{]} and
audio tagging {[}9{]}.

\item {} 
\sphinxAtStartPar
Medical Imaging: CNNs have also been widely used in different
medical image processing including classification, detection, and
segmentation tasks {[}10{]}.

\end{itemize}

\sphinxAtStartPar
Training Techniques
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Sub\sphinxhyphen{}Sampling Layer or Pooling Layer:  Two different techniques have
been used for the implementation of deep networks in the
sub\sphinxhyphen{}sampling or pooling layer: average and max\sphinxhyphen{}pooling.  While the
average Pooling calculate the average value for each patch on the
feature map, the max pooling calculate the maximum value for each
patch of the feature map.

\item {} 
\sphinxAtStartPar
Padding: It adds extra layer of zeros across the images so that the
output image has the same size as the input.

\item {} 
\sphinxAtStartPar
Data Augmentation: It is the addition of new data derived from the
given data. This might prove to be beneficial for prediction. It
includes rotation, shearing, zooming, cCropping, flipping and
changing the brightness level.

\end{enumerate}

\sphinxAtStartPar
Note that the performance of CNNs depends heavily on multiple
hyperparameters: number of layers, number of feature maps in each layer,
the use of dropouts, batch normalization, etc. Thus, it’s important that
you should first fine\sphinxhyphen{}tune the model hyperparameters by conducting lots
of experiments. Once you find the right set of hyperparameters, you need
to train the model for a number of epochs.


\subsubsection{Recurrent neural networks (RNNs)}
\label{\detokenize{Modelling/Neural_networks:recurrent-neural-networks-rnns}}
\sphinxAtStartPar
Topics to be covered:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Basics of RNNs

\item {} 
\sphinxAtStartPar
Vanishing and exploding gradient problem

\item {} 
\sphinxAtStartPar
Long short\sphinxhyphen{}term memory (LSTM)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Basics of RNNs}

\sphinxAtStartPar
The standard feedforward neural networks (i.e,, DNNs and CNNs) are
function generators associating appropriate output to input. However,
certain types of data are serial in nature. A recurrent neural network
(RNN) processes sequences such as stock prices, sentences one element at
a time while retaining a memory (called a state) of what has come
previously in the sequence. Recurrent means the output at the current
time step becomes the input to the next time step. At each element of
the sequence, the model considers not just the current input, but what
it remembers about the preceding elements. This memory allows the
network to learn long\sphinxhyphen{}term dependencies in a sequence which means it can
take the entire context into account when making a prediction such as
predicting the next word in a sentence. A RNN is designed to mimic the
human way of processing sequences: we consider the entire sentence when
we form a response instead of words by themselves. For example, consider
the following sentence:

\sphinxAtStartPar
“The concert was boring for the first few minutes but then was terribly
exciting.”

\sphinxAtStartPar
A machine learning model that considers the words in isolation would
probably conclude this sentence is negative. An RNN by contrast should
be able to see the words “but” and “terribly exciting” and realize that
the sentence turns from negative to positive because it has looked at
the entire sequence. Reading a whole sequence gives us a context for
processing its meaning, a concept encoded in recurrent neural networks.

\sphinxAtStartPar
Figure 4.3 General form of RNN

\sphinxAtStartPar
The left part of Figure 4.3 shows that the input\sphinxhyphen{}output relation of a
standard neural network is altered so that the output is fed into the
input. But, the right part of Figure 4.3 shows that the scheme unwrapped
through time. The input is the serial data  \( (x_1,........,x_T) \)
and the output is \( (o_1,........,o_T) \) . The output of a neural
network is fed into the next constituent neural network in the next
stage as part of the input. So the output  \( o_t \) depends on all
the inputs  \( (x_1,........,x_T) \) . It may be the case that it is
not a good idea to make the output  \( o_1 \)

\sphinxAtStartPar
dependent only on  \( x_1 \) . In fact  \( o_1 \) itself should be
produced in context.

\sphinxAtStartPar
RNNs can be used in different applications such as machine translation,
speech recognition, generating image descriptions, video tagging, and
language modeling.

\sphinxAtStartPar
Advantages of Recurrent Neural Network
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
An RNN remembers each and every information through time. It is
useful in time series prediction only because of the feature to
remember previous inputs as well. This is called Long Short Term
Memory.

\item {} 
\sphinxAtStartPar
Recurrent neural network are even used with convolutional layers to
extend the effective pixel neighborhood.

\end{enumerate}

\sphinxAtStartPar
Disadvantages of Recurrent Neural Network
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Gradient vanishing and exploding problems.

\item {} 
\sphinxAtStartPar
Training an RNN is a very difficult task.

\item {} 
\sphinxAtStartPar
It cannot process very long sequences if using tanh or relu as an
activation function.

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Vanishing and exploding gradient problem}

\sphinxAtStartPar
RNNs are very hard to train. Let us see the reason. The term  \( \frac{\partial E}{\partial h^l} \) used in backpropagation algorithm is a
product of a long chain of matrices: \( \left( \frac {\partial
E}{\partial h^l} \right ) = \left ( \frac {\partial
z^{l+1}}{\partial h^l} \right ) \left ( \frac {\partial
h^{l+1}}{\partial z^{l+1}} \right )...\left ( \frac {\partial
h^l}{\partial z^L} \right ) \left ( \frac {\partial E}{\partial
h^L} \right ), \) For the input \( x=h^0 \) , this chain is the
longest. If the activation function  \( sigmoid(t) \) is the sigmoid
function \( sigmoid(t) = \frac{1}{1+e^{-t}} \) its derivative is \(
sigmoid'(t) = \frac{e^t} { \left( 1 + e^t \right ) ^2 } \) which
gets very small so that it practically vanishes except at a small
interval near 0.  It is one of the reasons why people ReLU activation
function is prefered. But it is not a solution, as negative input values
also kill the gradient and make it stay there. Even if one avoids such
an outright vanishing gradient problem, the long matrix multiplication
in general may make the gradient vanish or explode. This kind of problem
gets even more aggravated in the case of RNNs, since RNNs normally
require long chain of backpropagation not only through the layers of
neural networks of constituent cells but also across the different
cells. Hence, RNNs are difficult to train.

\sphinxAtStartPar
\sphinxstylestrong{Long short\sphinxhyphen{}term memory (LSTM)}

\sphinxAtStartPar
The Long Short\sphinxhyphen{}Term Memory (LSTM) was first proposed by Hochreiter and
Schmidhuber {[}10{]} as a solution to the vanishing gradients problem. But
it did not attract much attention until people realized it indeed
provides a good solution to the vanishing and exploding gradient problem
of RNN as described above. We will only describe the architecture of its
cell. There are many variations in the cell architecture, but we present
only the basics.

\sphinxAtStartPar
At the heart of an RNN is a layer made of memory cells. The most popular
cell at the moment is the Long Short\sphinxhyphen{}Term Memory (LSTM) which maintains
a cell state as well as a carry for ensuring that the signal
(information in the form of a gradient) is not lost as the sequence is
processed. At each time step the LSTM considers the current word, the
carry, and the cell state. The LSTM has 3 different gates and weight
vectors: there is a “forget” gate for discarding irrelevant information;
an “input” gate for handling the current input, and an “output” gate for
producing predictions at each time step. However, as Chollet points out,
it is fruitless trying to assign specific meanings to each of the
elements in the cell.

\sphinxAtStartPar


\sphinxAtStartPar
Figure 4.4. Structure of LSTM cell

\sphinxAtStartPar
The structure of LSTM is shown in Figure 4.4 . The input vector is \(
x_t \) . The cell state denoted by  \( c_{t-1} \) and the hidden
state  \( h_{t-1} \) are fed into the LSTM cell and  \( c_t \) and
\( h_t \) are fed into the next cell. Internally, it has four states:
\( i_t \) (input),  \( f_t \) (forget),  \( o_t \) (output), and
\( g_t \) . The forget state  \( f_t \) is obtained as a sigmoid
output of a network with  \( x_t \) and  \( h_{t-1} \) fed into it
as inputs. Since  \( f_t \) is a sigmoid output, each element in it
has a value between 0 and 1. If it is close to 0, it erases  \(
c_{t-1} \) by multiplication; if it is close to 1, it  keeps  \(
c_{t-1} \) by multiplication. Thus,  \( f_t \) is given the name
”forget state” because of this property,  The input state  \( i_t \)
and the output state  \( o_t \) are obtained by using (1) and (3),
respectively. The state  \( g_t \) is also obtained similarly except
that it is an output of the form tanh . This gives the  \( \pm \)
sign. The product it  \( \\otimes \) gt is then added to  \( c_t \)
, and this gives new information to the cell state. The hidden state ht
is obtained as in (6), and the cell output  \( y_t \) is the same as
\( h_t \) . The whole scheme is depicted in Figure 4.4.
\begin{equation*}
\begin{split} 
\begin{array}{lcll}
i_t (input) &=& \sigma (W_{ix}x_t + W_{ih}h_{t-1} + b_i) &(1) \\
f_t (forget) &=& \sigma (W_{fx}x_t + W_{fh}h_{t-1} + b_f) & (2) \\
o_t (output)&=& \sigma (W_{ox}x_t + W_{oh}h_{t-1} + b_o) & (3) \\
g_t &=& tanh (W_{gx}x_t + W_{gh}h_{t-1} + b_g) & (4) \\
c_t (cell\ state) &=& ft\otimes c_{t-1} + i_t \otimes g_t & (5)\\
h_t (hidden\ state) &=& o_t \otimes tanh(c_t) &(6)\\
y_t (cell\ output) &=& h_t & (7)
\end{array}
\end{split}
\end{equation*}

\subsection{References:}
\label{\detokenize{Modelling/Neural_networks:references}}
\sphinxAtStartPar
{[}1{]} Fukushima, K. Neocognitron: A hierarchical neural network capable
of visual pattern recognition. Neural Netw. 1988, 1, 119–130.

\sphinxAtStartPar
{[}2{]} LeCun, Y.; Bottou, L.; Bengio, Y.; Haffner, P. Gradient\sphinxhyphen{}based
learning applied to document recognition. Proc. IEEE 1998, 86,
2278–2324.

\sphinxAtStartPar
{[}3{]} Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet
classification with deep convolutional neural networks. In Proceedings
of the 25th International Conference on Neural Information Processing
Systems, Lake Tahoe, NV, USA, 3–6 December 2012; pp. 1106–1114.

\sphinxAtStartPar
{[}4{]} Zeiler, M.D.; Fergus, R. Visualizing and understanding
convolutional networks. arXiv 2013, arXiv:1311.2901.

\sphinxAtStartPar
{[}5{]} Simonyan, K.; Zisserman, A. deep convolutional networks for
large\sphinxhyphen{}scale image recognition. arXiv 2014, arXiv:1409.1556.

\sphinxAtStartPar
{[}6{]} Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov,
D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A. Going deeper with
convolutions. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, Boston, MA, USA, 7–12 June 2015; pp. 1–9.

\sphinxAtStartPar
{[}7{]} He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for
image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, Las Vegas, NV, USA, 27–30 June 2016; pp.
770– 778.

\sphinxAtStartPar
{[}8{]} Hou, J.\sphinxhyphen{}C.; Wang, S.; Lai, Y.; Tsao, Y.; Chang, H.; Wang, H.
Audio\sphinxhyphen{}Visual Speech Enhancement Using Multimodal Deep Convolutional
Neural Networks. arXiv 2017, arXiv:1703.10893.

\sphinxAtStartPar
{[}9{]} Xu, Y.; Kong, Q.; Huang, Q.; Wang, W.; Plumbley, M.D.
Convolutional gated recurrent neural network incorporating spatial
features for audio tagging. In Proceedings of the 2017 International
Joint Conference on Neural Networks (IJCNN), Anchorage, AK, USA, 14–19
May 2017; pp. 3461–3466.

\sphinxAtStartPar
{[}10{]} Hochreiter, S., Schmidhuber, J., Courville, A., Long short\sphinxhyphen{}term
memory, Neural Computation 9(8):1735\sphinxhyphen{}80 (1997)

\sphinxstepscope


\section{Non\sphinxhyphen{}negative Matrix and Tensor Factorization}
\label{\detokenize{Modelling/Non-negative_Matrix_and_Tensor_Factorization:non-negative-matrix-and-tensor-factorization}}\label{\detokenize{Modelling/Non-negative_Matrix_and_Tensor_Factorization::doc}}

\subsection{Introduction}
\label{\detokenize{Modelling/Non-negative_Matrix_and_Tensor_Factorization:introduction}}
\sphinxAtStartPar
Many of the most descriptive features of speech are described by energy;
for example, formants are peaks and the fundamental frequency is visible
as a comb\sphinxhyphen{}structure in the power spectrum. A basic property of such
features is that they are positive\sphinxhyphen{}valued. Negative values in energy are
not physically realizable. However, most signal processing methods are
applicable only for real\sphinxhyphen{}valued variables and inclusion of a
non\sphinxhyphen{}negative constraints is cumbersome.

\sphinxAtStartPar
\sphinxstyleemphasis{Non\sphinxhyphen{}negative matrix factorization} (NMF or NNMF) and its tensor\sphinxhyphen{}valued
counterparts is a family of methods which explicitly assumes that the
input variables are \sphinxstyleemphasis{non\sphinxhyphen{}negative}, that is, they are by definition
applicable to energy\sphinxhyphen{}signals. In some sense, NMF methods are an
extension of \sphinxhref{https://en.wikipedia.org/wiki/Principal\_component\_analysis}{prinicipal component analsys
(PCA)} \sphinxhyphen{}type
and other {\hyperref[\detokenize{Modelling/Sub-space_models::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{subspace methods}}}} to positive\sphinxhyphen{}valued
signals.


\subsection{Model definition}
\label{\detokenize{Modelling/Non-negative_Matrix_and_Tensor_Factorization:model-definition}}
\sphinxAtStartPar
Specifically, suppose that the power (or magnitude) spectrum of one
window of a speech signal is represented as a \(Nx1\) vector
\(v_k\), and furthermore we arrange the \(K\) windows into an
\(NxK\) matrix \(V\). The signal model we use is then
\begin{equation*}
\begin{split} V \approx WH, \end{split}
\end{equation*}
\sphinxAtStartPar
where \(W\) is the \(NxM\) weight matrix, \(H\) is the \(MxK\) model matrix and
the scalar \(M\) is the model order.

\sphinxAtStartPar
The idea is that \(H\) is a fixed matrix corresponding to our model of the
signal, viz. the source model. It describes typical types features of
the data. With the weights \(W\), we interpolate between the columns
of \(H\). In some sense, this is then a generalization of a codebook (see
\DUrole{xref,myst}{vector quantization}), but such that we
interpolate between codevectors. In addition, we require that all
elements of \(W\) and \(H\) are non\sphinxhyphen{}negative, such that we ensure that \(V\)
is also non\sphinxhyphen{}negative.

\sphinxAtStartPar
Since the model order \(K\) is chosen to be smaller than either \(N\)
or \(K\), this mapping is generally an approximation. The model thus tries
to catch \sphinxstyleemphasis{the relevant features of the input signal with a low number of
parameters}.

\sphinxAtStartPar
The model is generally optimized by
\begin{equation*}
\begin{split} \min_{W,H} \| V - WH \|_F\qquad\text{such that}\qquad
W,H\geq 0. \end{split}
\end{equation*}
\sphinxAtStartPar
Here the norm refers to the \sphinxhref{https://en.wikipedia.org/wiki/Matrix\_norm\#Frobenius\_norm}{Frobenius
norm}, which
is defined as the square root sum of squared elements. We do not have
analytic solutions to the above optimization problem, but we can solve
it by numerical methods, which are included in typical software
libraries.


\subsection{Application}
\label{\detokenize{Modelling/Non-negative_Matrix_and_Tensor_Factorization:application}}
\sphinxAtStartPar
A typical use of NMF type algorithms is source separation, where we find
the solution of the above optimization problem and then identify those
dimensions of \(H\) which corresponds to the different sources. By
retaining only those dimensions of \(W\) which correspond to the desired
source, we can thus extract the desired source signal from their mixture
with the interfering other sources. For example, we might want to
extract a speech signal corrupted by noise by extracting the dimensions
corresponding to speech and removing those dimensions which correspond
to noise.

\sphinxAtStartPar
Note however that NMF\sphinxhyphen{}type methods extract only the power (or magnitude)
spectrum of the desired signal. In contrast, usually the input signal is
a time\sphinxhyphen{}frequency representation which has also a phase\sphinxhyphen{}component. After
application of NMF\sphinxhyphen{}estimation, we therefore need also an estimate of the
phase\sphinxhyphen{}component of the signal. Such methods will be discussed in the
\DUrole{xref,myst}{speech enhancement} chapter of this document.

\sphinxAtStartPar
For more information, see the Wikipedia article: \sphinxhref{https://en.wikipedia.org/wiki/Non-negative\_matrix\_factorization}{Non\sphinxhyphen{}negative matrix
factorization}.

\sphinxstepscope


\chapter{Evaluation of speech processing methods}
\label{\detokenize{Evaluation_of_speech_processing_methods:evaluation-of-speech-processing-methods}}\label{\detokenize{Evaluation_of_speech_processing_methods::doc}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Evaluation/Subjective_quality_evaluation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Subjective quality evaluation}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Evaluation/Objective_quality_evaluation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Objective quality evaluation}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Evaluation/Other_performance_measures::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Other performance measures}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Evaluation/Analysis_of_evaluation_results::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Analysis of evaluation results}}}}

\end{enumerate}

\sphinxstepscope


\section{Subjective quality evaluation}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:subjective-quality-evaluation}}\label{\detokenize{Evaluation/Subjective_quality_evaluation::doc}}
\sphinxAtStartPar
In speech processing applications where humans are the end\sphinxhyphen{}users, there
humans are also the ultimate measure of performance and quality.
\sphinxstyleemphasis{Subjective evaluation} refers to evaluation setups where human subjects
measure or quantify performance and quality. From a top\sphinxhyphen{}level
perspective the task is simple; subjects are asked to evaluate questions
such as
\begin{itemize}
\item {} 
\sphinxAtStartPar
Does X sound good?

\item {} 
\sphinxAtStartPar
How good does X sound?

\item {} 
\sphinxAtStartPar
Does X or Y sound better?

\item {} 
\sphinxAtStartPar
How intelligible is X?

\end{itemize}

\sphinxAtStartPar
However like always, the devil is in the details. Subjective evaluation
must be designed carefully such that questions address information which
is \sphinxstyleemphasis{useful} for measuring performance and such that information
extracted is \sphinxstyleemphasis{reliable} and \sphinxstyleemphasis{accurate}.

\sphinxAtStartPar
Most commonly, subjective evaluation in speech and audio refers to
\sphinxstyleemphasis{perceptual evaluation} of sound samples. In some cases evaluation can
also involve interactive elements, such as participation in a dialogue
over a telecommunication connection. In any case, perceptual evaluation
refers to evaluation through the subjects’ senses, which in this context
refers primarily to \sphinxstyleemphasis{hearing}. In other words, in a typical experiment
setup, subjects listen to sound samples and evaluate their quality.

\sphinxAtStartPar
\sphinxincludegraphics{{155472510}.jpg}

\sphinxAtStartPar
Photo by Anthony Brolin on Unsplash


\subsection{Aspects of quality}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:aspects-of-quality}}
\sphinxAtStartPar
Observe that the appropriate evaluation questions are tightly linked to
the application and context. For example, when designing a
teleconferencing system for business applications, we are interested in
entirely different types and aspects of quality than in design of
hearing\sphinxhyphen{}aids.

\sphinxAtStartPar
From a top\sphinxhyphen{}level perspective, we discuss quality for example, with
respect to
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{sound or speech quality}, relating to an intrinsic property of the
audio signal,

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{interaction and communication quality}, as the experience of
quality in terms of dynamic interaction,

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{service quality and user experience}, which is usually meant to
include beyond sound and interaction quality, the whole experience
of using a service or device, including responsiveness of system,
network coverage, user interface, visual and tactual design of
devices etc.

\end{itemize}


\subsubsection{Sound and speech quality}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:sound-and-speech-quality}}
\sphinxAtStartPar
The acoustic quality of the signal can be further described, for
example, through concepts such as
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{noisiness} or the amount of noise the speech signal is perceived to
have (perceptually uncorrelated noise)

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{distortion} describes how much parts of the speech signal are
destroyed (perceptually correlated noise), though often also
uncorrelated noises are referred to as distortions

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{intelligibility} is the level to which the meaning of the speech
signal can be understood

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{listening effort} refers to the amount of work a listener has to
use in listening to the signal and how much \sphinxstyleemphasis{listening fatigue}
and \sphinxstyleemphasis{annoyance} a user experiences

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{pleasantness} describes how much the speech signal annoys the
listener

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{resemblance} describes how close the speech signal is to the
original signal

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{naturalness} describes how natural an artificial speech source
sounds, often used as a last resort, when no other adjective feels
suitable, then we can see that “\sphinxstyleemphasis{It sounds aunnatural”}.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{acoustic distance} or how near or far the far\sphinxhyphen{}end speaker is
perceived to be, which is closely related to the amount of
reverberation the signal has (and how much delay the communication
path has)

\end{itemize}

\sphinxAtStartPar
Note that we can also think of intelligibility and listening effort as
separate aspects of sound quality. That is, we can perceive noisiness
and distortions, but still not be annoyed by the quality at all and be
able to listen to the sound effortlessly. Listening effort is usually a
prerequisite for loss of intelligibility; if we have to listen carefully
than it is exhausting in the long run. Quality, effort and
intelligibility are, in this sense, addressing different quality levels,
where intelligibility is an issue only at very bad quality, effort in
the middle range whereas quality is always relevant (see figure on the
right).


\subsubsection{Interaction quality}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:interaction-quality}}
\sphinxAtStartPar
The quality of interaction when using speech technology can be further
described, for example, through concepts such as
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{delay} describes the delay between the acoustic event at the
speakers end, to the time the event is perceived at the receiving
end. It is further divided into algorithmic delay caused by
processing, as well as delays caused by the transmission path.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{echo} is the feedback loop of sound in the sound system, where a
sound loudspeaker is picked by a microphone

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{presence or distance} is the feeling of proximity (and intimacy)
that a user experiences in communication

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{naturalness} can also here be used to describe communication with
the obvious meaning.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{175522003}.png}


\subsection{Choosing subjects \sphinxhyphen{} Naïve or expert?}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:choosing-subjects-naive-or-expert}}
\sphinxAtStartPar
When designing a subjective listening test, one of the first important
choices is whether listeners are naïve or expert listeners. With naïve
we refer to listeners who do not have prior experience in analytic
listening nor do they have other related expertise. Expert listeners are
then, obviously, subjects who are trained in the art of listening and
have the ability to analytically evaluate small differences in sounds
samples. Good expert listeners are typically researchers in the field
who have years of experience in developing speech and audio processing
and listening to sound samples. Some research labs even have their own
training for expert listeners.

\sphinxAtStartPar
Expert listeners are the best listeners in the sense that they can give
\sphinxstyleemphasis{accurate} and \sphinxstyleemphasis{repeatable} results. They can hear the smallest audible
differences and they can grade the differences \sphinxstyleemphasis{consistently}. In other
words, if an expert hears the same sounds in two tests, he or she can
give them exactly the same score both times, and (in an ideal world) his
or her expert colleagues will grade the sounds similarly. However, the
problem is that expert listeners have skills far beyond those of average
user of speech and audio products. If we want average users to enjoy our
products, then we should be measuring the preferences of average users.
It is uncertain whether the preferences of expert listeners align with
average users. For example, expert listeners might notice a highly
annoying distortion which an average user never discovers. The expert
would therefore be unable to enjoy the product which is great for the
average user.

\sphinxAtStartPar
Naïve listeners are therefore the best listeners in the sense that they
reflect best the \sphinxstyleemphasis{preferences of the average population}. The downside
is that since naïve listeners do not have experience in subjective
evaluation, their answers have usually a high variance (the measurement
is noisy). That is, if they hear the same samples twice, they are often
unable to repeat the same grade (intra\sphinxhyphen{}listener variance) and the
difference in grades between listeners is often high (inter\sphinxhyphen{}listener
variance). Consequently, to get useful results from naïve listeners, you
need a large number of subjects. To get statistically significant
results, you often need a minimum of \sphinxstyleemphasis{10 expert} listeners, where you
frequently need more than \sphinxstyleemphasis{50 naïve} listeners. The difficulty of the
task naturally can have a large impact on the required number of
subjects (difficult tasks require a larger number of listeners for both
expert and naïve listeners).

\sphinxAtStartPar
Note that the above definition leaves a large grey area between naïve
and expert listeners. A naïve listener looses the naïve status when
participating in a listening test. However, a naïve listener needs years
of training to become an expert listener. This is particularly
problematic in research labs, where there are plenty of young
researchers available for listening tests. They are not naïve listeners
any more, but they need training to become expert listeners. A typical
approach is then to include the younger listeners regularly in listening
tests, but remove those listeners in post\sphinxhyphen{}screening if their inter\sphinxhyphen{} or
intra\sphinxhyphen{}listener variance is too large. That way the listeners slowly gain
experience, but their errors do not get too much weight in the results.
This approach however has to be very clearly monitored such that it does
not lead to tampering of results (i.e. scientific misconduct). Any
listeners removed in post\sphinxhyphen{}screening and the motivations for
post\sphinxhyphen{}screening have to therefore be documented accurately.


\subsection{Experimental design}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:experimental-design}}
\sphinxAtStartPar
To get accurate results from an experiment, we have to design the
experiment such that it matches the performance qualities we want to
quantify. For example, in an extreme case, an evaluation of the
performance of noise reduction can be hampered, if a sound sample
features a speaker whose voice is annoying to the listeners. The
practical questions are however more nuanced. We need to consider for
example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
To which extent does the language of speech samples affects
listening test results? Can naïve listeners grade accurately speech
samples in a foreign language? Does the text\sphinxhyphen{}content of speech
samples affect grading: for example, if the spoken text is
politically loaded, would the grades of politically left\sphinxhyphen{} and
right\sphinxhyphen{}leaning subjects give different scores?

\item {} 
\sphinxAtStartPar
Does the text material, range of speakers and recording conditions
reflect the target users and environments? For example, if we test a
system with English\sphinxhyphen{}speaking listeners with speech samples in
English, do the results reflect performance for Chinese users? Are
all phonemes present in the material and do they appear equally
often as they do in the target languages? Does the material feature
background noises and room acoustics in the same proportion as
real\sphinxhyphen{}world scenarios? Does gender of the speaker or listener play a
role? Or their cultural background?

\item {} 
\sphinxAtStartPar
Is the subject learning from previous sounds, such that the answers
regarding the current sound are different from the previous sound?
That is, if the subject hears the same sound with different
distortions several times, then he already knows how the sound is
supposed to sound like. Then perhaps he evaluates distortions
differently, because he know how the sound is supposed to sound
like.

\end{itemize}

\sphinxAtStartPar
To take into account such considerations, experiments can be designed in
different ways, for example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
We can measure \sphinxstyleemphasis{absolute} quality (How good is X?), or \sphinxstyleemphasis{relative}
quality (How good is X in comparison to Y?) or we can \sphinxstyleemphasis{rank} samples
(“Which one is better, A or B?” or “Order samples A, B and C from
best to worst.”). Clearly absolute quality is often the most
important quality for users, since usually users do not have the
opportunity to test products side by side. However, for example
during development, two version of an algorithm could have very
similar quality, such that it would be difficult to determine
preference with an absolute quality measure. Relative quality
measures then give more detailed information, explicitly quantifying
the difference in performance. Ranking samples is usually used in
competitions, for example, when a company wants to choose a supplier
for a certain product, it is then useful to be able to measure the
ordering of products. It is thus more refined than relative measures
in terms of finding which one is better, but at the same time, it
does not say how large the difference is between particular samples.

\item {} 
\sphinxAtStartPar
In many applications, the target is to recover a signal after
transmission or from a noisy recording. The objective is thus to
obtain a signal which is as close as possible to the original
signal. It can then be useful to play the original signal to
subjects as a \sphinxstyleemphasis{reference}, such that they can compare performance
explicitly with the target signal. While this then naturally gives
listeners the opportunity to make more accurate evaluations, it is
also not realistic. In real life, we generally do not have access to
the original; for example, when speaking on the phone, we cannot
directly hear the original signal, but can hear only the transmitted
signal. We would therefore never be able to compare performance to
the target, but only absolute quality.In some cases it is also possible that some \DUrole{xref,myst}{speech
enhancement} methods improve quality such that
the output sounds better than the original non\sphinxhyphen{}distorted sound!

\item {} 
\sphinxAtStartPar
In some experimental designs, the subject can listen to sounds many
times, even in a loop. That way we make sure that the subject hears
all the minute details. However, that is unrealistic since in a real
scenario, like a telephone conversation, you usually can hear sounds
only once. Repeated sounds are therefore available only for expert
listeners.

\item {} 
\sphinxAtStartPar
In choice of samples, the length of samples should be chosen with
care. Longer samples reflect better real\sphinxhyphen{}life situations, but bring
many problems, for example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The ability to listeners to remember particular features of the
sound, especially in comparison to other sounds, is very
limited. This would reduce the accuracy of results.

\item {} 
\sphinxAtStartPar
Longer samples can have multiple different characteristics,
which would warrant a different score. The listener would then
have to perform a judgement; which part of the sentence or
sample is more important, and which type of features are more
important for quality? This can lead to ambiguous situations.

\end{itemize}

\sphinxAtStartPar
Listening to very short samples can, in turn, make features of the
sound audible which a listener could not hear in real\sphinxhyphen{}life setting.

\item {} 
\sphinxAtStartPar
Usually we prefer to have speech samples in the same language as the
listeners. With expert listeners this constraint might not be so
strict.

\item {} 
\sphinxAtStartPar
Speech samples should generally be \sphinxstyleemphasis{phonetically balanced}
“nonsense” sentences. With phonetically balanced, we refer to
sentences where all phonemes appear with the same frequency as they
appear on average in that particular language. With nonsense
sentences, we refer to text content which does not convey any
particular, loaded or surprising meaning. For example, “An apple on
the table” is a good sentence in the sense that it is grammatically
correct and there is nothing strange with it. Examples of bad
sentences would be “Elephants swimming in champagne”, “Corporations
kill babies” and “The dark scent of death and mourning”.

\item {} 
\sphinxAtStartPar
When playing samples to subjects, they will both \sphinxstyleemphasis{learn} more about
the samples, but also experience \sphinxstyleemphasis{fatigue}. Especially for naïve
listeners, the performance of subjects will therefore change during
an experiment. It is very difficult to take such changes in
performance into account in analysis and it is therefore usually
recommended to randomize the ordering of samples separately for each
listener. That way the effects of learning and fatigue will be
dispersed evenly across all samples, such that they have a uniform
effect on all samples.

\item {} 
\sphinxAtStartPar
To measure the ability of listeners to consistently grade samples,
it is common practice to include items in the test whose answers are
known. For example, we can
\begin{itemize}
\item {} 
\sphinxAtStartPar
repeat a sample twice, such that we measure the listeners
ability to give the same grade twice,

\item {} 
\sphinxAtStartPar
have the original \sphinxstyleemphasis{reference} signal hidden among test samples
(known as the \sphinxstyleemphasis{hidden reference}), such that we can measure the
listeners ability to give the perfect score to the perfect
sample,

\item {} 
\sphinxAtStartPar
include samples with known distortions among the test items,
such that we can compare results with prior experiments which
included the same known samples. Typically such known
distortions include for example low\sphinxhyphen{}pass filtered versions of
the original signal. Such samples are known as \sphinxstyleemphasis{anchors} and low
and high quality anchors are then respectively known as
\sphinxstyleemphasis{low\sphinxhyphen{}anchor} and \sphinxstyleemphasis{high\sphinxhyphen{}anchor}.

\end{itemize}

\end{itemize}


\subsection{Some use cases}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:some-use-cases}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{During research and development} of speech and audio processing
methods, researchers have to evaluate the performance of their
methods. Most typically such evaluations are quite informal; when
you get output from a new algorithm, the first thing to do is to
listen to the output \sphinxhyphen{} is it any good? In some stages of
development, such evaluations are an ongoing process; tweak a
parameter and listen how it affects the output. In early
development, listening is in practice also a sanity check;
programming errors often cause bad distortions on the output, which
can be caught by listening.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{When publishing results} and \sphinxstyleemphasis{at later stages of development}, it
is usually necessary to evaluate quality in a more formal manner.
The engineer developing an algorithm is not a good listener, because
he has extremely detailed knowledge about the performance and could
often spot his or her own method, from a set of sound samples. The
developer is therefore \sphinxstyleemphasis{biased} and not a reliable listener.Therefore, when publishing results we need \sphinxstyleemphasis{reproducible}
experiments in the sense that if another team would repeat the
listening experiment, then they could draw the same conclusions.
Listening tests therefore have to have a sufficient number of
listeners (expert or naïve) such that the outcome is statistically
significant (see {\hyperref[\detokenize{Evaluation/Analysis_of_evaluation_results::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Analysis of evaluation
results}}}}).

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{When selecting a product among competing candidates} we would like
to make a good evaluation. The demands are naturally very different
depending on the scenario; 1) if you want to choose between Skype,
Google and Signal for your personal VoIP calls during a visit
abroad, you are probably content with informal listening. 2) If on
the other hand, you are an engineer and assigned with the task of
choosing a codec for all VoIP calls within a multi\sphinxhyphen{}national company,
then you probably want to do a proper formal listening test.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Monitoring quality of in\sphinxhyphen{}production system}; The quality of a
running system can abruptly or gradually change due to bugs and
equipment\sphinxhyphen{}failures, including memory\sphinxhyphen{}errors. To detect such errors,
we need to monitor quality. Often such monitoring is based on
automated objective tests.

\end{itemize}


\subsection{Frequently used standards and recommendations for quality evaluation}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:frequently-used-standards-and-recommendations-for-quality-evaluation}}

\subsubsection{Expert listeners}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:expert-listeners}}\begin{itemize}
\item {} 
\sphinxAtStartPar
By far the most commonly used standard applied with expert listeners
is known as \sphinxhref{https://en.wikipedia.org/wiki/MUSHRA}{MUSHRA, or MUltiple Stimuli with Hidden Reference and
Anchor}, defined by
\sphinxhref{https://en.wikipedia.org/wiki/ITU-R}{ITU\sphinxhyphen{}R} recommendation
\sphinxhref{https://www.itu.int/rec/R-REC-BS.1534/en}{BS.1534\sphinxhyphen{}3}. It offers
direct comparison of multiple target samples, with a reference
signal, hidden reference and anchor. Users can switch between
samples on the fly and many interfaces also allow looping short
segments of the signal.Each sample is rated on an integer scale 1\sphinxhyphen{}100.MUSHRA is very useful for example when publishing results, because
it is simple to implement and well\sphinxhyphen{}known. Open source
implementations such as
\sphinxhref{https://www.audiolabs-erlangen.de/resources/webMUSHRA}{webMUSHRA}
are available.Practical experience have shown that MUSHRA is best applied for
intermediate quality samples, where a comparison of 2\sphinxhyphen{}5 samples is
desired. Moreover, a suitable length of sound samples ranges from 2
to approximately 10 seconds. Furthermore, if the overall length of a
MUSHRA test is more than, say, 30 minutes, then the fatigue of
listeners starts be a significant problem. With 10 good listeners it
is usually possible to achieve statistically significant results,
whereas 6 listeners can be sufficient for informal tests (e.g.
during testing). These numbers should not be taken as absolute, but
as practical guidance to give a rough idea of what makes a usable
test.MUSHRA is however often misused by omitting the anchors; a valid
argument for omitting anchors is that if the distortions in the
target samples are of a very different type then the typical
anchors, then anchors do not provide an added value. Still, such
omissions are not allowed by the MUSHRA standard.

\item {} 
\sphinxAtStartPar
For very small impairments in audio quality, Recommendation ITU\sphinxhyphen{}R
BS.1116\sphinxhyphen{}3 (ABC/HR) is recommended instead of MUSHRA (see
\sphinxurl{https://www.itu.int/rec/R-REC-BS.1116-3-201502-I/en}).

\end{itemize}

\sphinxAtStartPar
For illustrations and examples of the MUSHRA test, see
\sphinxurl{https://www.audiolabs-erlangen.de/resources/webMUSHRA}


\subsubsection{Naïve listeners}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:naive-listeners}}
\sphinxAtStartPar
\sphinxstylestrong{P.800} is the popular name of a set of listening tests defined in the standard
\sphinxhref{https://www.itu.int/en/ITU-T/Pages/default.aspx}{ITU\sphinxhyphen{}T}\sphinxhref{https://www.itu.int/rec/T-REC-P.800-199608-I/en}{Recommendation P.800 “Methods for subjective determination of transmission quality”}. It is intended to be a test which gives as realistic results as possible, by assessing performance in setups which resemble real use\sphinxhyphen{}cases.    The most significant consequences are that P.800 focuses on naïve    listeners and, since telecommunication devices are typically    hand\sphinxhyphen{}held over one ear, P.800 mandates tests with headphones which    are held only on one ear.      To make the test simpler for naïve listeners, P.800 most typically    uses an integer scale 1\sphinxhyphen{}5 known as \sphinxhref{https://en.wikipedia.org/wiki/Mean\_opinion\_score}{mean opinion score (MOS)}. Each grade    is given a characterisation such as


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Rating
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Label
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
Excellent
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
Good
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
Fair
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
Poor
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
Bad
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
This makes the ratings more concrete and easier to understand for users. A downside of labelling the ratings is that such labels are   specific to each language and the MOS scores given in different    languages might thus not be directly comparabale. Who is to know    whether \sphinxstyleemphasis{excellent}, \sphinxstyleemphasis{erinomainen}, \sphinxstyleemphasis{ممتاز}, and \sphinxstyleemphasis{маш сайн} mean exactly     the same thing? (Those are english, finnish, arabic and mongolian,    in case you were wondering.)

\sphinxAtStartPar
P.800 is further split into
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Conversation opinion tests}, where participants grade the        quality after \sphinxstyleemphasis{using} telecommunication system for a        conversation. Typically the question posed to participants is        “Opinion of the connection you have just been using: Excellent,        God, Fair, Poor, Bad”. An alternative is “Did you or your        partner have any difficulty in talking or hearing over the        connection? Yes/No”.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Listening opinion tests}, where participants grade the quality        after \sphinxstyleemphasis{listening} to the output of a telecommunication system.

\end{itemize}

\sphinxAtStartPar
The grading of listening opinion tests can, more specifically, be    one of the following:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Absolute category rating (ACR)}, where the above MOS scale is        used to answer questions like “How good is system X?”

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Degradation category rating (DCR)}, where the objective is to        evaluate the amount of degradation caused by some processing.        Samples are preseted to listeners by pairs (A\sphinxhyphen{}B) or repeated        pairs (A\sphinxhyphen{}B\sphinxhyphen{}A\sphinxhyphen{}B) where A is the quality reference and B the        degraded sample. Rating labels cane be for example

\end{itemize}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Rating
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Label
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
Degradation is inaudible
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
Degradation is audible but not annoying
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
Degradation is slightly annoying
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
Degradation is annoying
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
Degradation is very annoying
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Comparison category rating (CCR)}, is similar to DCR, but such        that the processed sample B can be also better than A. Rating        labels can then be for example

\end{itemize}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Rating
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Label
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
Much better
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
Better
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
Slightly better
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
About the same
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
Slightly worse
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{}2
&
\sphinxAtStartPar
Worse
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{}3
&
\sphinxAtStartPar
Much worse
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{P.804} Subjective diagnostic test method for conversational speech
quality analysis

\sphinxAtStartPar
\sphinxstylestrong{P.805} Subjective evaluation of conversational quality

\sphinxAtStartPar
\sphinxstylestrong{P.806} A subjective quality test methodology using multiple rating
scales

\sphinxAtStartPar
\sphinxstylestrong{P.807} Subjective test methodology for assessing speech
intelligibility

\sphinxAtStartPar
\sphinxstylestrong{P.808} Subjective evaluation of speech quality with a crowdsourcing
approach

\sphinxAtStartPar
\sphinxstylestrong{P.835} Subjective test methodology for evaluating speech
communication systems that include noise suppression algorithm

\sphinxAtStartPar
And many more, see \sphinxurl{https://www.itu.int/rec/T-REC-P/en}

\sphinxAtStartPar
Holding a phone on one ear

\sphinxAtStartPar
\sphinxincludegraphics{{155472516}.jpg}Photo by
Fezbot2000 on Unsplash


\subsection{Intelligibility testing}
\label{\detokenize{Evaluation/Subjective_quality_evaluation:intelligibility-testing}}
\sphinxAtStartPar
When a speech signal has a been corrupted by a considerable level of
noise and/or reverberation, it’s intelligibility starts to deteriorate.
We might miss\sphinxhyphen{}interpret or \sphinxhyphen{}understand words or entirely miss them.
Observe that the level of distortion is quite a bit higher than what we
usually consider in quality\sphinxhyphen{}tests.

\sphinxAtStartPar
Typically we have to choose between correctly interpreted words or
phonemes/letters. For example, if the sentence is
\begin{quote}

\sphinxAtStartPar
How to recognize speech?
\end{quote}

\sphinxAtStartPar
and what we hear is
\begin{quote}

\sphinxAtStartPar
How to wreck a nice beach?
\end{quote}

\sphinxAtStartPar
then we can count correct words for example as

\begin{sphinxVerbatim}[commandchars=\\\{\}]
How to recognize    speech?
How to wreck a nice beach?
       S     I I    S
\end{sphinxVerbatim}

\sphinxAtStartPar
where we use the notation S \sphinxhyphen{} substitution, I \sphinxhyphen{} insertion, D \sphinxhyphen{} deletion.
The \sphinxhref{https://en.wikipedia.org/wiki/Word\_error\_rate}{word error rate}
would then be
\begin{equation*}
\begin{split} WER = 100\times\frac{S+D+I}{N} \end{split}
\end{equation*}
\sphinxAtStartPar
where \(N\) is the total number of words. In the above example we thus have
WER = 100\%.

\sphinxAtStartPar
If we would go letter by letter, than instead we would have

\begin{sphinxVerbatim}[commandchars=\\\{\}]
How to  rec..og.nize speech
How to wreck a  nice  beach
       I     SI   S  DS S
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we would then define the letter error rate as
\begin{equation*}
\begin{split} LER = 100\times\frac{S+D+I}{N} \end{split}
\end{equation*}
\sphinxAtStartPar
where \(N\) is the total number of letters. The value in the above example
would then be 33\%.

\sphinxAtStartPar
It is clear that word error rate is thus much more strict than letter
error rate. A single incorrect letter will ruin a word, while the letter
error rate is affected much less. WER is much easier to compute than the
LER and also leads to fewer ambiguous situations. It is however
dependent on the application which measure is better suited.

\sphinxAtStartPar
Observe that both word and error rates are applicable as both objective
measures, in speech recognition experiments, as well as a subjective
measure, where human subjects evaluate the quality of sounds.

\sphinxstepscope


\section{Objective quality evaluation}
\label{\detokenize{Evaluation/Objective_quality_evaluation:objective-quality-evaluation}}\label{\detokenize{Evaluation/Objective_quality_evaluation::doc}}

\subsection{Objective estimators for perceptual quality}
\label{\detokenize{Evaluation/Objective_quality_evaluation:objective-estimators-for-perceptual-quality}}
\sphinxAtStartPar
With “objective evaluation” we usually refer to \sphinxstyleemphasis{estimators of
perceptual quality}, where the objective is to predict the mean output
of a {\hyperref[\detokenize{Evaluation/Subjective_quality_evaluation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{subjective listening}}}} test using an
algorithm. That is, we want a computer to listen to a sound sample and
try to “guess” what a human listener would say about its quality (on
average).

\sphinxAtStartPar
It is then clear that {\hyperref[\detokenize{Evaluation/Subjective_quality_evaluation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{\sphinxstyleemphasis{subjective
evaluation}}}}} is always the “true” measure
of performance and objective evaluation is an approximation thereof. In
this sense, subjective evaluation is “better”. However, there are many
good reasons to use objective instead of subjective evaluation:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Subjective evaluation is expensive}; a test requires that a large
number of persons listens to sound samples, which is both
time\sphinxhyphen{}consuming and requires infrastructure. Objective evaluation is
performed on a computer, such that you can generally test a large
number of sound samples in a short time.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Subjective evaluation is noisy}; even with a large number of expert
listeners it is generally difficult to get exactly the same result
in two consecutive tests. Objective evaluation always gives the same
rating for the same input, such that testing is consistent and
reliable. This is especially important for scientific
reproducibility; an independent laboratory can verify and confirm
your results, the objective measure always gives the same output.
With subjective evaluation, independent researchers can get
different results, and you can never be 100\% certain where the
difference in results comes from. Did one of the researchers do an
error or is it just that subjective listeners give always slightly
different results?

\end{itemize}

\sphinxAtStartPar
Some of the most frequently used objective measures include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/PESQ}{PESQ} is probably the most
frequently used objective evaluation method and it is defined in
\sphinxhref{https://www.itu.int/rec/T-REC-P.862/en}{ITU\sphinxhyphen{}T Recommendation P.862: Perceptual evaluation of speech quality
(PESQ): An objective method for end\sphinxhyphen{}to\sphinxhyphen{}end speech quality assessment
of narrow\sphinxhyphen{}band telephone networks and speech
codecs} (2001). It is thus
an evaluation method designed explicitly for telecommunications
applications. It estimates the mean score of an P.800 ACR test.PESQ accepts only narrow\sphinxhyphen{}band input and is \sphinxstyleemphasis{not directly applicable}
on other bandwidths. The degradation types whose effect PESQ can
reliably predict are
\begin{itemize}
\item {} 
\sphinxAtStartPar
Speech input levels to a codec

\item {} 
\sphinxAtStartPar
Transmission channel errors

\item {} 
\sphinxAtStartPar
Packet loss and packet loss concealment with CELP codecs

\item {} 
\sphinxAtStartPar
Bit rates if a codec has more than one bit\sphinxhyphen{}rate mode

\item {} 
\sphinxAtStartPar
Transcodings

\item {} 
\sphinxAtStartPar
Environmental noise at the sending side

\item {} 
\sphinxAtStartPar
Effect of varying delay in listening only tests

\item {} 
\sphinxAtStartPar
Short\sphinxhyphen{}term time warping of audio signal

\item {} 
\sphinxAtStartPar
Long\sphinxhyphen{}term time warping of audio signal

\end{itemize}

\sphinxAtStartPar
Observe that distortions other than those listed above can provide
unreliable results. An important missing feature are distortions
caused by spectral processing, such as musical noise. Specifically,
for example, using PESQ to evaluate \DUrole{xref,myst}{speech
enhancement} methods based on processing in the
\DUrole{xref,myst}{STFT} domain, \sphinxstyleemphasis{can give unreliable
results}.

\item {} 
\sphinxAtStartPar
Perceptual Objective Listening Quality Assessment
(\sphinxhref{https://en.wikipedia.org/wiki/POLQA}{POLQA}) is the
successor of PESQ and defined in \sphinxhref{http://www.itu.int/rec/T-REC-P.863/en}{ITU\sphinxhyphen{}T Recommendation P.863:
Perceptual objective listening quality
assessment}. It is important
to notice that for most practical purposes, POLQA is better than
PESQ. It has a wider range of applications and acceptable
degradation types and the output is more reliable. However, from a
scientific perspective it is extremely regrettable that
implementations of POLQA are commercial and \sphinxstyleemphasis{expensive} products,
rendering application of POLQA infeasible in normal scientific work.
Even if an individual team could afford purchasing a POLQA licence,
verification of POLQA results by independent research labs is
possible only if they also purchase a POLQA licence. Despite of its
limitations, PESQ has therefore remained the scientific standard in
objective evaluation of speech.

\item {} 
\sphinxAtStartPar
Perceptual Evaluation of Audio Quality
(\sphinxhref{https://en.wikipedia.org/wiki/PEAQ}{PEAQ}) evaluates,
instead of only speech, also other types of audio samples. It is
therefore less accurate with respect to distortions specific to
speech signals, but it generalizes better to other audio such as
music and background noises. The measure is defined in
ITU\sphinxhyphen{}R
Recommendation BS.1387: Method for objective measurements of
perceived audio quality (PEAQ).

\item {} 
\sphinxAtStartPar
The \sphinxhref{https://ieeexplore.ieee.org/document/5713237}{short\sphinxhyphen{}term objective intelligibility
(STOI)} measure
focuses on how \sphinxstyleemphasis{intelligible} a speech sample is. It is thus clearly
focused on lower\sphinxhyphen{}quality scenarios where speech is so badly
corrupted that it is hard to understand what is said. Like all
objective measures, it is not a completely reliable estimate of
quality, but can be useful in combination with other measures. A
good feature of STOI is that an \sphinxhref{http://amtoolbox.sourceforge.net/amt-0.9.5/doc/speech/taal2011\_code.php}{implementation is
available}.

\end{itemize}


\subsection{Other objective performance criteria}
\label{\detokenize{Evaluation/Objective_quality_evaluation:other-objective-performance-criteria}}
\sphinxAtStartPar
There are many cases where other performance criteria are well\sphinxhyphen{}warranted
than merely prediction of subjective listening test results. Most
typically these criteria are applied when there is no user involved,
such as speech recognition, or, when we want to have more detailed
characterization of performance than given by predictors of subjective
listening test results.

\sphinxAtStartPar
Some examples of such performance criteria include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{\sphinxhref{https://en.wikipedia.org/wiki/Word\_error\_rate}{Word error rate
(WER)}} is used in
speech recognition to measure the proportion of words correctly
recognized from a test signal.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{\sphinxhref{https://en.wikipedia.org/wiki/Signal-to-noise\_ratio}{Signal to noise ratio
(SNR)}} is used
to measure the proportion of the desirable speech signal and
undesirable noise components (which includes for example background
noises, distortions caused by processing algorithms and
transmission, as well as undesirable competing speakers).

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Perceptual signal to noise ratio (pSNR)} measures SNR in a
perceptually motivated domain. Essentially distortions are weighted
such that they approximately correspond to human perception. This is
similar to the above predictors of subjective listening tests, but
works also on small segments of speech. It can be used to for
detailed analysis of distortions to, for example, which parts of the
signal contain undesirable distortions.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{The speech distortion index (SDI)} measures the amount by which a
desirable speech signal is distorted. In \DUrole{xref,myst}{speech
enhancement}, it is often used in combination
with the \sphinxstyleemphasis{noise attenuation factor} (NAF), which measures the amount
by which undesirable noises are removed. It is clear that by doing
nothing, we obtain a perfect SDI and by setting the output to zero,
we obtain a perfect NAF. Neither outcome is usually satisfactory. It
is therefore usually not clear what the right balance between the
two measures are.

\item {} 
\sphinxAtStartPar
Unweighted and weighted average recall (UAR, WAR) are often used to
measure performance in speech classification tasks, such as
classifying a speech segment into one of finite number of possible
emotions. UAR is defined as the mean of class\sphinxhyphen{}specific recalls (the
proportion of class samples recognized correctly) while WAR is the
overall proportion of samples recognized correctly across all
classes (sometimes also referred to as \sphinxstyleemphasis{accuracy}). UAR is often
preferred over WAR in experiments where there is a notable class
imbalance in the test data, and where it is important to have
systems that are also sensitive to the less\sphinxhyphen{}frequent classes.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Receiver\_operating\_characteristic}{Receiver operating characteristic (ROC)
curves}
and its derivatives such as \sphinxhref{https://en.wikipedia.org/wiki/Receiver\_operating\_characteristic\#Area\_under\_the\_curve}{area under the curve
(AUC)}
or equal error rate (EER) are often used to report performance of
systems that have some type of detection threshold that can be
varied, and when performance for each threshold value is measured in
terms of \sphinxhref{https://en.wikipedia.org/wiki/Precision\_and\_recall}{precision and
recall}. For
instance, performance of speaker verification systems is often
evaluated using such metrics.

\end{itemize}

\sphinxstepscope


\section{Other performance measures}
\label{\detokenize{Evaluation/Other_performance_measures:other-performance-measures}}\label{\detokenize{Evaluation/Other_performance_measures::doc}}

\subsection{Computational Complexity}
\label{\detokenize{Evaluation/Other_performance_measures:computational-complexity}}
\sphinxAtStartPar
On an application level, speech processing algorithms usually are used
in low\sphinxhyphen{}resource devices like mobile phones. Mobile devices have limited
computational capabilities and, in order to preserve their battery, it
is important to design efficient algorithms for them. There are multiple
ways of analyzing the computational complexity of an algorithm depending
on the stage of the design process or the purpose of the final
application:


\subsubsection{Big\sphinxhyphen{}O notation:}
\label{\detokenize{Evaluation/Other_performance_measures:big-o-notation}}
\sphinxAtStartPar
The complexity of an algorithm is usually understood as a measurement of
the time that an algorithm would take to complete, given an input of
size n. When the size of the input grows, the computing time should
remain within a practical bound. For this reason, complexity is measured
asymptotically as n approaches infinity. The most popular representation
of algorithmic complexity is the Big\sphinxhyphen{}O notation. The Big\sphinxhyphen{}O notation
gives an upper bound to the growth of the computing time of an
algorithm. This proves especially useful, because this notation allows
us to compare algorithms in worst case scenarios. Figure 1 shows the
growth rate of different Big\sphinxhyphen{}O notations with respect to the input size.

\sphinxAtStartPar
For example, a complexity of O(n), read as “O n complexity’, represents
an algorithm whose computation time grows linearly with the input size.
Some examples of every type of complexity are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
O(1) \sphinxhyphen{} The computation time does not grow with the size of the
input:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Accessing an array index (a = array{[}4{]}).

\end{itemize}

\item {} 
\sphinxAtStartPar
O(log n) \sphinxhyphen{} The computation time grows with the logarithm of the
input size:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Binary Search algorithms.

\end{itemize}

\item {} 
\sphinxAtStartPar
O(n) \sphinxhyphen{} The computation time grows linearly with the input size:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Traversing an array.

\item {} 
\sphinxAtStartPar
Comparing two strings.

\end{itemize}

\item {} 
\sphinxAtStartPar
O(n2) \sphinxhyphen{} The computation time grows with the square of the
input size:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Matrix operations like traversing a 2D array or multiplying a 1D
array with a 2D matrix.

\item {} 
\sphinxAtStartPar
Conventional Discrete Fourier Transform (Matrix
multiplication).

\end{itemize}

\item {} 
\sphinxAtStartPar
O(n log n) \sphinxhyphen{} The “log n” term is added when O(n2)
algorithms are performed with Divide and Conquer techniques to
increase their efficiency.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Fast Fourier Transform.

\end{itemize}

\item {} 
\sphinxAtStartPar
O(2n) \sphinxhyphen{} The computation time doubles with each addition
to the input data, therefore it grows exponentially:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Recursive algorithms → To solve a problem of size N, it is
necessary to solve two problems of size N \sphinxhyphen{} 1.

\end{itemize}

\item {} 
\sphinxAtStartPar
O(n!) \sphinxhyphen{} Factorial growth represents algorithms that grow even faster
than exponential examples:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Finding all the possible permutations of a list.

\end{itemize}

\end{itemize}


\subsubsection{Weighted Million Operations Per Second (WMOPS),   ITU\sphinxhyphen{}T. Software tool library: User’s manual, 2009 }
\label{\detokenize{Evaluation/Other_performance_measures:weighted-million-operations-per-second-wmops-itu-t-software-tool-library-users-manual-2009}}
\sphinxAtStartPar
The Big\sphinxhyphen{}O notations gives us an intuitive idea of the complexity of
specific algorithms. This allows us to compare which algorithm to use
and choose the most efficient option. However, in applications like
speech coding, it is important to know the exact number of operations
that the system needs to perform in order to process each frame of
audio.

\sphinxAtStartPar
The ITU\sphinxhyphen{}T provides guidelines to measure the number of operations in a
program. This measurement takes into account that not all the operations
have the same computational load and scales their values accordingly.
For example, a logarithm is a much heavier operation than an addition.
The final result is represented as Weighed Million Operations Per Second
(WMOPS). Table 1 shows the weights used for each specific operation
carried out.

\sphinxAtStartPar
\sphinxincludegraphics{{175510471}.png}

\sphinxAtStartPar
\sphinxstylestrong{Figure 1:} Evolution of computation time for multiple Big\sphinxhyphen{}O notations
dependint on the input size.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline

\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
Operation
&
\sphinxAtStartPar
Example
&
\sphinxAtStartPar
Weight
\\
\hline
\sphinxAtStartPar
Addition
&
\sphinxAtStartPar
a = b + c
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
Multiplication
&
\sphinxAtStartPar
a = b ∗ c
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
Multiplication + addition
&
\sphinxAtStartPar
a+ = b ∗ c
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
Move
&
\sphinxAtStartPar
a = b
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
Store in array
&
\sphinxAtStartPar
a{[}i{]} = b{[}i{]} + c{[}i{]}
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
Logical
&
\sphinxAtStartPar
AND, OR, etc.
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
Shift
&
\sphinxAtStartPar
a = b >> c
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
Branch
&
\sphinxAtStartPar
if, if…else
&
\sphinxAtStartPar
4
\\
\hline
\sphinxAtStartPar
Division
&
\sphinxAtStartPar
a = b/c
&
\sphinxAtStartPar
18
\\
\hline
\sphinxAtStartPar
Square\sphinxhyphen{}root
&
\sphinxAtStartPar
a = sqrt(b)
&
\sphinxAtStartPar
10
\\
\hline
\sphinxAtStartPar
Transcendental
&
\sphinxAtStartPar
sine, arctan, etc.
&
\sphinxAtStartPar
25
\\
\hline
\sphinxAtStartPar
Function call
&
\sphinxAtStartPar
a = func(b, c, d)
&
\sphinxAtStartPar
2 + number of arguments passed and returned
\\
\hline
\sphinxAtStartPar
Loop initialization
&
\sphinxAtStartPar
for(i=0;i
&
\sphinxAtStartPar
3
\\
\hline
\sphinxAtStartPar
Indirect addressing
&
\sphinxAtStartPar
a = b.c
&
\sphinxAtStartPar
2
\\
\hline
\sphinxAtStartPar
Pointer initialization
&
\sphinxAtStartPar
a{[}i{]}
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
Exponential
&
\sphinxAtStartPar
pow, en
&
\sphinxAtStartPar
25
\\
\hline
\sphinxAtStartPar
Logarithm
&
\sphinxAtStartPar
log
&
\sphinxAtStartPar
25
\\
\hline
\sphinxAtStartPar
Conditional test
&
\sphinxAtStartPar
used in conjunction with BRANCH
&
\sphinxAtStartPar
2
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxstylestrong{Table 1:} Operations accounted by the WMOPS tool and their relative
weight.

\sphinxstepscope


\section{Analysis of evaluation results}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:analysis-of-evaluation-results}}\label{\detokenize{Evaluation/Analysis_of_evaluation_results::doc}}
\sphinxAtStartPar
To determine the quality and performance of speech processing methods,
we often use {\hyperref[\detokenize{Evaluation/Subjective_quality_evaluation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{subjective}}}} and {\hyperref[\detokenize{Evaluation/Objective_quality_evaluation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{objective
evaluation}}}} methods. These methods however
give an array of results, one result for each sound sample for objective
methods, and in the case of subjective listening tests, one result for
each sounds sample per listener. Invariably, the results are noisy in
the sense that each sound sample and each listener will give a different
result. How do we then determine whether one method is better than the
other? In short, we have to perform statistical analysis of results. The
types of question we can answer with statistical analysis include:
“Based on measurement results, which result is most probable, that A is
better than B, that B is better than A, or is it impossible to
determine?”


\subsection{Informal analysis}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:informal-analysis}}
\sphinxAtStartPar
In practical laboratory work, after an experiment, you would always like
to get a first impression of the results as quickly as possible. With
informal analysis, we here refer to quick\sphinxhyphen{}and\sphinxhyphen{}dirty evaluation of data
to determine whether there it is worth investing time in a more formal
analysis. That is, if we already in the informal analysis determine that
our new method is not really better than past methods, then it more
productive to improve the method rather than perform a detailed analysis
of results. On the other hand, a detailed analysis can sometimes reveal
effects which are not visible in informal analysis and which could be
used to improve the method in question. In any case, when reporting
results (in a publication or even just to your superior), informal
analysis should never replace properly applied statistical tests.
Informal tests just given an indication of how much work will be
required for proper tests.


\subsubsection{Example 1}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:example-1}}
\sphinxAtStartPar
Suppose we want to compare methods A and B, and we have already applied
PESQ on the outputs of both methods for 100 speech samples. How do we
know if A is better than B?

\sphinxAtStartPar
\sphinxstylestrong{First idea:} Calculate the mean and standard deviation of both
measurements. Which method has a higher (better) mean? Is the standard
deviation clearly smaller than the difference in means?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Suppose the mean of PESQ scores for A and B are 4.5 and 3.1, and
their corresponding standard deviations are 0.2 and 0.15. Clearly A
has a higher (better) PESQ score than B. The standard deviations are
small which indicates that the measurement rarely diverge far from
the mean. \sphinxstyleemphasis{This is the best case scenario} but unfortunately it
happens rarely in practice.

\item {} 
\sphinxAtStartPar
Suppose the mean of PESQ scores for A and B are 3.5 and 3.1, and
their corresponding standard deviations are 0.2 and 0.15.  Clearly
\sphinxstyleemphasis{the mean} PESQ score is higher (better) for A than B. In this case
it is however not immediately clear whether we can draw any definite
conclusions. Is the difference significant? The standard deviations
are not much smaller than the difference between the means,
indicating that sometimes A could be worse than B! \sphinxstyleemphasis{Further analysis
is required.}

\end{itemize}

\sphinxAtStartPar
Note that here we have chosen to discuss the \sphinxstyleemphasis{standard deviations},
although we could equivalently present the \sphinxstyleemphasis{variances} of measurements.
The benefit of the standard deviation is that it is expressed in the
same units as the mean, such that we can directly get an intuitive
impression of the magnitude of the differences in means in comparison to
the standard deviations.


\subsubsection{Informative illustrations 1}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:informative-illustrations-1}}

\paragraph{Histograms}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:histograms}}
\sphinxAtStartPar
By plotting the histograms of two measurements, we can study the
distribution of measurements.  For example in Histogram 1 on the right,
we can see two distributions (blue and green), whose means are 2.8 and
2.2. We can clearly see that the means of the sample are different, but
it is not immediately clear whether that is just a random coincidence or
if this is statistically significant. In Histogram 2 on the right, we
see a much clearer difference. Though the means are the same, 2.8 and
2.2, the standard deviations are now smaller 0.25 instead of 1.0, such
that we can be fairly confident that the difference in means is
\sphinxhref{http://significant.In}{significant.In} Histogram 3, there is not doubt left, the distributions
are not overlapping and a statistical test would for sure show a
significant difference.

\sphinxAtStartPar
If we then compare histograms 2 and 4, we see that they have the same
means and standard deviations. However, while from histogram 2 we might
not be entirely certain that the difference is significant, in histogram
4 the amount of overlap is much reduced because the distributions are
skewed. Therefore already in informal analysis, we can be rather
confident that there is a significant difference in distributions. This
example thus demonstrates that the histograms can reveal properties of
the measurements which cannot be captured by the mean and standard
deviation.

\sphinxAtStartPar
A further example which demonstrates how some differences are not
captured by the mean and standard deviation is illustrated in Histogram
5. Here both measurements have the same mean, which could give the
impression that both measurements are “the same”. However, by looking at
the histogram, we find that the blue curve actually has two peaks, one
clearly lower and another higher than the green curve. Further analysis
is thus needed to determine the cause for the strange distribution.
Typically, for example, some methods behave differently for different
inputs. A speech enhancement method could be for example be effective
for voiced sounds, but fail for unvoiced sounds, such that the output
SNR has two peaks corresponding to respective classes of sounds.

\sphinxAtStartPar
These two last examples, Histograms 4 and 5, illustrate why it is in
general important to test whether evaluation results follow a Gaussian
(normal) distribution. Analysis of approximately Gaussian results is
less complicated then the analysis of skewed and multi\sphinxhyphen{}modal
distributions. However, applying methods which assume Gaussian
distributions on data which does not follow Gaussian distributions will
\sphinxstyleemphasis{often lead to incorrect conclusions}.

\sphinxAtStartPar
Histogram 1: Large overlap between distributions A (blue) and B (green)

\sphinxAtStartPar
\sphinxincludegraphics{{155473463}.png}

\sphinxAtStartPar
Histogram 3: Very small overlap between distributions

\sphinxAtStartPar
\sphinxincludegraphics{{155473465}.png}

\sphinxAtStartPar
Histogram 5: Bimodal distribution for A, that is, the blue curve has two
peaks.

\sphinxAtStartPar
\sphinxincludegraphics{{155473467}.png}

\sphinxAtStartPar
Histogram 2: Smaller overlap between distributions

\sphinxAtStartPar
\sphinxincludegraphics{{155473464}.png}
Histogram 4: Skewed distributions such that overlap is smaller than
standard deviation would indicate

\sphinxAtStartPar
\sphinxincludegraphics{{155473466}.png}


\paragraph{Scatter plots}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:scatter-plots}}
\sphinxAtStartPar
A useful tool for determining whether there are in hidden structures
between the results of two methods is to plot a scatter plot of the
results, where we plot dots (or other symbols) in a graph whose x\sphinxhyphen{} and
y\sphinxhyphen{} axis correspond to the evaluation results of two methods. Scatter
plots 1\sphinxhyphen{}3 on the right illustrate different typical scenarios. Plot 1
shows measurements in a circular area, indicating that the measurements
are uncorrelated. In speech and audio experiments this is highly
unusual. Scatter plot 2 is much more common, where measurements form a
tight band. This means that measurements are correlated. For example, in
a speech coding scenario, it could be that some speech samples are easy
to encode with high quality, such that methods A and B both give high
scores. Our objective is however to determine whether A or B is better,
and therefore we want to cancel out the effect of these correlations. We
can then take the difference between A and B to get the relative quality
of the two methods (see histogram on the right).

\sphinxAtStartPar
Another typical scenario is illustrated in Scatter plot 3, where the
scatter plot seems to form two distinctive groups. This would indicate
that there are some underlying categories in the data (such as
male/female, voiced/unvoiced, speech/silence) where performance is
different. Typically we would then like to find out what the categories
to determine if we can improve performance based on that information.
For example, if performance is bad for unvoiced sounds, we could have a
detector for unvoiced sounds, and perform different processing for such
sounds. See also section “Parallel plots” below.

\sphinxAtStartPar
Scatter plot 1: Two uncorrelated measurements A and B.

\sphinxAtStartPar
\sphinxincludegraphics{{155473529}.png}

\sphinxAtStartPar
Scatter plot 3: Two somewhat separate groups with different means and
correlations.

\sphinxAtStartPar
\sphinxincludegraphics{{155473532}.png}

\sphinxAtStartPar
Scatter plot 2 illustrating two measurements with slightly differing
means but high correlation (measurements form a tight group). In such a
case, the difference between the two measurements can often be very
informative, illustrated in the histogram below. We see that the mean
difference is 0.6 with a standard deviation of 0.21 and that the
distribution is clearly separated from 0 (=clearly separated from the
null\sphinxhyphen{}hypothesis “no difference between A and B”).

\sphinxAtStartPar
\sphinxincludegraphics{{155473530}.png}
\sphinxincludegraphics{{155473531}.png}


\paragraph{Box plots}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:box-plots}}
\sphinxAtStartPar
For comparison of distributions of several different measurements, we
often use box\sphinxhyphen{}plots (see Figure on the right). Here we have the sound
samples on x\sphinxhyphen{}axis (here called “Items”) and the improvement in POLQA
scores on the y\sphinxhyphen{}axis (for more about delta scores, see section “Delta”
below). The red line in the midle represent the median of all scores,
the blue box contains 50\% of all measurements and the black horizontal
lines indicate the largest and lowest scores. In other words, each
quartile which contains 25\% of measurements, is indicated by an
interval. An exception are \sphinxstyleemphasis{outliers}, that is, those measurements which
are deemed \sphinxstyleemphasis{exceptional} in some sense, are marked with red plus\sphinxhyphen{}signs.
Note that the choice of outliers is an sensitive issue; you cannot just
choose which values are exceptional, but have to apply formal methods
(it is an advanced topic which is not discussed here).

\sphinxAtStartPar
Note that box\sphinxhyphen{}plots are visualization methods and cannot be used alone
to make conclusions about statistical significance. Instead, they
valuable for descriptive analysis of results. For example in the figure
on the right, female and male speakers are the three items on the left
and right, respectively. Clearly we get an impression that the delta
scores of the female speakers are higher than those of male speakers.
However, the distributions are overlapping to such an extent that we
would need proper statistical analysis to determine how likely it is
that this is not just a coincidence.

\sphinxAtStartPar
In any case, box plots are useful in illustration of for example MUSHRA,
POLQA and PESQ scores as well as their delta scores. Box plots contain
less information than the corresponding histograms, but that omission
reduces clutter; We can easily display 20 items side by side with a
box\sphinxhyphen{}plot, whereas histograms of 20 items would be a mess.

\sphinxAtStartPar
\sphinxincludegraphics{{155473908}.png}

\sphinxAtStartPar
Image by Sneha Das (with permission)


\paragraph{Confusion matrix}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:confusion-matrix}}
\sphinxAtStartPar
A \sphinxhref{https://en.wikipedia.org/wiki/Confusion\_matrix}{confusion matrix} is
the equivalent of a scatter plot for categorical data, often either
expressed in a table format or visualized as the as a heat\sphinxhyphen{}map. For
example, suppose you have a classifier whose objective is to classify
between speech, music and background noise. For each testing sample, you
then have the target output and the actual output of your classifier,
which you can write as a matrix such as:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
Actual music
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Actual speech
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Actual noise
\\
\hline
\sphinxAtStartPar
\sphinxstyleemphasis{Predicted music}
&
\sphinxAtStartPar
78
&
\sphinxAtStartPar
2
&
\sphinxAtStartPar
13
\\
\hline
\sphinxAtStartPar
*Predicted speech
&
\sphinxAtStartPar
3
&
\sphinxAtStartPar
77
&
\sphinxAtStartPar
5
\\
\hline
\sphinxAtStartPar
\sphinxstyleemphasis{Predicted noise}
&
\sphinxAtStartPar
22
&
\sphinxAtStartPar
18
&
\sphinxAtStartPar
83
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Often the largest value in each column is identified by bold\sphinxhyphen{}face. In
this toy example, all classes are usually predicted correctly, which
means that for each actual label, the most often predicted label matches
the actual label. That is, music is most often classified as music (78
times), but also quite often as noise (22 times). Similarly, speech is
classified most often as speech (77 times) but also often as noise (18
times). Noise, on the other hand, is rarely classified as speech (5
times) and almost always correctly as noise (83 times).

\sphinxAtStartPar
If the number of labels is large, then it is often useful to plot the
matrix as a heat\sphinxhyphen{}map.


\subsubsection{Example 2}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:example-2}}
\sphinxAtStartPar
Suppose we want to compare methods A and B, and we have already
calculated the frame\sphinxhyphen{}wise SNR on the outputs of both methods for a range
of speech samples. We have also already calculated means and standard
deviations and plotted histograms and scatter plots. The only we have
found that there seems to be distinct categories of results indicated by
a bimodal or multi\sphinxhyphen{}modal (multipeak) distribution of SNRs.

\sphinxAtStartPar
How do we know if A is better than B? How does behaviour of A differ
from B? How do we characterize the differences between A and B?

\sphinxAtStartPar
\sphinxstylestrong{First idea:} In the previous example we already demonstrated the
usefulness of illustrations. One of the all\sphinxhyphen{}time favourite illustrations
is to plot the original sound signal (or its spectrogram) side\sphinxhyphen{}by\sphinxhyphen{}side
with the frame\sphinxhyphen{}by\sphinxhyphen{}frame measurement results.


\subsubsection{Informative illustrations 2}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:informative-illustrations-2}}

\paragraph{Parallel plots}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:parallel-plots}}
\sphinxAtStartPar
When doing frame\sphinxhyphen{}by\sphinxhyphen{}frame measurements such as SNR, we can plot results
over time to see whether there are any particular structures in the
results. Typically, for example, it could be that the SNR is high for
some continuous sections and poor somewhere else. Such cases could be
for example voiced and unvoiced sounds, where the speech processing
method behaves differently. However, to determine which regions
correspond to which types of speech, we can plot the waveform (or
spectrogram) in parallel with the SNR.

\sphinxAtStartPar
As an example, consider the output of a trivial \DUrole{xref,myst}{voice activity detector
(VAD)} illustrated on the right. The
top\sphinxhyphen{}pane contains the original waveform, the second pane signal energy
and the third pane the thresholded energy as VAD output. We can
immediately see that the VAD output is correct when signal energy is
high. However, in low\sphinxhyphen{}energy parts of the speech signal, VAD output
oscillates between speech and non\sphinxhyphen{}speech, even if it would make sense
that the whole segment (between 1.1 and 2.7s) would be labelled speech.
In the fourth pane, we see a comparison of the raw/original, desired and
post\sphinxhyphen{}processed output methods (the results are shifted vertically for
better clarity).

\sphinxAtStartPar
In this way, parallel plots are very good for illustrating and
characterizing performance of a system. We can easily relate properties
of the speech signal with the outputs, such that we can describe which
types of inputs give which types of outputs.

\sphinxAtStartPar
\sphinxincludegraphics{{155473937}.png}


\subsubsection{Extracting hidden structures}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:extracting-hidden-structures}}

\paragraph{Deltas}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:deltas}}
\sphinxAtStartPar
People are often unable to accurately grade absolute quality, even if
they can grade relative quality very accurately. That is, for example,
in a MUSHRA test, it is perfectly normal that three subjects would grade
two methods A and B such that:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline

\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
Subject
&
\sphinxAtStartPar
Grade for A
&
\sphinxAtStartPar
Grade for B
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
35
&
\sphinxAtStartPar
37
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
44
&
\sphinxAtStartPar
48
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
51
&
\sphinxAtStartPar
57
\\
\hline
\sphinxAtStartPar
Mean
&
\sphinxAtStartPar
43.3
&
\sphinxAtStartPar
47.3
\\
\hline
\sphinxAtStartPar
Std
&
\sphinxAtStartPar
8.0
&
\sphinxAtStartPar
10.0
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
In other words, if we look at the absolute mean value and its standard
deviation, we find that B has a higher mean, but by only a tiny amount
in comparison to the standard deviation. However, closer inspection
reveals that the score of B is \sphinxstyleemphasis{always} higher than that of A. Every
subject thought that B is better than A!

\sphinxAtStartPar
Thus if we calculate the difference in scores between A and B, perhaps
that would give us a more conclusive answer.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Subject
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Grade for A
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Grade for B
&\sphinxstyletheadfamily 
\sphinxAtStartPar
A\sphinxhyphen{}B
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
35
&
\sphinxAtStartPar
37
&
\sphinxAtStartPar
2
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
44
&
\sphinxAtStartPar
48
&
\sphinxAtStartPar
4
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
51
&
\sphinxAtStartPar
57
&
\sphinxAtStartPar
6
\\
\hline
\sphinxAtStartPar
Mean
&
\sphinxAtStartPar
43.3
&
\sphinxAtStartPar
47.3
&
\sphinxAtStartPar
4
\\
\hline
\sphinxAtStartPar
Std
&
\sphinxAtStartPar
8.0
&
\sphinxAtStartPar
10.0
&
\sphinxAtStartPar
2
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Clearly we now see that the difference A\sphinxhyphen{}B has a mean value of 4 and the
standard deviation is only 2. By collecting more listeners, then it
could be possible that this is significant difference between A and B.

\sphinxAtStartPar
The same approach can be applied also to other tests. For example, PESQ
scores for two methods A and B can vary a lot between speech samples,
but often the difference PESQ(A)\sphinxhyphen{}PESQ(B) can be much more consistent.

\sphinxAtStartPar
A formal way to check whether it is reasonable to use delta\sphinxhyphen{}values would
be a correlation test (see below).


\subsection{Statistical tests}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:statistical-tests}}
\sphinxAtStartPar
\sphinxstylestrong{DISCLAIMER:} If you have not already, \sphinxstyleemphasis{you should} take a course in
statistical analysis. There are plenty of good courses out there. In
this short chapter we can only skim through some basic principles, which
can not replace a thorough understanding of methods. Applying
statistical tests without understanding them properly will \sphinxstyleemphasis{regularly
lead to incorrect conclusions}.


\subsubsection{Student’s t\sphinxhyphen{}test}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:student-s-t-test}}
\sphinxAtStartPar
When we want to determine if two normally distributed sets of values
have the same mean, we use \sphinxhref{https://en.wikipedia.org/wiki/Student\%27s\_t-test}{Student’s
t\sphinxhyphen{}test}. To be
applicable, it is critical that the sets really do follow the Gaussian
distribution (see “Normality tests” below). The t\sphinxhyphen{}test is intended for
small data\sphinxhyphen{}sets, usually with less than 30 data\sphinxhyphen{}points and it is not
meaningful for larger sets. Typical applications in speech and audio
are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Suppose we have made a MUSHRA\sphinxhyphen{}test, where we want to compare weather
a new method B is better than the old method A. The mean of B is
better (higher) than that of A, but how do we know if the result is
statistically significant? First of all, we need to check that
measurements follow the Gaussian distribution (see “Normality tests”
below). If distributions are indeed Gaussian, we can apply the
t\sphinxhyphen{}test. Given a threshold for significance (say 5\%), the t\sphinxhyphen{}test
gives an answer; either H=0, the difference between A and B is not
significant or, H=1, that B is better than A.

\item {} 
\sphinxAtStartPar
Similarly, we can calculate the difference between A and B, and if
the distribution is Gaussian, we can determine whether the
difference to zero is statistically significant.

\end{itemize}

\sphinxAtStartPar
If the data sets are not normally distributed, it is often possible to
use the \sphinxhref{https://en.wikipedia.org/wiki/Mann\%E2\%80\%93Whitney\_U\_test}{Wilcoxon rank\sphinxhyphen{}sum
test} or the
\sphinxhref{https://en.wikipedia.org/wiki/Wilcoxon\_signed-rank\_test}{Wilcoxon signed\sphinxhyphen{}rank
test} instead.


\subsubsection{Normality tests}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:normality-tests}}
\sphinxAtStartPar
Many statistical tests are only applicable when the input signal follows
a Gaussian distribution. To test whether a distribution indeed is
Gaussian, we use a \sphinxhref{https://en.wikipedia.org/wiki/Normality\_test}{normality
test}. The \sphinxhref{https://en.wikipedia.org/wiki/Shapiro\%E2\%80\%93Wilk\_test}{Shapiro\sphinxhyphen{}Wilk
test} is one
particular test of normality, which is particularly reliable. The output
of normality tests is that, given a particular level of confidence (such
as 5\%), the given data set is Gaussian.


\subsubsection{ANOVA}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:anova}}
\sphinxAtStartPar
When comparing multiple data sets at the same time, \sphinxhref{https://en.wikipedia.org/wiki/Analysis\_of\_variance}{analysis of
variance} (ANOVA)
can be interpreted as a generalization of the t\sphinxhyphen{}test. It tries to
explain phenomenons in the data by assigning them to different sources
of variation. Say if we observe differences between methods A and B, but
know that some listeners have taken the test in the morning and others
in the evening, we can analyse whether the difference is due to the the
time of day or due to inherent differences in A and B.

\sphinxAtStartPar
Again it is important to observe the assumptions of this test; namely,
the classical version of ANOVA assumes that prediction errors are
normal, that measurements are independent and the assigned groups of
data have uniform statistics.


\subsubsection{Correlation tests}
\label{\detokenize{Evaluation/Analysis_of_evaluation_results:correlation-tests}}
\sphinxAtStartPar
Often two data sets are correlated. For example, if we analyse the
fundamental frequency and intensity of a speech signal, we probably find
that shouted speech has a high intensity and fundamental frequency,
whereas silent speech has low intensity and a low fundamental. To check
whether two data sets indeed have a meaningful correlation, we use
correlation tests. The two most common correlation tests are
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Pearson\_correlation\_coefficient}{Pearson correlation
coefficient}
(PCC), which essentially fits a linear regression line through the
data, such that we can then use the t\sphinxhyphen{}test to determine whether the
thus\sphinxhyphen{}explained data is significantly different from the original. In
other words, the Pearson correlation coefficient assumes a linear
correlation between variables.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Spearman\%27s\_rank\_correlation\_coefficient}{Spearman’s rank correlation
coefficient}
analyses whether data can be explained by a monotonic function.

\end{itemize}

\sphinxstepscope


\chapter{Speech analysis}
\label{\detokenize{Speech_analysis:speech-analysis}}\label{\detokenize{Speech_analysis::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Analysis/Fundamental_frequency_estimation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Fundamental frequency estimation}}}}

\item {} 
\sphinxAtStartPar
Formant estimation and tracking

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Analysis/Measurements_for_medical_applications::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Measurements for medical applications}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Analysis/Inverse_filtering_for_glottal_activity_estimation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Inverse filtering for glottal activity
estimation}}}}

\end{itemize}

\sphinxstepscope


\section{Fundamental frequency estimation}
\label{\detokenize{Analysis/Fundamental_frequency_estimation:fundamental-frequency-estimation}}\label{\detokenize{Analysis/Fundamental_frequency_estimation::doc}}
\sphinxAtStartPar
The \DUrole{xref,myst}{fundamental frequency (F0)} is central
in describing speech signals whereby we need methods for estimating the
\(F_0\) from speech signals. In speech analysis applications, it can be
informative to study the absolute value of the fundamental frequency as
such, but more commonly, extraction of the \(F_0\) is usually a
pre\sphinxhyphen{}processing step. For example, in \DUrole{xref,myst}{recognition
tasks}, \(F_0\) is often used as a
feature for machine learning methods. A voice activity detector could,
for instance, set a lower and higher threshold on the \(F_0\), such that
sounds with an \(F_0\) outside the valid range would be classified as
non\sphinxhyphen{}speech.

\sphinxAtStartPar
The fundamental frequency is visible in multiple different domains:
\begin{itemize}
\item {} 
\sphinxAtStartPar
In an acoustic time\sphinxhyphen{}signal, the \(F_0\) is visible as a repetition after
every \(T\) samples.

\item {} 
\sphinxAtStartPar
In the \DUrole{xref,myst}{autocovariance or
\sphinxhyphen{}correlation}, the \(F_0\) is visible
as a peak at lag \(T\) as well as its integer multiples \(kT\).

\item {} 
\sphinxAtStartPar
In the magnitude, power or log\sphinxhyphen{}magnitude
\DUrole{xref,myst}{spectrum}, the \(F_0\) is visible as a peak at
the frequency \(F_0=F_{s}/T\), as well as its integer
multiples, where \(F_{s}\) is the sampling frequency.

\item {} 
\sphinxAtStartPar
In the \DUrole{xref,myst}{cepstrum}, the \(F_0\) is visible as a peak at
quefrency \(T\) as well as its integer multiples \(kT\).

\end{itemize}

\sphinxAtStartPar
Consequently, we can use any of these domains to estimate the
fundamental frequency \(F_0\). A typical approach applicable in all domains
except the time\sphinxhyphen{}domain, is peak\sphinxhyphen{}picking. The fundamental frequency
corresponds to a peak in each domain, such that we can determine the \(F_0\)
by finding the highest peak. For better robustness to spurious peaks and
for computational efficiency, we naturally limit our search to the range
of valid \(F_0\)’s, such as  \( 80\leq F_0\leq 450. \)

\sphinxAtStartPar
The harmonic structure however poses a problem for peak\sphinxhyphen{}picking. Peaks
appear at integer multiples of either \(F_0\) or lag T, such that sometimes,
by coincidence or due to estimation errors, the harmonic peaks can be
higher than the primary peak. Such estimation errors are known
as \sphinxstyleemphasis{octave errors}, because the error in \(F_0\) corresponds to the musical
interval of an octave. A typical post\sphinxhyphen{}processing step is therefore to
check for octave jumps. We can check whether \(F_0/2\) or \(F_0/3\) would
correspond to a sensible \(F_0\). Alternatively, we can check whether the
previous analysis frame had an \(F_0\) which an octave or two octaves off.
Depending on application, we can then fix errors or label problematic
zones for later use.

\sphinxAtStartPar
Another problem with peak\sphinxhyphen{}picking is that peak locations might not align
with the samples. For example in the autocorrelation domain, the true
length of the period could be 100.3 samples. However, the peak in the
autocorrelation would then appear at lag 100. One approach would then be
to use quadratic interpolation between samples in the vicinity of a peak
and use the location of the maximum of the interpolated peak as an
estimate of the peak location. Interpolation makes the estimate less
sensitive to noise. For example, background noise could happen to have a
peak at lag 102, such that desired maximum of 100 is lower than the peak
at 102. By using data from more samples, as in the interpolation
approach, we can therefore reduce the likelihood that a single corrupted
data point would cause an error.

\sphinxAtStartPar
An alternative approach to peak\sphinxhyphen{}picking is to use all distinctive peaks
to jointly estimate the \(F_0\). That is, if you find \(N\) peaks at
frequencies \(p_{k}\), which approximately correspond to harmonic
peaks \(kF0\), then you can approximate \( F_0 \approx \frac1N
\sum_{k=1}^N p_k/k. \) Another alternative is to calculate the
distance between consecutive peaks an estimate \( F_0 \approx
\frac1{N-1} \sum_{k=1}^{N-1} (p_{k+1}-p_k). \) We can also combine
these methods as we like.

\sphinxAtStartPar
In many other applications, we use discrete Fourier transform (DFT) or
cosine transforms (DCT) to resolve frequency components. It would
therefore be tempting to apply the same approach also here. In such a
domain we would also already have a joint estimate which does not rely
on single data points. However, note that the spectrum is already the
DFT of the time signal and the cepstrum is the DCT or DFT of the
log\sphinxhyphen{}spectrum. Additional transforms therefore usually do not resolve the
ambiguity between harmonics.

\sphinxstepscope


\section{Measurements for medical applications}
\label{\detokenize{Analysis/Measurements_for_medical_applications:measurements-for-medical-applications}}\label{\detokenize{Analysis/Measurements_for_medical_applications::doc}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Electroglottograph}{Electroglottography}
(Wikipedia)

\item {} 
\sphinxAtStartPar
Stroboscopy and
\sphinxhref{https://en.wikipedia.org/wiki/Videokymography}{videokymography}
(Wikipedia)

\item {} 
\sphinxAtStartPar
Highspeed camera

\item {} 
\sphinxAtStartPar
MRI

\item {} 
\sphinxAtStartPar
Rothenberg mask

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Analysis/Inverse_filtering_for_glottal_activity_estimation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Inverse filtering for glottal activity estimation}}}}

\end{enumerate}

\sphinxstepscope


\subsection{Inverse filtering for glottal activity estimation}
\label{\detokenize{Analysis/Inverse_filtering_for_glottal_activity_estimation:inverse-filtering-for-glottal-activity-estimation}}\label{\detokenize{Analysis/Inverse_filtering_for_glottal_activity_estimation::doc}}

\subsubsection{Motivation}
\label{\detokenize{Analysis/Inverse_filtering_for_glottal_activity_estimation:motivation}}
\sphinxAtStartPar
The main source of speech sounds, where the acoustic signal is first
created, are the vocal folds, which oscillate in the airflow generated
by the lungs (for details, see \DUrole{xref,myst}{Speech production and acoustic
properties}). The opening
between the vocal folds is known as the \sphinxstyleemphasis{glottis} and the movements of
vocal folds as well and the corresponding airflow are jointly known as
glottal activity.

\sphinxAtStartPar
Since glottal activity is thus central to speech, it is also important
to understand how it works. What makes a “good” voice? If there is a
disruption to the vocal folds, how does that affect the voice? These are
often medical questions, but they have a large social and societal
impact. If you loose your voice, you can easily become isolated since
you loose an important mode of communication. If you are in a voice
profession such as teaching, sales, singing or acting, also your ability
to work relies on your voice such that any disturbance in the voice
impedes your ability to work. Studying the glottis is thus paramount.

\sphinxAtStartPar
The vocal folds are located in the neck, covered and surrounded by a
cartilage. Accessing them is therefore difficult. Putting a camera in
the throat is uncomfortable to say the least and even when it is
possible, it impedes normal speech, giving measurements a bias of
unknown size. Moreover, since the vocal folds oscillate with a
fundamental frequency that can go up to 400 Hz, we need a camera whose
frame rate is at least 4000 Hz to get 10 frames per period. In other
words, we would need a high\sphinxhyphen{}speed camera. While such cameras are today
readily available, they need a lot of light, which generates a lot of
heat, which is not compatible within the sensitive tissues inside the
throat. Imaging with other methods, X\sphinxhyphen{}rays or magnetic resonance
imaging, generally have a slower frame rate and some imaging like X\sphinxhyphen{}rays
also generate harmful radiation (esp. at high frame rates).

\sphinxAtStartPar
The cartilage surrounding the vocal folds also prevents ultrasound
measurements. The only usable direct measurement is \sphinxhref{https://en.wikipedia.org/wiki/Electroglottograph}{electroglottography
(EGG)}, which measures
the impedance through the neck using electrodes. It measures
conductivity, which is highly dependent on the contact area of the vocal
folds, thus giving information about the position of the vocal folds.
However, this information is usually one\sphinxhyphen{}dimensional which limits the
usability of such measurements. It also sensitive to and requires
careful placement of electrodes.

\sphinxAtStartPar
What remains is the acoustic signal. With a microphone, we can record
the sound emitted from the mouth, and try to deduce the movements of the
vocal folds from the sound. It is minimally invasive, because we do not
need to insert any sensors inside or onto the the body. Airflow through
the glottis is closely related to the movements of the vocal folds; when
the vocal folds are open air can flow and when they are closed, airflow
is stopped. Airflow in turn is related to the acoustic signal; sound is
a variation in air pressure such that the gradient of airflow
approximately translates to the corresponding sound.

\sphinxAtStartPar
When sound is generated in the vocal folds, it is however acoustically
shaped by the vocal tract (for details, again, see \DUrole{xref,myst}{Speech production
and acoustic properties});
some frequencies are emphasised and others attenuated. To analyse
glottal activity, we therefore need to cancel the acoustic effect of the
vocal tract. Recall that the effect of the vocal tract can be
efficiently modelled by a linear predictive filter. We can thus estimate
a filter corresponding to the effect of the vocal tract, and then invert
the effect of that filter. This process is known as \sphinxstyleemphasis{inverse filtering}.


\subsubsection{Signal Model}
\label{\detokenize{Analysis/Inverse_filtering_for_glottal_activity_estimation:signal-model}}
\sphinxAtStartPar
In glottal inverse filtering, we follow the source\sphinxhyphen{}filter paradigm for
speech source modelling, where the acoustic speech signal is modelled as
an excitation signal filtered by the vocal tract. The assumption is that
these two components are independent. By assuming that we know the
effect of the vocal tract, we can therefore remove its effect by
inverting its effect. If we further model the vocal tract as a
tube\sphinxhyphen{}model, its effect corresponds to IIR filtering, such that the
inverse process is FIR filtering.

\sphinxAtStartPar
The main difficulty in of glottal inverse filtering is estimation of the
filter corresponding to the effect of the vocal tract. The task
resembles classical \DUrole{xref,myst}{linear predictive} modelling,
where the parameters of an IIR filter are uniquely estimated from the
autocorrelation of the signal. Covertly, this however assumes that the
excitation is uncorrelated white noise. However, in voiced speech, the
excitation is the glottal excitation, which resembles a half\sphinxhyphen{}wave
rectified sinusoid. That is, it is a fairly smooth curve with a
characteristic comb\sphinxhyphen{}structure in the spectrum, as well as a distinct
tilt with more energy at low frequencies. Though we know a lot about the
glottal excitation, it is hard to model. If we would have the glottal
excitation, then the vocal tract would be easy to estimate and vice
versa \sphinxhyphen{} a typical chicken\sphinxhyphen{}and\sphinxhyphen{}egg problem.

\sphinxAtStartPar
One of the most popular methods for glottal inverse filtering is
iterative adaptive inverse filtering (IAIF), where both the glottal
excitation and the vocal tract are modelled with linear predictive
filtering, and both filters are estimated in an alternating iteration.
Many improvements based on more advanced signal models as well as
machine learning have been proposed, but the question is far from
solved. A central problem in evaluation such methods is the ground
truth. We can estimate curves which look like glottal excitations, but
we would need to know the actual movements of the vocal folds to verify
the accuracy of the obtained curves. Since we do not have a satisfactory
direct method for observing glottal activity, which would not bias
results, we cannot verify our models.

\sphinxstepscope


\section{Forensic analysis}
\label{\detokenize{Analysis/Forensic_analysis:forensic-analysis}}\label{\detokenize{Analysis/Forensic_analysis::doc}}
\sphinxstepscope


\chapter{Recognition tasks in speech processing}
\label{\detokenize{Recognition_tasks_in_speech_processing:recognition-tasks-in-speech-processing}}\label{\detokenize{Recognition_tasks_in_speech_processing::doc}}
\sphinxAtStartPar
Typical tasks in speech processing, where machine learning is often
applied include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Speech recognition}, which refers to converting an acoustic
waveform of spoken speech to the corresponding text
(speech\sphinxhyphen{}to\sphinxhyphen{}text).

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Speaker recognition} and \sphinxstyleemphasis{speaker verification,} which refer to,
respectively, identifying the speaker (who is speaking?) and
verifying whether the speaker is who he claims to be (is it really
you?).

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Speech synthesis}, which entails the creation of a natural sounding
speech signal from text input (text\sphinxhyphen{}to\sphinxhyphen{}speech).

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{{\hyperref[\detokenize{Speech_enhancement::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Speech enhancement}}}}}, refers to improving a
recorded speech signal, for example with the objective of removing
background noise (noise attenuation) or the effect of room
acoustics.

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Recognition/Wake-word_and_keyword_spotting::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{\sphinxstyleemphasis{Wake\sphinxhyphen{}word} and \sphinxstyleemphasis{keyword detection}}}}}, refers to the task
where the purpose is to find single characterizing words from
continuous speech. The idea is that by using a light\sphinxhyphen{}weight
algorithm, we can extract useful information without a
computationally complex speech recognizer. Specifically, wake\sphinxhyphen{}word
detection refers to the waiting for the activation command, that is,
the device sleeps until the wake\sphinxhyphen{}word is heard. Keyword detection
can refer to similar task, or for example, the task of recognizing
the topic of a conversation.

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Recognition/Voice_activity_detection::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{\sphinxstyleemphasis{Voice activity detection} (VAD)}}}},
refers to the task of determining whether a signal contains speech
or not (is someone speaking?). Many of the above tasks are
resource\sphinxhyphen{}intensive operations, such that we would like to, for
example, use speech recognition only when speech is present. We can
therefore first use a simple VAD to determine whether a signal is
speech or not, and only start the speech recognizer when speech is
present.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Speaker\_diarisation}{\sphinxstyleemphasis{Speech
diarisation}} is
the process of segmenting a multi\sphinxhyphen{}speaker conversation into
continuous single\sphinxhyphen{}speaker segments.

\item {} 
\sphinxAtStartPar
\DUrole{xref,myst}{\sphinxstyleemphasis{Paralinguistic analysis tasks}},
refers generally to the extraction of non\sphinxhyphen{}linguistic and non\sphinxhyphen{}speaker
identity related information from speech signals, such as speaker
emotions, health, attitude, sleepiness etc.

\end{itemize}

\sphinxstepscope


\section{Voice Activity Detection (VAD)}
\label{\detokenize{Recognition/Voice_activity_detection:voice-activity-detection-vad}}\label{\detokenize{Recognition/Voice_activity_detection::doc}}

\subsection{Introduction}
\label{\detokenize{Recognition/Voice_activity_detection:introduction}}
\sphinxAtStartPar
\sphinxincludegraphics{{vad_spp-1}.png}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Voice activity detection} (VAD) (or speech activity detection, or
speech detection) refers to a class of methods which detect whether
a sound signal contains speech or not.

\item {} 
\sphinxAtStartPar
A closely related and partly overlapping task is \sphinxstyleemphasis{speech presence
probability} (SPP) estimation.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Instead of a binary present/not\sphinxhyphen{}present decision, SPP gives a
probability level that the signal contains speech.

\item {} 
\sphinxAtStartPar
A VAD can be derived from SPP by setting a threshold probability
above which the signal is considered to contain speech.

\item {} 
\sphinxAtStartPar
In most cases, SPP is thus the more fundamental problem.

\end{itemize}

\item {} 
\sphinxAtStartPar
Voice activity detection is used as a pre\sphinxhyphen{}processing algorithm for
almost all other speech processing methods.
\begin{itemize}
\item {} 
\sphinxAtStartPar
In \sphinxstyleemphasis{speech coding}, it is used to to determine when speech
transmission can be switched off to reduce the amount of
transmitted data.

\item {} 
\sphinxAtStartPar
In \sphinxstyleemphasis{speech recognition}, it is used to find out what parts of
the signal should be fed to the recognition engine. Since
recognition is a computationally complex operation, ignoring
non\sphinxhyphen{}speech parts saves CPU power.

\end{itemize}

\item {} 
\sphinxAtStartPar
VAD or SPP is thus used mostly as a resource\sphinxhyphen{}saving operation.

\item {} 
\sphinxAtStartPar
In \sphinxstyleemphasis{speech enhancement}, where we want to reduce or remove noise in
a speech signal, we can estimate noise characteristics from
non\sphinxhyphen{}speech parts (learn/adapt) and remove noise from the speech
parts (apply).

\item {} 
\sphinxAtStartPar
A closely related method in \sphinxstyleemphasis{audio} applications is \sphinxstyleemphasis{noise gateing},
where typically a microphone signal is muted whenever there is no
signal present.
\begin{itemize}
\item {} 
\sphinxAtStartPar
For example, when a singer is not singing in the microphone,
then the microphone is off. When the singer is not singing,
microphone signal is only noise and therefore the noise gate
removes (gates) noise.

\end{itemize}

\item {} 
\sphinxAtStartPar
VADs can thus also be used in improving signal quality.

\end{itemize}


\subsection{Low\sphinxhyphen{}noise VAD = Trivial case}
\label{\detokenize{Recognition/Voice_activity_detection:low-noise-vad-trivial-case}}
\sphinxAtStartPar
\sphinxincludegraphics{{vad_sad-1}.png}
\begin{itemize}
\item {} 
\sphinxAtStartPar
To introduce basic vocabulary and methodology, let us consider a
case where a speaker is speaking in an (otherwise) silent
environment.
\begin{itemize}
\item {} 
\sphinxAtStartPar
When there is no speech, there is silence.

\item {} 
\sphinxAtStartPar
(Any) Signal activity indicates voice activity.

\end{itemize}

\item {} 
\sphinxAtStartPar
Signal activity can be measured by, for example, estimating signal
energy per frame \(\Rightarrow\) the \sphinxstyleemphasis{energy thresholding algorithm}.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{vad_ene-1}.png}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Clearly energy thresholding works for silent speech signals.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Low\sphinxhyphen{}energy frames are correctly labeled as non\sphinxhyphen{}speech and speech
parts are likewise correctly labeled.

\end{itemize}

\item {} 
\sphinxAtStartPar
It is however not trivial to choose an appropriate threshold\sphinxhyphen{}level.
\begin{itemize}
\item {} 
\sphinxAtStartPar
A low threshold level would make sure that all speech\sphinxhyphen{}frames are
correctly labeled. However, we might then also label frames with
other sounds, like breathing sounds or other background noises,
as speech frames.

\item {} 
\sphinxAtStartPar
A high threshold would make sure that all detected speech\sphinxhyphen{}frames
actually are truly speech frames. But then we could miss offsets
(sounds which are trailing off), since they often have a low
energy.

\end{itemize}

\item {} 
\sphinxAtStartPar
What strategy should we use to choose a threshold?
\begin{itemize}
\item {} 
\sphinxAtStartPar
What is the correct label for something like breathing\sphinxhyphen{}noises?

\item {} 
\sphinxAtStartPar
How do we actually measure performance of a VAD?

\end{itemize}

\end{itemize}


\subsection{VAD objective and performance measurement}
\label{\detokenize{Recognition/Voice_activity_detection:vad-objective-and-performance-measurement}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The objective of a VAD implementation depends heavily on the
application.
\begin{itemize}
\item {} 
\sphinxAtStartPar
In speech coding, our actual objective is to reduce bitrate
without decreasing quality. \(\Rightarrow\) We want to make sure
that no speech frames are classified as background noise,
because that would reduce quality.

\sphinxAtStartPar
\(\Rightarrow\) We make a conservative estimate.

\item {} 
\sphinxAtStartPar
In keyword spotting (think “Siri” or “OK Google”), we want to
detect the start of a particular combination of words. The VADs
task is to avoid running a computationally expensive keyword
spotting algorithm all the time. Missing one keyword is not so
bad (the user would then just try again), but if it is too
sensitive then the application would drain the battery.

\sphinxAtStartPar
\(\Rightarrow\) We want to be sure that only keywords are spotted.

\end{itemize}

\item {} 
\sphinxAtStartPar
The objective of a VAD implementation depends heavily on the
application.
\begin{itemize}
\item {} 
\sphinxAtStartPar
In speech enhancement, we want to find non\sphinxhyphen{}speech areas such
that we can there estimate noise characteristics, such that we
can remove anything which looks like noise. We want to be sure
that there is no speech in the noise estimate, otherwise we
would end up removing some speech and not only noise.

\item {} 
\sphinxAtStartPar
In speech recognition, VAD is used purely for resource saving.
We do not want to reduce accuracy of the recognition, but want
to minimize CPU usage.

\end{itemize}

\item {} 
\sphinxAtStartPar
We need a set of performance measures which reflect these different
objectives.

\item {} 
\sphinxAtStartPar
The performance is then often described by looking at how often are
frames which do contain speech labeled as speech/non\sphinxhyphen{}speech, and how
often is non\sphinxhyphen{}speech labeled as speech/non\sphinxhyphen{}speech?

\end{itemize}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Input
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Speech
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Non\sphinxhyphen{}speech
\\
\hline
\sphinxAtStartPar
Speech
&
\sphinxAtStartPar
True positive
&
\sphinxAtStartPar
False negative
\\
\hline
\sphinxAtStartPar
Non\sphinxhyphen{}speech
&
\sphinxAtStartPar
False positive
&
\sphinxAtStartPar
True negative
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\begin{itemize}
\item {} 
\sphinxAtStartPar
For speech coding, we want to keep the number of false negatives low, and false positives are of only secondary importance.

\item {} 
\sphinxAtStartPar
For keyword spotting, we want to keep the number of false positives low, and false negatives are secondary importance.

\end{itemize}


\subsubsection{Performance in noise – \sphinxhyphen{}3dB / \sphinxhyphen{}4dB threshold}
\label{\detokenize{Recognition/Voice_activity_detection:performance-in-noise-3db-4db-threshold}}
\sphinxAtStartPar
\sphinxincludegraphics{{vad_ene_noise3-1}.png}
\sphinxincludegraphics{{vad_ene_noise4-1}.png}


\subsection{Post\sphinxhyphen{}processing}
\label{\detokenize{Recognition/Voice_activity_detection:post-processing}}
\sphinxAtStartPar
\sphinxincludegraphics{{vad_ene_hyst-1}.png}
\begin{itemize}
\item {} 
\sphinxAtStartPar
We already saw that speech coding wants to avoid false negatives
(=speech frames labeled as non\sphinxhyphen{}speech).

\item {} 
\sphinxAtStartPar
Can we identify typical situations where false negatives occur?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Offsets (where a phonation ends) often have low energy\(\Rightarrow\) Easily misclassified as non\sphinxhyphen{}speech.

\item {} 
\sphinxAtStartPar
Stops have a silence in the middle of an utterance.\(\Rightarrow\) Easily misclassified as non\sphinxhyphen{}speech.

\end{itemize}

\item {} 
\sphinxAtStartPar
We should be careful at the end of phonations.
\begin{itemize}
\item {} 
\sphinxAtStartPar
We can use a \sphinxstyleemphasis{hangover} time, such that after a speech segment
we keep the label as speech for a while until we are sure that
speech has ended.

\item {} 
\sphinxAtStartPar
For onsets (starts of phonemes) we usually want to be very
sensitive.

\end{itemize}

\item {} 
\sphinxAtStartPar
We obtain a hysteresis rule;
\begin{itemize}
\item {} 
\sphinxAtStartPar
If any of the last \(K\) frames was identified as speech, then the
current frame is labelled as speech. Otherwise non\sphinxhyphen{}speech.

\end{itemize}

\end{itemize}


\subsection{VAD for noisy speech}
\label{\detokenize{Recognition/Voice_activity_detection:vad-for-noisy-speech}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Clean speech (absolutely no background noise) is very rare if not
impossible to achieve.

\item {} 
\sphinxAtStartPar
Real\sphinxhyphen{}life speech recordings practically always have varying amounts
of background noise.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Performance of energy thresholding decreases rapidly when the
SNR drops.

\item {} 
\sphinxAtStartPar
For example, weak offsets easily disappear in noise.

\end{itemize}

\item {} 
\sphinxAtStartPar
We need more advanced VAD methods for noisy speech.
\begin{itemize}
\item {} 
\sphinxAtStartPar
We need to identify characteristics which differentiate between
speech and noise.

\item {} 
\sphinxAtStartPar
Measures for such characteristics are known as \sphinxstyleemphasis{features}.

\end{itemize}

\end{itemize}


\subsubsection{Features}
\label{\detokenize{Recognition/Voice_activity_detection:features}}
\sphinxAtStartPar
\sphinxincludegraphics{{vad_features1-1}.png}
\begin{itemize}
\item {} 
\sphinxAtStartPar
In VAD, with features we try to measure some property of the signal
which would give an indication to whether the signal is speech or
non\sphinxhyphen{}speech.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Signal energy is naturally a useful feature, since the energy of
speech varies a lot.

\item {} 
\sphinxAtStartPar
Voiced sounds generally have energy mainly at the low
frequencies, whereby estimators for spectral tilt are often
useful. For example,
\begin{itemize}
\item {} 
\sphinxAtStartPar
Zero\sphinxhyphen{}crossings (per time unit) is high for high\sphinxhyphen{}frequency
signals (noise) and low for low\sphinxhyphen{}frequency signals (voiced
speech), whereby it can be used as a feature.

\item {} 
\sphinxAtStartPar
The lag\sphinxhyphen{}1 autocorrelation is high (close to one) for
low\sphinxhyphen{}frequency signals and low (close to \sphinxhyphen{}1) for
high\sphinxhyphen{}frequency signals.

\end{itemize}

\end{itemize}

\item {} 
\sphinxAtStartPar
Speech sounds can be efficiently modelled by linear prediction.
\begin{itemize}
\item {} 
\sphinxAtStartPar
If the prediction error is small, then it is likely that the
signal is speech.

\item {} 
\sphinxAtStartPar
If the prediction error is large, then it is probably
non\sphinxhyphen{}speech.

\end{itemize}

\item {} 
\sphinxAtStartPar
Voiced speech has by definition a prominent pitch.
\begin{itemize}
\item {} 
\sphinxAtStartPar
If we can identify a prominent pitch in the range then it is
likely voiced speech.

\end{itemize}

\item {} 
\sphinxAtStartPar
Speech information is described effectively by their spectral
envelope.
\begin{itemize}
\item {} 
\sphinxAtStartPar
MFCC can be used as a description of envelope information and it
is thus a useful set of features.

\item {} 
\sphinxAtStartPar
Linear prediction parameters (esp. prediction residual) also
describe envelope information and can thus also be used as a
feature\sphinxhyphen{}set.
\sphinxincludegraphics{{vad_features2-1}.png}

\end{itemize}

\item {} 
\sphinxAtStartPar
Speech features vary rapidly and frequently.
\begin{itemize}
\item {} 
\sphinxAtStartPar
By looking at the rate of change \(\Delta_k=f_{k+1}-f_k\) in other
features \(f_k\), we obtain information about the rate of change
of the signal. (Estimate of derivative)

\item {} 
\sphinxAtStartPar
Likewise, we can look at the second difference
\(\Delta\Delta_k=\Delta_{k+1}-\Delta_k\). (Estimate of second
derivative)

\item {} 
\sphinxAtStartPar
These first and second order differences can be used as features
and they are known as \(\Delta\)\sphinxhyphen{} and \(\Delta\Delta\)\sphinxhyphen{}features.

\end{itemize}

\end{itemize}


\subsection{Classifier}
\label{\detokenize{Recognition/Voice_activity_detection:classifier}}\begin{itemize}
\item {} 
\sphinxAtStartPar
We have collected a set of indicators for speech, the features,
whereby the next step is to merge the information from these
features to make a decision between speech and non\sphinxhyphen{}speech.

\item {} 
\sphinxAtStartPar
Classification is generic problem, with plenty of solutions such as
\begin{itemize}
\item {} 
\sphinxAtStartPar
decision trees (low\sphinxhyphen{}complexity, requires manual tuning)

\item {} 
\sphinxAtStartPar
linear classifier (relatively low\sphinxhyphen{}complexity, training from
data)

\item {} 
\sphinxAtStartPar
advanced methods such as neural networks, Gaussian mixture
models etc. (high\sphinxhyphen{}complexity, high\sphinxhyphen{}accuracy, training from data)
\sphinxincludegraphics{{vad2-1}.png}

\end{itemize}

\end{itemize}


\subsubsection{Decision trees (historical)}
\label{\detokenize{Recognition/Voice_activity_detection:decision-trees-historical}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Make a sequence of binary decisions (for example, is low or high
energy?) to decide whether signal is speech or non\sphinxhyphen{}speech.

\item {} 
\sphinxAtStartPar
Decision trees are very simple to implement.

\item {} 
\sphinxAtStartPar
Hard\sphinxhyphen{}coded \(\Rightarrow\) not very flexible.

\item {} 
\sphinxAtStartPar
Noise in one feature can cause us to follow wrong path.
\begin{itemize}
\item {} 
\sphinxAtStartPar
One noisy feature can break whole decision tree.

\end{itemize}

\item {} 
\sphinxAtStartPar
Requires that each decision is manually tuned\(\Rightarrow\) Lots of work, especially when tree is large

\item {} 
\sphinxAtStartPar
Structure and development becomes very complex if the number of
features increase.

\item {} 
\sphinxAtStartPar
Suitable for low\sphinxhyphen{}complexity systems and low\sphinxhyphen{}noise scenarios where
accuracy requirements are not so high.

\item {} 
\sphinxAtStartPar
I did not prepare an illustration/figure of result.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{vad_decisiontree-1}.png}


\subsubsection{Linear classifier}
\label{\detokenize{Recognition/Voice_activity_detection:linear-classifier}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Instead of manually\sphinxhyphen{}tuned, binary decisions, can we use observed
data to make a statistical estimate?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Using training data would automate the tuning of the model.Accuracy can be improved by adding more data.

\item {} 
\sphinxAtStartPar
By replacing binary decisions, we can let tendencies in several
features improve accuracy.

\end{itemize}

\item {} 
\sphinxAtStartPar
Linear classifiers attempt to achieve a decision as a weighted sum
of the features.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Let \(\xi_k\) be the features.

\item {} 
\sphinxAtStartPar
The decision is then obtained by \(\eta = \sum_k\omega_k\xi_k\),
where \(\omega_k\) are scalar weights.

\item {} 
\sphinxAtStartPar
The objective is to find weights \(\omega_k\) such that \$\(\eta = 
      \begin{cases}
        -1 & \text{non-speech} \\
        +1 & \text{speech}.
      \end{cases}\)\$

\end{itemize}

\item {} 
\sphinxAtStartPar
We then need to develop a method for choosing optimal
weights \(\omega_k\).

\item {} 
\sphinxAtStartPar
The first step is to define an \sphinxstyleemphasis{objective function}, which we can
minimize.
\begin{itemize}
\item {} 
\sphinxAtStartPar
A good starting point is the classification error.

\item {} 
\sphinxAtStartPar
If \(\eta\) is the desired class for a frame and our classifier
gives \(\hat\eta\), then the classification error is
\(\nu^2 = (\eta -\hat\eta)^2\).

\item {} 
\sphinxAtStartPar
By minimizing the classification error, we can determine optimal
parameters \(\omega_k\).

\end{itemize}

\item {} 
\sphinxAtStartPar
Let \(x_k\) be the vector of all features for a frame \(k\) and
\(X=[x_0,\,x_1\dots]\) a matrix with all features for all frames.
\begin{itemize}
\item {} 
\sphinxAtStartPar
The classification of a single frame is then \(\eta_k=x_k^Tw\).

\item {} 
\sphinxAtStartPar
The classification of all frames is then a vector \(y=X^Tw\),
where \(w\) is the vector of weights \(\omega_k\).

\item {} 
\sphinxAtStartPar
The sum of classification errors is the norm \(\|y-\hat y\|^2\).

\end{itemize}

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{vad_linearclassifier-1}.png}


\paragraph{A bit of math}
\label{\detokenize{Recognition/Voice_activity_detection:a-bit-of-math}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The minimum of the classification error \(\|y-\hat y\|^2\) can be
found by setting the partial derivative to zero. \$\(\begin{split}
      0 &= \frac\partial{\partial w}\|y-\hat y\|^2
      = \frac\partial{\partial w}\|y-X^Tw\|^2
      \\&
      = \frac\partial{\partial w}(y^Ty+w^TXX^Tw-2w^TXy)
      \\&
      =2XX^Tw - 2Xy.
    \end{split}\)\$

\item {} 
\sphinxAtStartPar
The solution is the Moore\sphinxhyphen{}Penrose pseudo\sphinxhyphen{}inverse
\$\(w = (XX^T)^{-1}Xy^T := X^\dagger y.\)\$

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Note:} This is a very common mathematical approach for solving
problems in speech processing, so it is much more important and
broadly applicable than “only” VAD.

\end{itemize}


\paragraph{Pre\sphinxhyphen{}whitening (advanced topic)}
\label{\detokenize{Recognition/Voice_activity_detection:pre-whitening-advanced-topic}}\begin{itemize}
\item {} 
\sphinxAtStartPar
If the range of values from features are very different, we end up
with problems.
\begin{itemize}
\item {} 
\sphinxAtStartPar
A very loud voice will overrun weaker ones, even if the loud one
is full of crap.

\item {} 
\sphinxAtStartPar
The range (mean and variance) of features need to be normalized.

\item {} 
\sphinxAtStartPar
Correlations between features are also undesirable.

\end{itemize}

\item {} 
\sphinxAtStartPar
The first step is removal of the mean, \(x' = x - E[x]\), where
\(E[x]\approx \frac1N\sum_kx_k\) and \(N\) is the number of frames.

\item {} 
\sphinxAtStartPar
The covariance of the features is then
\(C=E[x'(x')^T]\approx \frac1N X^TX\), where \(X\) now contains the
zero\sphinxhyphen{}mean features.

\item {} 
\sphinxAtStartPar
The eigenvalue decomposition of \(C\) is \(C=V^TDV\), whereby we can
define the pre\sphinxhyphen{}whitening transform \(A=D^{1/2}V\) and \(x'' = Ax'\).
\begin{itemize}
\item {} 
\sphinxAtStartPar
The covariance of the modified vector is
\(E[x''(x'')^T] = AE[x'(x')^T]A^T = A C A^T = D^{1/2}V V^TDV V^T D^{1/2}=I\).

\item {} 
\sphinxAtStartPar
That is, \(x''\) has uncorrelated samples with equal variance and
zero\sphinxhyphen{}mean.

\end{itemize}

\item {} 
\sphinxAtStartPar
What you need to know is that pre\sphinxhyphen{}whitening is a pre\sphinxhyphen{}processing
step, applied \sphinxstyleemphasis{before} training \(w\).

\item {} 
\sphinxAtStartPar
We thus train the classifier on the modified vectors
\(x''=A(x-E[x])\), to obtain the weights \(w\).

\item {} 
\sphinxAtStartPar
The classifier with pre\sphinxhyphen{}whitening is
\$\(\nu=w^T x''=w^T A(x-E[x]) = \hat w^T (x-E[x])\)\( where \)\textbackslash{}hat w=Aw\$.

\item {} 
\sphinxAtStartPar
In other words, the pre\sphinxhyphen{}whitening can be included in the weights, so
no additional complexity is introduced other than removal of the
mean (which is trivial).

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{vad_features3-1}.png}
\sphinxincludegraphics{{vad_features4-1}.png}


\subsubsection{Post\sphinxhyphen{}processing}
\label{\detokenize{Recognition/Voice_activity_detection:id1}}
\sphinxAtStartPar
\sphinxincludegraphics{{vad_features5-1}.png}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Above, we trained the classifier to get values \(\pm1\), but when
using the classifier, we choose classes based on a threshold.

\item {} 
\sphinxAtStartPar
In practice, it does not matter if the output value is \(0.1\), \(1\) or
\(1000\), because if it is above threshold, then it belongs to the
same class.

\item {} 
\sphinxAtStartPar
Trying to hit \(\pm1\) is therefore an unnecessarily difficult task!
\begin{itemize}
\item {} 
\sphinxAtStartPar
We should be just trying to obtain a multidimensional threshold
which separates classes!

\end{itemize}

\item {} 
\sphinxAtStartPar
We can truncate the output such that big values are reduced to
\(\pm1\).

\item {} 
\sphinxAtStartPar
The sigmoid function is a map \({\mathbb R}\rightarrow[0,1]\) defined
as \$\(f(\nu) = \frac1{1+\exp(-\nu)}.\)\$

\item {} 
\sphinxAtStartPar
If we apply the sigmoid function on the output of the linear
classifier, then overshooting is not a problem anymore:
\$\(\hat \nu = f(w^T x) \qquad\in[0,1].\)\$

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{sigmoid-1}.png}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Then there is no analytic solution, but we must use iterative
methods.

\item {} 
\sphinxAtStartPar
This function is known as a \sphinxstyleemphasis{perceptron}. Combining perceptrons into
a interconnected network gives a \sphinxstyleemphasis{neural network.}\(\Rightarrow\) A topic for other courses.

\item {} 
\sphinxAtStartPar
Linear classifiers are only slightly more complex than decisions
trees, but much more accurate.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Main complexity of VAD lies in feature\sphinxhyphen{}extraction anyway, so the
differences in complexity of decision trees and linear
classifiers is negligible.

\end{itemize}

\item {} 
\sphinxAtStartPar
The main advantages of linear classifiers in comparison to decision
trees are that
\begin{itemize}
\item {} 
\sphinxAtStartPar
(unbiased) we can use real data to train the model, whereby we
can be certain that it corresponds to reality (no bias due to
manual tuning),

\item {} 
\sphinxAtStartPar
(robust) whereas noise in one feature can break a decision tree,
linear classifiers merge information from all features, thus
reducing effect of noise.

\end{itemize}

\end{itemize}


\subsubsection{More advanced classifiers}
\label{\detokenize{Recognition/Voice_activity_detection:more-advanced-classifiers}}\begin{itemize}
\item {} 
\sphinxAtStartPar
There exists a large range of better and more complex classifiers in
the general field of machine learning.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Linear discriminant analysis (LDA) – splits the feature space
using hyper\sphinxhyphen{}planes.

\item {} 
\sphinxAtStartPar
Gaussian mixture models (GMM) – the feature space is modelled
by a sum of Gaussians.

\item {} 
\sphinxAtStartPar
Deep neural networks (DNN) – similar to linear classifiers but adds
non\sphinxhyphen{}linear mappings and several layers of sums.

\item {} 
\sphinxAtStartPar
K\sphinxhyphen{}nearest neighbors (kNN), support vector machine (SVM), random
forest classifiers, etc.

\end{itemize}

\item {} 
\sphinxAtStartPar
These methods are in general more effective, but training and
application is more complex.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Advice: Try a simple approach first and see if its good enough.

\end{itemize}

\end{itemize}


\subsection{Speech Presence Probability}
\label{\detokenize{Recognition/Voice_activity_detection:speech-presence-probability}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The output of the classifier is a continuous number, but it is
thresholded to obtain a decision.

\item {} 
\sphinxAtStartPar
The continuous output contains a lot of information about the signal
which is lost when applying thresholding.
\begin{itemize}
\item {} 
\sphinxAtStartPar
With a high value we are really certain that the signal is
speech, while a value near the threshold is relatively
uncertain.

\end{itemize}

\item {} 
\sphinxAtStartPar
We can use the classifier output as an estimate of the probability
that the signal is speech \(\Rightarrow\) It is an estimator for
\sphinxstyleemphasis{speech presence probability}.

\item {} 
\sphinxAtStartPar
Subsequent applications can use this information as input to improve
performance.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{vad_spp-1}.png}


\subsubsection{Output before thresholding}
\label{\detokenize{Recognition/Voice_activity_detection:output-before-thresholding}}
\sphinxAtStartPar
\sphinxincludegraphics{{vad_features5-1}.png}


\subsection{Noise types}
\label{\detokenize{Recognition/Voice_activity_detection:noise-types}}\begin{itemize}
\item {} 
\sphinxAtStartPar
As noted before, VAD is trivial in noise\sphinxhyphen{}free scenarios.

\item {} 
\sphinxAtStartPar
In practice, typical background noise types are for example, office
noise, car noise, cafeteria (babble) noise, street noise, factory
noise, …

\item {} 
\sphinxAtStartPar
Clearly the problem is easier if the noise has a very different
character than the speech signal.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Speech is quickly varying \(\Rightarrow\) stationary noises are
easy.

\item {} 
\sphinxAtStartPar
Speech is dominated by low frequencies \(\Rightarrow\) high
frequency noises are easy.

\end{itemize}

\item {} 
\sphinxAtStartPar
The classic worst case is a competing (undesired) speaker, when
someone else is speaking in the background (babble noise).
\begin{itemize}
\item {} 
\sphinxAtStartPar
However, that would be difficult also for a human listener,
whereby it actually is a very difficult problem.

\end{itemize}

\end{itemize}


\subsection{Conclusions}
\label{\detokenize{Recognition/Voice_activity_detection:conclusions}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Voice activity detection is a type of methods which attempt to
determine if a signal is speech or non\sphinxhyphen{}speech.
\begin{itemize}
\item {} 
\sphinxAtStartPar
In a noise\sphinxhyphen{}free scenario the task is trivial, but it is also not
a realistic scenario.

\end{itemize}

\item {} 
\sphinxAtStartPar
The basic idea of algorithms is:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Calculate a set of features from the signal which are designed
to analyze properties which differentiate speech and non\sphinxhyphen{}speech.

\item {} 
\sphinxAtStartPar
Merge the information from the features in a classifier, which
returns the likelihood that the signal is speech.

\item {} 
\sphinxAtStartPar
Threshold the classifier output to determine whether the signal
is speech or not.

\end{enumerate}

\item {} 
\sphinxAtStartPar
VADs are used as a low\sphinxhyphen{}complexity pre\sphinxhyphen{}processing method, to save
resources (e.g. complexity or bitrate) in the main task.

\end{itemize}

\sphinxstepscope


\section{Wake\sphinxhyphen{}word and keyword spotting}
\label{\detokenize{Recognition/Wake-word_and_keyword_spotting:wake-word-and-keyword-spotting}}\label{\detokenize{Recognition/Wake-word_and_keyword_spotting::doc}}
\sphinxAtStartPar
Full large\sphinxhyphen{}vocabulary speech recognition is unnecessary and far too
expensive for many practical applications. For a table lamp, it would
quite sufficient if it understands “Light off” and “Light on.”  Besides,
most of the time, speech devices are just sitting there waiting for
instructions when nobody is speaking. Speech recognition algorithms
moreover use a lot of computational resources such that it would be
wasteful to have them analyse sounds when there is no speech present.
\DUrole{xref,myst}{Voice activity detection (VAD)} takes
care of the first part, to detect whether speech is present or not, such
that all activities are in a sleep mode when speech is not present.

\sphinxAtStartPar
Wake\sphinxhyphen{}word and keyword spotting refer to small\sphinxhyphen{}vocabulary speech
recognition tasks. They are used either in very simple applications
where proper speech recognition is unnecessarily complex (keyword
spotting), and in pre\sphinxhyphen{}processing tasks, where we want to save resources
by waiting for a “Hey computer!”. In the latter task, wake\sphinxhyphen{}word spotting
is thus a trigger for more complex speech processing tasks. Though the
two tasks have rather different objectives, the underlying technology is
very similar and they will here be discussed jointly.

\sphinxAtStartPar
Most typically wake\sphinxhyphen{}word and keyword spotting algorithms run on devices
with limited resources. They can be limited in memory footprint and in
computation resources (CPU power) or often both. Increasing amount of
memory or using a larger CPU would both increase cost of device
(investment cost), but would also require more power (maintenance cost).
In small devices such marginal costs are a very significant part of the
overall cost of the device.

\sphinxAtStartPar
The overall structure of of keyword\sphinxhyphen{}spotting algorithms is illustrated
below. The input speech signal is first converted to a feature
representation, such as \DUrole{xref,myst}{MFCCs}, which are fed to a
\DUrole{xref,myst}{neural network}, and the output is the likelihood of
each keyword.

\sphinxAtStartPar
\sphinxincludegraphics{{155474775}.png}

\sphinxAtStartPar
Adapted from {[}\hyperlink{cite.References:id44}{Zhang \sphinxstyleemphasis{et al.}, 2017}{]}

\sphinxAtStartPar
In other words, keyword and wake\sphinxhyphen{}word spotters have a small set of
accepted keywords, which are hard\sphinxhyphen{}coded into the software. If we have
\sphinxstyleemphasis{N} possible keywords, then the neural network has \sphinxstyleemphasis{N} outputs
corresponding to the probability that the input is each of those
keywords. The output is then thresholded such that that keyword is
chosen which has the largest probability.

\sphinxAtStartPar
The choice of structure for the neural network is highly dependent on
the device constraints as well as the objective, operating environment
and context of the application. Usually the structure is a deep neural
network featuring some recurrent, convolutional and long\sphinxhyphen{}short term
memory (LSTM) components.

\sphinxAtStartPar
A central challenge in training keyword spotting algorithms is finding
and choosing training data. To get good quality, you would typically
need several tens of thousands of utterances of the keywords, spoken by
a large range of different speakers and in different environments. For
example the \sphinxhref{https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html}{“Speech Commands Dataset” by
Google}
has 65.000 utterances of 30 short words. However, the choice of keywords
is naturally dependent on those functions that the keyword spotter
should activate, or the desired wake\sphinxhyphen{}word. For real\sphinxhyphen{}world applications
we therefore often cannot use pre\sphinxhyphen{}collected datasets, but have to
collect our own. You can just imagine the workload required to collect
65.000 utterances from over a thousand speakers!


\subsection{References}
\label{\detokenize{Recognition/Wake-word_and_keyword_spotting:references}}
\sphinxstepscope


\section{Speech Recognition}
\label{\detokenize{Recognition/Speech_Recognition:speech-recognition}}\label{\detokenize{Recognition/Speech_Recognition::doc}}

\subsection{Introduction to ASR}
\label{\detokenize{Recognition/Speech_Recognition:introduction-to-asr}}
\sphinxAtStartPar
An ASR system produces the most likely word sequence given an incoming
speech signal.  The statistical approach for speech recognition has
dominated Automatic Speech Recognition (ASR) research over the last few
decades leading to a number of successes. The problem of speech
recognition is defined as the conversion of spoken utterances into
textual sentences by a machine.  In the statistical framework, the
Bayesian decision rule is employed to find the most probable word
sequence, \( \hat H \) , given the observation sequence \( O = (o_1,
. . . , o_T ) \) :
\begin{equation*}
\begin{split} \hat H= \operatorname*{argmax}_H \;P(H|O) \end{split}
\end{equation*}
\sphinxAtStartPar
Following Bayes’ rule, the posterior probability in the above equation
can be expressed as a conditional probability of the word sequence given
the acoustic observations,  \( P(O|H) \) , multiplied by a prior
probability of the word sequence,  \( P(H) \) , and normalized by the
marginal likelihood of observation sequences, \( P(O) \) :
\begin{equation*}
\begin{split} \hat H= \operatorname*{argmax}_H \; \frac {P(O|H)\;P(H)}
{P(O)} $$ $$ \hat H= \operatorname*{argmax}_H \; P(O|H)\;P(H)
\end{split}
\end{equation*}
\sphinxAtStartPar
The marginal probability, \( P(O) \) , is discarded in the second
equation since it is constant with respect to the ranking of hypotheses,
and hence does not alter the search for the best hypothesis.  \(
P(O|H) \) is calculated by the acoustic model and  \( P(H) \) is
modeled by the language model.


\subsection{Component of ASR}
\label{\detokenize{Recognition/Speech_Recognition:component-of-asr}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Feature Extraction: It converts the speech signal into a sequence of
acoustic feature vectors. These observations should be compact and
carry sufficient information for recognition in the later stage.

\item {} 
\sphinxAtStartPar
Acoustic Model: It Contains a statistical representation of the
distinct sounds that make up each word in the Language Model or
Grammar.  Each distinct sound corresponds to a phoneme.

\item {} 
\sphinxAtStartPar
Language Model: It contain a very large list of words and their
probability of occurrence in a given sequence.

\item {} 
\sphinxAtStartPar
Decoder: It is a software program that takes the sounds spoken by a
user and searches the acoustic Model for the equivalent sounds. 
When a match is made, the decoder determines the phoneme
corresponding to the sound.  It keeps track of the matching phonemes
until it reaches a pause in the users speech.  It then searches the
language model  for the equivalent series of phonemes.  If a match
is made, it returns the text of the corresponding word or phrase to
the calling program.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{165127140}.png}
Architecture of an ASR system


\subsection{Types of ASR}
\label{\detokenize{Recognition/Speech_Recognition:types-of-asr}}
\sphinxAtStartPar
Speech recognition systems can be classified on the basis of the
constraints under which they are developed and which they consequently
impose on their users. These constraints include: speaker dependence,
type of utterance, size of the vocabulary, linguistic constraints, type
of speech and environment of use. We will describe each constraint as
follows:

\sphinxAtStartPar
\sphinxstylestrong{Speaker Dependence}: Speaker dependent speech recognition system
requires the user to be involved in its development whereas speaker
independent systems do not. Speaker independent systems can be used by
anybody. Speaker dependent systems usually perform much better than
speaker independent systems. This is due to the fact that the acoustic
variations among different speakers are very difficult to describe and
model. There are approaches to make a system speaker independent. The
first one is the use of multiple representations for each reference to
capture the speaker variation and the second one is the speaker
adaptation approach.

\sphinxAtStartPar
\sphinxstylestrong{Type of Utterance}: A speech recognizer may recognize every word
independently. It may require its user to speak each word in a sentence
separating them by artificial pause or it may allow the user to speak in
a natural way. The first type of system is categorized as an isolated
word recognition system. It is the simplest form of a recognition
strategy. It can be developed using word\sphinxhyphen{}based acoustic models without
any language model. If, however, the vocabulary increases sentences
composed of isolated words to be recognized, the use of sub\sphinxhyphen{}word
acoustic models and language models become important. The second one is
the continuous speech recognition systems. It allows the users to utter
the message in a relatively or completely unconstrained manner. Such
recognizers must be capable of performing well in the presence of all
the co\sphinxhyphen{}articulatory effects. Developing continuous speech recognition
systems is, therefore, the most difficult task. This is due to the
following properties of continuous speech:  word boundaries are unclear
in continuous speech; and co\sphinxhyphen{}articulatory effects are much stronger in
continuous speech

\sphinxAtStartPar
\sphinxstylestrong{Vocabulary Size}: The number of words in the vocabulary is a
constraint that makes a speech recognition system small, medium or
large. As a rule of thumb, small vocabulary systems are those which have
a vocabulary size in the range of 1\sphinxhyphen{}99 words; medium, 100\sphinxhyphen{}999 words; and
large, 1000 words or more. Large vocabulary speech recognition systems
perform much worse compared to small vocabulary systems due to different
factors such as word confusion that increases with the number of words
in the vocabulary. For small vocabulary recognizer, each word can be
modeled. However, it is not possible to train acoustic models for
thousands of words separately because we cannot have enough training
speech and storage for parameters of the speech that is needed. The
development of large vocabulary recognizer, therefore, requires the use
of sub\sphinxhyphen{}word units. On the other hand, the use of sub\sphinxhyphen{}word units results
in performance degradation since they cannot capture co\sphinxhyphen{}articulatory
effects as words do. The search process in large vocabulary recognizer
also uses pruning instead of performing a complete search.

\sphinxAtStartPar
\sphinxstylestrong{Type of Speech:} A speech recognizer can be developed to recognize
only read speech or to allow the user speak spontaneously. The latter is
more difficult to build than the former due to the fact that spontaneous
speech is characterized by false starts, incomplete sentences, unlimited
vocabulary and reduced pronunciation quality. The primary difference in
recognition error rates between read and spontaneous speech are due to
disfluencies in spontaneous speech. Disfluencies in spontaneous speech
can be characterized by long pauses and mispronunciations. Spontaneous
is, therefore, both acoustically and grammatically difficult to
recognize.

\sphinxAtStartPar
\sphinxstylestrong{Environment}: Speech recognizer may require the speech to be clean
from environmental noises, acoustic distortions, microphones and
transmission channel distortions or they may ideally handle any of these
problems. While current speech recognizer give acceptable performance in
carefully controlled environments, their performance degrades rapidly
when they are applied in noisy environments. This noise can take the
form of speech from other speakers; equipment sounds, air conditioners
or others. The noise might also be created by the speaker himself in a
form of lip smacks, coughs or sneezes.


\subsection{Models for Large Vocabulary Speech Recognition (LVCSR)}
\label{\detokenize{Recognition/Speech_Recognition:models-for-large-vocabulary-speech-recognition-lvcsr}}
\sphinxAtStartPar
LVCSR can be divided into two categories: HMM\sphinxhyphen{}based model and the
end\sphinxhyphen{}to\sphinxhyphen{}end model.


\subsubsection{HMM\sphinxhyphen{}Based Model}
\label{\detokenize{Recognition/Speech_Recognition:hmm-based-model}}
\sphinxAtStartPar
The HMM\sphinxhyphen{}based model has been the main LVCSR model for many years with
the best recognition accuracy. An HMM\sphinxhyphen{}based model is divided into three
parts:acoustic, pronunciation and language model. In HMM based model,
each model is independent of each other and plays a different role.
While the acoustic model models the mapping between speech input and
feature sequence, the pronunciation model maps between phonemes (or
sub\sphinxhyphen{}phonemes) to graphemes, and the language model maps the character
sequence to fluent final transcription.

\sphinxAtStartPar
\sphinxstylestrong{Acoustic Model:}  In the acoustic model, the observation
probability  is generally represented by GMM. The posterior probability
distribution of hidden state can be calculated by DNN method. These two
different calculations result into two different models, namely HMM\sphinxhyphen{}GMM
and HMM\sphinxhyphen{}DNN. HMM\sphinxhyphen{}GMM model was a general structure for many speech
recognition systems. However, with the development of deep learning
technology, DNN is introduced into speech recognition for acoustic
modeling. DNN has been used to calculate the posterior probability of
the HMM state replacing the conventional GMM observation probability.
Thus, HMM\sphinxhyphen{}GMM model is replaced by HMM\sphinxhyphen{}DNN since HMM\sphinxhyphen{}GMM provides better
results compared to HMM\sphinxhyphen{}GMM and becomes state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art ASR model. In
the HMM\sphinxhyphen{}based model, different modules use different technologies and
have different roles. While the HMM is mainly used to do dynamic time
warping at the frame level, GMM and DNN are used to calculate emission
probability of HMM hidden states.

\sphinxAtStartPar
\sphinxstylestrong{Pronunciation Model:} Its main objective is achieve the connection
between acoustic sequence and language sequence. The dictionary includes
various levels of mapping, such as pronunciation to phone, phone to
trip\sphinxhyphen{}hone. The dictionary is used to achieve structural mapping and map
the probability calculation relationship.

\sphinxAtStartPar
\sphinxstylestrong{Language Model:} It contains rudimentary syntactic information. Its
aim is to predict the likelihood of specific words occurring one after
another in a given language. Typical recognizers use n\sphinxhyphen{}gram language
models. An n\sphinxhyphen{}gram contains the prior probability of the occurrence of a
word (unigram), or of a sequence of words (bigram, trigram etc.):

\sphinxAtStartPar
unigram probability \( P(w_i) \)

\sphinxAtStartPar
bigram probability \( P(w_i|w_{i−1}) \)

\sphinxAtStartPar
ngram probability \( P(w_n|w_{n−1},w_{n−2}, …,w_1) \)

\sphinxAtStartPar
\sphinxstylestrong{Limitations of HMM\sphinxhyphen{}models}
\begin{itemize}
\item {} 
\sphinxAtStartPar
The training process is complex and difficult to be globally
optimized. HMM\sphinxhyphen{}based model often uses different training methods and
data sets to train different modules. Each module is independently
optimized with their own optimization objective functions which are
generally different from the true LVCSR performance evaluation
criteria. So the optimality of each module does not necessarily
bring global optimality.

\item {} 
\sphinxAtStartPar
Conditional independence assumptions. To simplify the model’s
construction and training, the HMM\sphinxhyphen{}based model uses conditional
independence assumptions within HMM and between different modules.
This does not match the actual situation of LVCSR.

\end{itemize}


\subsubsection{End\sphinxhyphen{}to\sphinxhyphen{}End Model}
\label{\detokenize{Recognition/Speech_Recognition:end-to-end-model}}
\sphinxAtStartPar
Because of the  above\sphinxhyphen{}mentioned shortcomings of the HMM\sphinxhyphen{}based model and
coupled with the promotion of deep learning technology, more and more
works began to study end\sphinxhyphen{}to\sphinxhyphen{}end LVCSR. The end\sphinxhyphen{}to\sphinxhyphen{}end model is a system
that directly maps input audio sequence to sequence of words or other
graphemes.

\sphinxAtStartPar
\sphinxincludegraphics{{165127650}.png}
Function structure of end\sphinxhyphen{}to\sphinxhyphen{}end model

\sphinxAtStartPar
Most end\sphinxhyphen{}to\sphinxhyphen{}end speech recognition models include the following parts:
the encoder maps speech input sequence to feature sequence; the aligner
realizes the alignment between feature sequence and language; the
decoder decodes the final identification result. Note that this division
does not always exist since end\sphinxhyphen{}to\sphinxhyphen{}end itself is a complete
structure. Contrary to the HMM\sphinxhyphen{}based model that  consists of multiple
modules, the end\sphinxhyphen{}to\sphinxhyphen{}end model replaces multiple modules with a deep
network, realizing the direct mapping of acoustic signals into label
sequences without carefully\sphinxhyphen{}designed intermediate states. In addition to
this, there is no need to perform posterior processing on the output.

\sphinxAtStartPar
Compared to HMM\sphinxhyphen{}based model, the main characteristics of end\sphinxhyphen{}to\sphinxhyphen{}end
LVCSR are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Multiple modules are merged into one network for joint training. The
benefit of merging multiple modules is there is no need to design
many modules to realize the mapping between various intermediate
states. Joint training enables the end\sphinxhyphen{}to\sphinxhyphen{}end model to use a
function that is highly relevant to the final evaluation criteria as
a global optimization goal, thereby seeking globally optimal
results.

\item {} 
\sphinxAtStartPar
It directly maps input acoustic signature sequence to the text
result sequence, and does not require further processing to achieve
the true transcription or to improve recognition performance . But,
in the HMM\sphinxhyphen{}based models, there is usually an internal representation
for pronunciation of a character chain.

\end{itemize}

\sphinxAtStartPar
These features of of end\sphinxhyphen{}to\sphinxhyphen{}end LVCSR model enables to greatly simplify
the construction and training of speech recognition models.

\sphinxAtStartPar
The end\sphinxhyphen{}to\sphinxhyphen{}end model are mainly divided into three different categories
depending on their implementations of soft alignment:
\begin{itemize}
\item {} 
\sphinxAtStartPar
CTC\sphinxhyphen{}based: It first enumerates all possible hard alignments. Then,
it achieves soft alignment by aggregating these hard alignments. CTC
assumes that output labels are independent of each other when
enumerating hard alignments.

\item {} 
\sphinxAtStartPar
RNN\sphinxhyphen{}transducer: It also enumerates all possible hard alignments and
then aggregates them for soft alignment. But unlike CTC,
RNN\sphinxhyphen{}transducer does not make independent assumptions about labels
when enumerating hard alignments. Thus, it is different from CTC in
terms of path definition and probability calculation.

\item {} 
\sphinxAtStartPar
Attention\sphinxhyphen{}based: This method no longer enumerates all possible hard
alignments, but uses attention mechanism to directly calculate the
soft alignment information between input data and output label.

\end{itemize}

\sphinxAtStartPar
CTC\sphinxhyphen{}Based End\sphinxhyphen{}to\sphinxhyphen{}End Model

\sphinxAtStartPar
Although HMM\sphinxhyphen{}DNN provides still state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art results, the role
played by DNN is limited. It is mainly used to model the posterior state
probability of HMM’s hidden state. The time\sphinxhyphen{}domain feature is still
modeled by HMM. When attempting to model time\sphinxhyphen{}domain features using RNN
or CNN instead of HMM, it faces a data alignment problem: both RNN and
CNN’s loss functions are defined at each point in the sequence, so in
order to be able to perform training, it is necessary to know the
alignment relation between RNN output sequence and target sequence.

\sphinxAtStartPar
CTC makes it possible to make fuller use of DNN in speech recognition
and build end\sphinxhyphen{}to\sphinxhyphen{}end models, which is a breakthrough in the development
of end\sphinxhyphen{}to\sphinxhyphen{}end method. Essentially, CTC is a loss function, but it solves
hard alignment problem while calculating the loss. CTC mainly overcomes
the following two difficulties for end\sphinxhyphen{}to\sphinxhyphen{}end LVCSR models:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data alignment problem. CTC no longer needs to segment and align
training data. This solves the alignment problem so that DNN can be
used to model time\sphinxhyphen{}domain features, which greatly enhances DNN’s
role in LVCSR tasks.

\item {} 
\sphinxAtStartPar
Directly output the target transcriptions. Traditional models often
output phonemes or other small units, and further processing is
required to obtain the final transcriptions. CTC eliminates the need
for small units and direct output in final target form, greatly
simplifying the construction and training of end\sphinxhyphen{}to\sphinxhyphen{}end model.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{RNN\sphinxhyphen{}Transducer End\sphinxhyphen{}to\sphinxhyphen{}End Model}

\sphinxAtStartPar
CTC has two main deficiencies in CTC which hinder its effectiveness:
\begin{itemize}
\item {} 
\sphinxAtStartPar
CTC cannot model interdependencies within the output sequence
because it assumes that output elements are independent of each
other. Therefore, CTC cannot learn the language model. The speech
recognition network trained by CTC should be treated as only an
acoustic model.

\item {} 
\sphinxAtStartPar
CTC can only map input sequences to output sequences that are
shorter than it. Thus, it is powerless for scenarios where output
sequence is longer.

\end{itemize}

\sphinxAtStartPar
For speech recognition, the first point has huge impact. RNN\sphinxhyphen{}transducer
was proposed to solve the above\sphinxhyphen{}mentioned shortcomings of CTC.
Theoretically, it can map an input to any finite, discrete output
sequence. Interdependencies between input and output and within output
elements are also jointly modeled.

\sphinxAtStartPar
The RNN\sphinxhyphen{}transducer has many similarities with CT: their main goals is to
solve the forced segmentation alignment problem in speech recognition;
they both introduce a “blank” label; they both calculate the probability
of all possible paths and aggregate them to get the label sequence.
However, their path generation processes and the path probability
calculation methods are completely different. This gives rise to the
advantages of RNN\sphinxhyphen{}transducer over CTC.


\subsection{Types of errors made by speech recognizers}
\label{\detokenize{Recognition/Speech_Recognition:types-of-errors-made-by-speech-recognizers}}
\sphinxAtStartPar
Though ASR research has come a long way, today’s systems are far from
being perfect. Speech recognizer are brittle and make errors due to
various causes. Most errors made by ASR systems fall into one of the
following categories:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Out\sphinxhyphen{}of\sphinxhyphen{}vocabulary (OOV) errors}: Current state of the art speech
recognizers have closed vocabularies. This means that they are
incapable of recognizing words outside their training vocabulary.
Besides misrecognition, the presence of an out\sphinxhyphen{}of\sphinxhyphen{}vocabulary word in
input utterance causes the system to err to a similar word in its
vocabulary. Special techniques for handling OOV words have been
developed for HMM\sphinxhyphen{}GMM and neural ASR systems (see, e.g., {[}\hyperlink{cite.References:id45}{Zhang, 2019}{]}).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Homophone substitution}: These errors can occur if more than one
lexical entry has the same pronunciation (phone sequence), i.e.,
they are homophones. While decoding, homophones may be confused with
one another causing errors. In general, a well\sphinxhyphen{}functioning language
model should disambiguate homophones based on the context.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Language model bias}: Because of an undue bias  towards the
language model (effected by a high relative weight on the language
model), the decoder may be forced to reject the true hypothesis in
favor of a spurious candidate with high language model probability.
These errors may occur along with analogous acoustic model bias.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multiple acoustic problems}: This is a broad category of errors
comprising those due to bad pronunciation entries; disfluency,
mispronunciation by the speaker himself/herself, or errors made by
acoustic models (possibly due to acoustic noise, data mismatch
between training and usage etc.).

\end{itemize}


\subsection{Challenges of ASR}
\label{\detokenize{Recognition/Speech_Recognition:challenges-of-asr}}
\sphinxAtStartPar
Recent advances in ASR has brought automatic speech recognition accuracy
close to human performance in many practical tasks. However, there are
still challenges:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}vocabulary words are difficult to recognize correctly

\item {} 
\sphinxAtStartPar
Varying environmental noises impair recognition accuracy.

\item {} 
\sphinxAtStartPar
Overlapping speech is problematic for ASR system.

\item {} 
\sphinxAtStartPar
Recognizing children’s speech and the speech of people with speech
production disabilities is suboptimal with regular training data.

\item {} 
\sphinxAtStartPar
DNN\sphinxhyphen{}based models usually require a lot of data for training, in the
order of thousands of hours. End\sphinxhyphen{}to\sphinxhyphen{}end models may need up to
100,000h of speech to reach high performance.

\item {} 
\sphinxAtStartPar
Uncertainty self\sphinxhyphen{}awareness is limited: typical ASR systems always
output the most likely word sequence instead of reporting if some
part of the input was incomprehensible or highly uncertain.

\end{itemize}


\subsection{Evaluation}
\label{\detokenize{Recognition/Speech_Recognition:evaluation}}
\sphinxAtStartPar
The performance of an ASR system is measured by comparing the
hypothesized transcriptions and reference transcriptions. Word error
rate (WER) is the most widely used metric. The two word sequences are
first aligned using a dynamic programming\sphinxhyphen{}based string alignment
algorithm. After the alignment, the number of deletions (D),
substitutions (S), and insertions (I) are determined. The
deletions, substitutions and insertions are all considered as errors,
and the WER is calculated by the rate of the number of errors to the
number of words (N) in the reference.
\begin{equation*}
\begin{split} WER = \frac{I + D + S}{N} * 100\% \end{split}
\end{equation*}
\sphinxAtStartPar
Sentence Error Rate (SER) is also sometime used to evaluate the
performance of ASR systems. SER computes the percentage of sentences
with at least one error.


\subsubsection{References}
\label{\detokenize{Recognition/Speech_Recognition:references}}
\sphinxstepscope


\section{Speaker Recognition and Verification}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:speaker-recognition-and-verification}}\label{\detokenize{Recognition/Speaker_Recognition_and_Verification::doc}}

\subsection{\sphinxstylestrong{1. Introduction to Speaker Recognition}}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:introduction-to-speaker-recognition}}
\sphinxAtStartPar
Speaker recognition is the task of identifying a speaker using their
voice. Speaker recognition is classified into two parts: speaker
identification and speaker verification. While speaker identification is
the process of determining which voice in a group of known voices best
matches the speaker’, speaker verification is the task of accepting or
rejecting the identity claim of a speaker by analyzing their acoustic
samples. Speaker verification systems are computationally less complex
than speaker identification systems since they require a comparison
between only one or two models, whereas speaker identification requires
comparison of one model to N speaker models.

\sphinxAtStartPar
Speaker verification methods are divided into text\sphinxhyphen{}dependent and
text\sphinxhyphen{}independent methods. In text\sphinxhyphen{}dependent methods, the speaker
verification system has prior knowledge about the text to be spoken and
the user is expected to speak this text. However, in a text\sphinxhyphen{}independent
system, the system has no prior knowledge about the text to be spoken
and the user is not expected to be cooperative. Text\sphinxhyphen{}dependent systems
achieve high speaker verification performance from relatively short
utterances, while text\sphinxhyphen{}independent systems require long utterances to
train reliable models and achieve good performance.

\sphinxAtStartPar
Block diagram of a basic speaker verification system

\sphinxAtStartPar
As it is shown in the above block diagram of a basic speaker
verification system, a speaker verification system involves two main
phases: the training phase in which the target speakers are enrolled and
the testing phase in which a decision about the identity of the speaker
is taken. From a training point of view, speaker models can be
classified into generative and discriminative. Generative models such as
Gaussian Mixture Model (GMM) estimate the feature distribution within
each speaker. Discriminative models such as Support Vector Machine and
Deep Neural Network (DNN), in contrast, model the boundary between
speakers.

\sphinxAtStartPar
The performance of speaker verification systems is degraded by the
variability in channels and sessions between enrolment and verification
speech signals. Factors which affect channel/session variability
include:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Channel mismatch between enrolment and verification speech signals
such as using different microphones in enrolment and verification
speech signals.

\item {} 
\sphinxAtStartPar
Environmental noise and reverberation conditions.

\item {} 
\sphinxAtStartPar
The differences in speaker voice such as ageing, health, speaking
style and emotional state.

\item {} 
\sphinxAtStartPar
Transmission channel such as landline, mobile phone, microphone and
voice over Internet protocol (VoIP).

\end{enumerate}


\subsection{2. Front\sphinxhyphen{}end Processing}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:front-end-processing}}
\sphinxAtStartPar
Many front\sphinxhyphen{}end processing are often used to process the speech signals
and to extract the features which are used in the speaker verification
system. The front\sphinxhyphen{}end processing consists of mainly voice activity
detection (VAD), feature extraction and channel compensation techniques;
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{\DUrole{xref,myst}{Voice activity detection (VAD)}};
The main goal of voice activity detection is to determine which
segments of a signal are speech and non\sphinxhyphen{}speech. A robust VAD
algorithm can improve the performance of a speaker verification
system by making sure that speaker identity is calculated only from
speech regions. Therefore, it is necessary to review the VAD
algorithm to overcome the problems in designing a robust speaker
verification system. The three widely used techniques for VAD are
the following: energy based, model based and hybrid approaches.

\item {} 
\sphinxAtStartPar
Feature extraction techniques are used to transform the speech
signals into acoustic feature vectors. Thus, the extracted acoustic
features should carry the essential characteristics of the speech
signal which recognizes the identity of the speaker by their voice.
The aim of feature extraction is to reduce the dimension of acoustic
feature vectors by removing unwanted information and emphasizing the
speaker\sphinxhyphen{}specific information. The MFCCs are commonly used as the
feature extraction technique for the modern speaker verification.

\item {} 
\sphinxAtStartPar
Channel compensation techniques are used to reduce the effect of
channel mismatch and environmental noise. Channel compensation can
be used in different stages of speaker verification such as feature
and model domains. Various channel compensation techniques such as
cepstral mean subtraction (CMS) , feature warping, cepstral mean
variance normalization (CMVN)  and relative spectral (RASTA)
processing have been used to reduce the effect of channel mismatch
during the feature extraction phase. In the model domain, Joint
Factor Analysis (JFA) and i\sphinxhyphen{}vectors are used to combat enrolment and
verification mismatch.

\end{enumerate}


\subsection{3. Speaker Modeling Techniques}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:speaker-modeling-techniques}}
\sphinxAtStartPar
One of the crucial issues in speaker diarization is the techniques
employed for speaker modeling. Several modeling techniques have been
used in speaker recognition and speaker diarization tasks. The
state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art speaker modeling techniques in speaker diarization are
the following:


\subsection{3.1. Gaussian Mixture Modeling (GMM) \sphinxhyphen{} Universal Background Model (UBM) Approach}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:gaussian-mixture-modeling-gmm-universal-background-model-ubm-approach}}
\sphinxAtStartPar
A Gaussian Mixture Model (GMM) is a parametric probability density
function represented as a weighted sum of Gaussian component densities.
GMMs have been successfully used to model the speech features in
different speech processing applications. A Gaussian mixture model is a
weighted sum of M component Gaussian densities. Each of the components
is a multi\sphinxhyphen{}variant Gaussian function. A GMM is represented by mean
vectors, covariance matrices and mixture weights.

\sphinxAtStartPar
\textbackslash{}{[} \textbackslash{}lambda = \textbackslash{}\{w\_i, \textbackslash{}mu\_i, \textbackslash{}Sigma\_i\textbackslash{}\}, \textbackslash{}quad i=
1,…….,\textbackslash{}textit\{C\} \textbackslash{}{]}

\sphinxAtStartPar
The covariance matrices of a GMM,  \textbackslash{}( \textbackslash{}Sigma\_i \textbackslash{}) , can be full rank
or constrained to be diagonal. The parameters of a GMM can also be
shared, or tied, among the Gaussian components. The number of GMM
components and type of covariance matrices are often determined based on
the amount of data available for estimating GMM parameters.

\sphinxAtStartPar
In speaker recognition, a speaker can be modeled by a GMM from training
data or using Maximum A Posteriori (MAP) adaptation. While the speaker
model is built using the training utterances of a specific speaker in
the GMM training, the model is also usually adapted from a large number
of speakers called Universal Background Model in MAP adaptation.

\sphinxAtStartPar
Given a set of training vectors and a GMM configuration, there are
several techniques available for estimating the parameters of a GMM .
The most popular and used method is the maximum likelihood (ML)
estimation.

\sphinxAtStartPar
The ML estimation finds the model parameters that maximize the
likelihood of the GMM given a set of data. Assuming an independence
between the training vectors \textbackslash{}( X = \textbackslash{}\{x\_i,\textbackslash{}dots,x\_N\textbackslash{}\} \textbackslash{}) , the GMM
likelihood is typically described as:

\sphinxAtStartPar
\textbackslash{}{[} p(X|\textbackslash{}lambda) = \textbackslash{}prod\_\{t=1\}\textasciicircum{}\{N\} p(x\_t|\textbackslash{}lambda) (1) \textbackslash{}{]}

\sphinxAtStartPar
Since direct maximization is not possible on equation on equation 1, the
ML parameters are obtained iteratively using expectation\sphinxhyphen{}maximization
(EM) algorithm.  The EM iteratively estimate new model parameters  \textbackslash{}(
\textbackslash{}bar\{\textbackslash{}lambda\} \textbackslash{})  based on a given model \(\\lambda\) such that \textbackslash{}(
p(X|\textbackslash{}bar\{\textbackslash{}lambda\}) \textbackslash{}ge p(X|\textbackslash{}lambda) \textbackslash{}) .

\sphinxAtStartPar
Example of
speaker model adaptation

\sphinxAtStartPar
The parameters of a GMM can also be estimated using Maximum A Posteriori
(MAP) estimation, in addition to the EM algorithm. The MAP estimation
technique derives a speaker model by adapting from a universal
background model (UBM). The “Expectation” step of EM and MAP are the
same. MAP adapts the new sufficient statistics by combining them with
old statistics from the prior mixture parameters.

\sphinxAtStartPar
Given a prior model and training vectors from the desired class, \textbackslash{}( X =
\{x\_1 … , x\_T \} \textbackslash{}) , we first determine the probabilistic alignment
of the training vectors into the prior mixture components. For mixture 
\textbackslash{}( i \textbackslash{}) in the prior model  \textbackslash{}( Pr(i|x\_t,\textbackslash{}lambda\_\{UBM\}) \textbackslash{}) is
computed as the percentage of the mixture component  \textbackslash{}( i \textbackslash{}) to the
total likelihood,

\sphinxAtStartPar
\textbackslash{}{[} Pr(i|x\_t,\textbackslash{}lambda\_\{UBM\})= \textbackslash{}frac \{w\_i\textbackslash{},g(x\_t|\textbackslash{}mu\_i,\textbackslash{}Sigma\_i)\}
\{\textbackslash{}sum\_\{i=1\}\textasciicircum{}\{M\} w\_i\textbackslash{},g(x\_t|\textbackslash{}mu\_i,\textbackslash{}Sigma\_i)\} \textbackslash{}{]}

\sphinxAtStartPar
Then, the sufficient statistics for the weight, mean and variance
parameters is computed as follows:

\sphinxAtStartPar
\textbackslash{}{[} n\_i=\textbackslash{}sum\_\{t=1\}\textasciicircum{}\{T\}Pr(i|x\_t,\textbackslash{}lambda\_\{prior\}) \textbackslash{}, weight \textbackslash{}{]}
\textbackslash{}{[}
E\_i(x)=\textbackslash{}frac\{1\}\{n\_i\}\textbackslash{}sum\_\{t=1\}\textasciicircum{}\{T\}Pr(i|x\_t,\textbackslash{}lambda\_\{prior\})x\_t
\textbackslash{};\textbackslash{}; mean \textbackslash{}{]} \textbackslash{}{[}
E\_i(x\textasciicircum{}2)=\textbackslash{}frac\{1\}\{n\_i\}\textbackslash{}sum\_\{t=1\}\textasciicircum{}\{T\}Pr(i|x\_t,\textbackslash{}lambda\_\{prior\})x\_t\textasciicircum{}2
\textbackslash{};\textbackslash{}; variance \textbackslash{}{]}

\sphinxAtStartPar
Finally, the new sufficient statistics from the training data are used
to update the prior sufficient statistics for mixture  \textbackslash{}( i \textbackslash{}) to
create the adapted mixture weight, mean and variance for mixture
\textbackslash{}textit\{i\} as follows:

\sphinxAtStartPar
\textbackslash{}{[} w\_i={[}\textbackslash{}alpha\textasciicircum{}w\_in\_i/T + (1\sphinxhyphen{}\textbackslash{}alpha\textasciicircum{}w\_i)w\_i{]}\textbackslash{}gamma \textbackslash{}{]} \textbackslash{}{[}
\textbackslash{}mu\_i=\textbackslash{}alpha\textasciicircum{}m\_iE\_i(x) + (1\sphinxhyphen{}\textbackslash{}alpha\textasciicircum{}m\_i)\textbackslash{}mu\_i \textbackslash{}{]} \textbackslash{}{[}
\textbackslash{}mu\textasciicircum{}2\_i=\textbackslash{}alpha\textasciicircum{}v\_iE\_i(x\textasciicircum{}2) +
(1\sphinxhyphen{}\textbackslash{}alpha\textasciicircum{}v\_i)(\textbackslash{}sigma\textasciicircum{}2\_i+\textbackslash{}mu\textasciicircum{}2\_i)\sphinxhyphen{}\textbackslash{}mu\textasciicircum{}2\_i \textbackslash{}{]}

\sphinxAtStartPar
The adaptation coefficients controlling the balance between old and new
estimates are  \textbackslash{}( \textbackslash{}\{\textbackslash{}alpha\textasciicircum{}w\_i, \textbackslash{}alpha\textasciicircum{}m\_i, \textbackslash{}alpha\textasciicircum{}v\_i\textbackslash{}\} \textbackslash{}) for
the weights, means and variances, respectively. The scale factor, \textbackslash{}(
\textbackslash{}gamma \textbackslash{}) , is computed over all adapted mixture weights to ensure
they sum to unity.


\subsection{3.2. i\sphinxhyphen{}Vectors}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:i-vectors}}
\sphinxAtStartPar
Different approaches have been developed recently to improve the
performance of speaker recognition systems. The most popular ones were
based on GMM\sphinxhyphen{}UBM. The Joint Factor Analysis (JFA) is then built on the
success of the GMM\sphinxhyphen{}UBM approach. JFA modeling defines two distinct
spaces: the speaker space defined by the eigenvoice matrix and the
channel space represented by the eigen\sphinxhyphen{}channel matrix. The channel
factors estimated using JFA, which are supposed to model only channel
effects, also contain information about speakers. A new speaker
verification system has been proposed using factor analysis as a feature
extractor that defines only a single space, instead of two separate
spaces. In this new space, a given speech recording is represented by a
new vector, called total factors as it contains the speaker and channel
variabilities simultaneously. Speaker recognition based on the i\sphinxhyphen{}vector
framework is currently the state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art in the field.

\sphinxAtStartPar
Given an utterance, the speaker and channel dependent GMM supervector is
defined as follows:

\sphinxAtStartPar
\textbackslash{}{[} M=m+Tw \textbackslash{}{]}

\sphinxAtStartPar
where  \textbackslash{}( m \textbackslash{}) is a speaker and channel independent supervector,  \textbackslash{}(
T \textbackslash{}) is a rectangular matrix of low rank and  \textbackslash{}( w \textbackslash{}) is a random
vector having a standard normal distribution \textbackslash{}( N(0,1) \textbackslash{}) . The
components of the vector  \textbackslash{}( w \textbackslash{}) are the total factors. These new
vectors are called i\sphinxhyphen{}vectors.  \textbackslash{}( M \textbackslash{}) is assumed to be normally
distributed with mean vector and covariance matrix \textbackslash{}( TT\textasciicircum{}t \textbackslash{}) .

\sphinxAtStartPar
The total factor is a hidden variable, which can be defined by its
posterior distribution conditioned to the Baum–Welch statistics for a
given utterance. This posterior distribution is a Gaussian distribution
and the mean of this distribution corresponds exactly to i\sphinxhyphen{}vector. The
Baum–Welch statistics are extracted using the UBM.

\sphinxAtStartPar
Given a sequence of L frames  \textbackslash{}( \textbackslash{}\{y\_1,y\_2,……,y\_n\textbackslash{}\} \textbackslash{}) and a
UBM  \textbackslash{}( \textbackslash{}Omega \textbackslash{}) composed of \textbackslash{}( C \textbackslash{}) mixture components defined in
some feature space of dimension \textbackslash{}( F \textbackslash{}) , the Baum–Welch statistics
needed to estimate the i\sphinxhyphen{}vector for a given speech utterance  \textbackslash{}( u \textbackslash{})
is given by :

\sphinxAtStartPar
\textbackslash{}{[} N\_c=\textbackslash{}sum\_\{t=1\}\textasciicircum{}\{L\}P(c|y\_t,\textbackslash{}Omega) \textbackslash{}{]} \textbackslash{}{[}
F\_c=\textbackslash{}sum\_\{t=1\}\textasciicircum{}\{L\}P(c|y\_t,\textbackslash{}Omega)y\_t \textbackslash{}{]}

\sphinxAtStartPar
where  \textbackslash{}( m\_c \textbackslash{}) is the mean of UBM mixture component \textbackslash{}( c \textbackslash{}) .
The i\sphinxhyphen{}vector for a given utterance can be obtained using the following
equation:

\sphinxAtStartPar
\textbackslash{}{[} w=(I + T\textasciicircum{}t \textbackslash{}Sigma\textasciicircum{}\{\sphinxhyphen{}1\} N(u)T)\textasciicircum{}\{\sphinxhyphen{}1\}. \textbackslash{},T\textasciicircum{}t\textbackslash{}Sigma\textasciicircum{}\{\sphinxhyphen{}1\}\textbackslash{}hat\{F\}(u)
\textbackslash{}{]}

\sphinxAtStartPar
where  \textbackslash{}( N\_u \textbackslash{}) is a diagonal matrix of dimension  \textbackslash{}( CF \textbackslash{}times CF
\textbackslash{}) whose diagonal blocks are \textbackslash{}( N\_cI(c=1,……, C) \textbackslash{}) . The
supervector obtained by concatenating all first\sphinxhyphen{}order Baum–Welch
statistics \textbackslash{}( F\_c \textbackslash{}) for a given utterance  \textbackslash{}( u \textbackslash{}) is represented
by  \textbackslash{}( \textbackslash{}hat\{F\}(u) \textbackslash{}) which has  \textbackslash{}( CF \textbackslash{}times 1 \textbackslash{}) dimension. The
diagonal covariance matrix,  \textbackslash{}( \textbackslash{}Sigma \textbackslash{}) , with dimension \textbackslash{}( CF
\textbackslash{}times CF \textbackslash{}) estimated during factor analysis training models the
residual variability not captured by the total variability matrix \textbackslash{}( T
\textbackslash{}) .

\sphinxAtStartPar
Process of
i\sphinxhyphen{}Vector extraction

\sphinxAtStartPar
One of the most widely used feature normalization techniques of
i\sphinxhyphen{}vectors is length normalization. Length normalization ensures that the
distribution of i\sphinxhyphen{}vectors matches the Gaussian normal distribution and
makes the distributions of i\sphinxhyphen{}vector more similar. Performing whitening
before length normalization improves the performance of speaker
verification systems.  i\sphinxhyphen{}vector normalization improves the gaussianity
of the i\sphinxhyphen{}vectors and reduces the gap between the underlying assumptions
of the data and real distributions. It also reduces the dataset shift
between development and test i\sphinxhyphen{}vectors.

\sphinxAtStartPar
\textbackslash{}{[} w\textbackslash{}leftarrow
\textbackslash{}frac\{\textbackslash{}Sigma\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}frac\{1\}\{2\}\}(w\sphinxhyphen{}\textbackslash{}mu)\}\{||\textbackslash{}Sigma\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}frac\{1\}\{2\}\}(w\sphinxhyphen{}\textbackslash{}mu)||\}
\textbackslash{}{]}

\sphinxAtStartPar
where  \textbackslash{}( \textbackslash{}mu \textbackslash{}) and  \textbackslash{}( \textbackslash{}Sigma \textbackslash{}) are the mean and the covariance
matrix of a training corpus, respectively. The data is standardized
according to  covariance matrix  \textbackslash{}( \textbackslash{}Sigma \textbackslash{}) and length\sphinxhyphen{}normalized
(i.e., the i\sphinxhyphen{}vectors are confined to the hypersphere of unit radius.

\sphinxAtStartPar
The two most widely and common intersession compensation techniques of
i\sphinxhyphen{}vectors are Within\sphinxhyphen{}Class Covariance Normalization (WCCN) and Linear
Discriminant Analysis (LDA). WCCN uses the within\sphinxhyphen{}class covariance
matrix to normalize the cosine kernel functions in order to compensate
for intersession variability. LDA attempts to define a reduced special
axes that minimize the within\sphinxhyphen{}speaker variability caused by channel
effects, and maximize between\sphinxhyphen{}speaker variability.

\sphinxAtStartPar
\sphinxstylestrong{Cosine Distance}

\sphinxAtStartPar
Once the i\sphinxhyphen{}vectors are extracted from the outputs of speech clusters,
cosine distance scoring tests the hypothesis if two i\sphinxhyphen{}vectors belong to
the same speaker or different speakers. Given two i\sphinxhyphen{}vectors, the cosine
distance among them is calculated as follows:

\sphinxAtStartPar
\textbackslash{}{[} cos(w\_i,w\_j)=\textbackslash{}frac\{w\_i.w\_j\}\{||w\_i||.||w\_j||\}\textbackslash{}gtreqless
\textbackslash{}theta \textbackslash{}{]}

\sphinxAtStartPar
where  \textbackslash{}( \textbackslash{}theta \textbackslash{}) is the threshold value, and  \textbackslash{}( cos(w\_i,w\_j) \textbackslash{})
is the cosine distance score between clusters  \textbackslash{}( i \textbackslash{}) and \textbackslash{}( j \textbackslash{}) .
The corresponding i\sphinxhyphen{}vectors extracted for clusters  \textbackslash{}( i \textbackslash{}) and  \textbackslash{}( j
\textbackslash{}) are represented by  \textbackslash{}( w\_i \textbackslash{}) and \textbackslash{}( w\_j \textbackslash{}) , respectively.

\sphinxAtStartPar
The cosine distance scoring considers only the angle between two
i\sphinxhyphen{}vectors, not their magnitude. Since the non\sphinxhyphen{}speaker information such
as session and channel variabilities affect the i\sphinxhyphen{}vector magnitude,
removing the magnitudes can increase the robustness of i\sphinxhyphen{}vector systems.

\sphinxAtStartPar
\sphinxstylestrong{Probabilistic Linear Discriminant Analysis}

\sphinxAtStartPar
The i\sphinxhyphen{}vector representation followed by probabilistic linear
discriminant analysis (PLDA) modeling technique is the state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art
in speaker verification systems. PLDA has been successfully applied in
speaker recognition experiments. It is also applied to handle speaker
and session variability in speaker verification task. It has also been
successfully applied in speaker clustering since it can separate speaker
and noise specific parts of an audio signal which is essential for
speaker diarization.

\sphinxAtStartPar


\sphinxAtStartPar
Example of PLDA model

\sphinxAtStartPar
In PLDA, assuming that the training data consists of \textbackslash{}textit\{J\}
i\sphinxhyphen{}vectors where each of these i\sphinxhyphen{}vectors belong to speaker \textbackslash{}textit\{I\},
the \textbackslash{}textit\{j’th\} i\sphinxhyphen{}vector of the \textbackslash{}textit\{I’th\} speaker is denoted by:

\sphinxAtStartPar
\textbackslash{}{[} w\_\{ij\}=\textbackslash{}mu + Fh\_i + Gy\_\{ij\} + \textbackslash{}Sigma\_\{ij\} \textbackslash{}{]}

\sphinxAtStartPar
where  \textbackslash{}( \textbackslash{}mu \textbackslash{})  is the overall speaker and segment independent mean
of the i\sphinxhyphen{}vectors in the training dataset, columns of the matrix F define
the between\sphinxhyphen{}speaker variability and columns of the matrix G define the
basis for the within\sphinxhyphen{}speaker variability subspace.  \textbackslash{}( \textbackslash{}Sigma\_\{ij\}
\textbackslash{}) represents any unexplained data variation. The components of the
vector  \textbackslash{}( h\_i \textbackslash{}) are the eigenvoice factor loadings and components of
the vector  \textbackslash{}( y\_\{ij\} \textbackslash{}) are the eigen\sphinxhyphen{}channel factor loadings. The
term  \textbackslash{}( Fh\_i \textbackslash{}) depends only on the identity of the speaker, not on
the particular segment.

\sphinxAtStartPar
Although the PLDA model assumes Gaussian behavior, there is empirical
evidence that channel and speaker effects result in i\sphinxhyphen{}vectors that are
non\sphinxhyphen{}Gaussian.  It is reported in that the use of Student’s
t\sphinxhyphen{}distribution, on the assumed Gaussian PLDA model, improves the
performance. Since this normalization technique is complicated, a
non\sphinxhyphen{}linear transformation of i\sphinxhyphen{}vectors called radial Gaussianization has
been proposed. It whitens the i\sphinxhyphen{}vectors and performs length
normalization. This restores the Gaussian assumptions of the PLDA model.

\sphinxAtStartPar
A variant of PLDA model called Gaussian PLDA (GPLDA) is shown to provide
better results.  Because of its low computational requirements, and its
performance, it is the most widely used PLDA modeling. In GPLDA model,
the within\sphinxhyphen{}speaker variability is modeled by a full covariance residual
term  which allows us to omit the channel subspace. The generative PLDA
model for the i\sphinxhyphen{}vector is  represented by

\sphinxAtStartPar
\textbackslash{}{[} w\_\{ij\}=\textbackslash{}mu + Fh\_i + \textbackslash{}Sigma\_\{ij\} \textbackslash{}{]}

\sphinxAtStartPar
The residual term  \textbackslash{}( \textbackslash{}Sigma \textbackslash{}) representing the within\sphinxhyphen{}speaker
variability is assumed to have a normal distribution with full
covariance matrix \textbackslash{}( \textbackslash{}Sigma \textbackslash{}) .

\sphinxAtStartPar
Given two i\sphinxhyphen{}vectors  \textbackslash{}( w\_1 \textbackslash{}) and \textbackslash{}( w\_1 \textbackslash{}) , the PLDA computes the
likelihood ratio of the two i\sphinxhyphen{}vectors as follows:

\sphinxAtStartPar
\textbackslash{}{[} Score(w\_1,w\_2)= \textbackslash{}frac\{p(w\_1,w\_2|H\_1)\}\{p(w\_1|H\_2) p(w\_2|H\_2)\}
\textbackslash{}{]}

\sphinxAtStartPar
where the hypothesis  \textbackslash{}( H\_1 \textbackslash{}) indicates that both i\sphinxhyphen{}vectors belong
to the same speaker and  \textbackslash{}( H\_0 \textbackslash{}) indicates they belong to two
different speakers.


\subsection{3.3.  Deep Learning (DL)}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:deep-learning-dl}}
\sphinxAtStartPar
The recent advances in computing hardware, new DL architectures and
training methods, and access to large amount of training data has
inspired the research community to make use of DL technology again as in
speaker recognition systems.  DL techniques can be used in the frontend
or/and backend of a speaker recognition system. The whole end\sphinxhyphen{}to\sphinxhyphen{}end
recognition process can even be performed by a DL architecture.

\sphinxAtStartPar
**Deep Learning Frontends: **The traditional i\sphinxhyphen{}vector approach consists
of mainly three stages: Baum\sphinxhyphen{}Welch statistics collection, i\sphinxhyphen{}vector
extraction, and PLDA backend. Recently, it is shown that if the
Baum\sphinxhyphen{}Welch statistics are computed with respect to a DNN rather than a
GMM or if bottleneck features are used in addition to conventional
spectral features, a substantial improvement can be achieved. Another
possible use of DL in the frontend is to represent the speaker
characteristics of a speech signal with a single low dimensional vector
using a DL architecture, rather than the traditional i\sphinxhyphen{}vector algorithm.
These vectors are often referred to as speaker embeddings. Typically,
the inputs of the neural network are a sequence of feature vectors and
the outputs are speaker classes.

\sphinxAtStartPar
**Deep Learning Backends: **One of the most effective backend techniques
for i\sphinxhyphen{}vectors is PLDA which performs the scoring along with the session
variability compensation. Usually, a large number of different speakers
with several speech samples each are necessary for PLDA to work
efficiently. Access to the speaker labeled data is costly and in some
cases almost impossible. Moreover, the amount of the performance gain,
in terms of accuracy, for short utterances is not as much as that for
long utterances. These facts motivated the research community to
look for DL based alternative backends. Several techniques have been
proposed. Most of these approaches use the speaker labels of the
background data for training, as in PLDA, and mostly with no significant
gain compared to PLDA.

\sphinxAtStartPar
**Deep Learning End\sphinxhyphen{}to\sphinxhyphen{}Ends: **It is also interesting to train an
end\sphinxhyphen{}to\sphinxhyphen{}end recognition system capable of doing multiple stages of signal
processing with a unified DL architecture. The neural network will be
responsible for the whole process from the feature extraction to the
final similarity scores. However, working directly on the audio signals
in the time domain is still computationally too expensive and,
therefore, the current end\sphinxhyphen{}to\sphinxhyphen{}end DL systems take mainly the handcrafted
feature vectors, e.g., MFCCs, as inputs. Recently, there have been
several attempts to build an end\sphinxhyphen{}to\sphinxhyphen{}end speaker recognition system using
DL though most of them focus on text\sphinxhyphen{}dependent speaker recognition.


\subsection{4. Applications of Speaker Recognition}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:applications-of-speaker-recognition}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Transaction authentication – Toll fraud prevention, telephone credit
card purchases, telephone brokerage (e.g., stock trading)

\item {} 
\sphinxAtStartPar
Access control – Physical facilities, computers and data networks

\item {} 
\sphinxAtStartPar
Monitoring – Remote time and attendance logging, home parole
verification, prison telephone usage

\item {} 
\sphinxAtStartPar
Information retrieval – Customer information for call centers, audio
indexing (speech skimming device), speaker diarization

\item {} 
\sphinxAtStartPar
Forensics – Voice sample matching

\end{itemize}


\subsection{5. Performance Evaluations}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:performance-evaluations}}
\sphinxAtStartPar
The performance of the speaker verification is measured in terms of
errors. The types of error and evaluation metrics commonly used in
speaker verification systems are the following.


\subsection{5.1 Types of errors}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:types-of-errors}}
\sphinxAtStartPar
False acceptance: A false acceptance occurs when the speech segments
from an imposter speaker are falsely accepted as a target speaker by the
system.

\sphinxAtStartPar
\textbackslash{}{[} False\textbackslash{}; Acceptance = \textbackslash{}frac\{Total \textbackslash{}; number \textbackslash{};of\textbackslash{}; false\textbackslash{};
acceptance\textbackslash{}; errors\} \{Total\textbackslash{}; number\textbackslash{}; of\textbackslash{}; imposter\textbackslash{}; speaker\textbackslash{};
attempts\} \textbackslash{}{]}

\sphinxAtStartPar
****

\sphinxAtStartPar
\sphinxstylestrong{False rejection:} A false rejection occurs when the target speaker is
rejected by the verification systems.

\sphinxAtStartPar
\textbackslash{}{[} False\textbackslash{};rejection = \textbackslash{}frac\{Total \textbackslash{};number\textbackslash{}; of\textbackslash{}; false\textbackslash{};
rejection\textbackslash{}; errors\} \{Total\textbackslash{}; number\textbackslash{}; of\textbackslash{}; enrolled\textbackslash{}; speaker\textbackslash{};
attempts\} \textbackslash{}{]}


\subsection{5.2 Performance metrics}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:performance-metrics}}
\sphinxAtStartPar
The performance metrics of speaker verification systems can be measured
using the equal error rate (EER) and minimum decision cost function
(mDCF). These measures represent different performance characteristics
of system though the accuracy of the measurements is based on the number
of trials evaluated in order to robustly compute the relevant
statistics. Speaker verification performance can also be represented
graphically by using the detection error trade\sphinxhyphen{}off (DET) plot. The EER
is obtained when the false acceptance rate and false rejection rate have
the same value. The performance of the system improves if the value of
ERR is lower because the sum of total error of the false acceptance and
false rejection at the point of ERR decreases. The decision cost
function (DCF) is defined by assigning a cost of each error and taking
into account the prior probability of target and impostor trails. The
decision cost function is defined as:

\sphinxAtStartPar
\textbackslash{}{[} DCF = C\_\{miss\}P\_\{miss\}P\_\{target\} + C\_\{fa\}P\_\{fa\}P\_\{impostor\}
\textbackslash{}{]}

\sphinxAtStartPar
where  \textbackslash{}( C\_\{miss\} \textbackslash{}) and  \textbackslash{}( C\_\{fa\} \textbackslash{}) are the cost functions of
a missed detection and false alarm, respectively. The prior
probabilities of target and impostor trails are given by  \textbackslash{}(
P\_\{target\} \textbackslash{}) and   \textbackslash{}( P\_\{impostor\} \textbackslash{}) , respectively. The
percentages of the missed target and falsely accepted impostors’ trails
are represented by  \textbackslash{}( P\_\{miss\} \textbackslash{}) and \textbackslash{}( P\_\{fa\} \textbackslash{}) ,
respectively. The mDCF is used to evaluate speaker verification by
selecting the minimum value of DCF estimated by changing the threshold
value. The mDCF can be used to evaluate speaker verification by
selecting the minimum value of DCF estimated by changing the threshold
value.

\sphinxAtStartPar
\textbackslash{}{[} mDCF = min{[}C\_\{miss\}P\_\{miss\}P\_\{target\} +
C\_\{fa\}P\_\{fa\}P\_\{impostor\}{]} \textbackslash{}{]}

\sphinxAtStartPar
where  \textbackslash{}( P\_\{miss\} \textbackslash{}) and  \textbackslash{}( P\_\{fa\} \textbackslash{}) are the miss and false
alarm rates recorded from the trials, and the other parameters are
adjusted to suit the evaluation of application\sphinxhyphen{}specific requirements.




\subsection{Attachments:}
\label{\detokenize{Recognition/Speaker_Recognition_and_Verification:attachments}}








\sphinxstepscope


\section{Speaker Diarization}
\label{\detokenize{Recognition/Speaker_Diarization:speaker-diarization}}\label{\detokenize{Recognition/Speaker_Diarization::doc}}

\subsection{1. Introduction to Speaker Diarization}
\label{\detokenize{Recognition/Speaker_Diarization:introduction-to-speaker-diarization}}
\sphinxAtStartPar
Speaker diarization is the process of segmenting and clustering a speech
recording into homogeneous regions and answers the question “who spoke
when” without any prior knowledge about the speakers. A typical
diarization system performs three basic tasks. Firstly, it discriminates
speech segments from the non\sphinxhyphen{}speech ones. Secondly, it detects speaker
change points to segment the audio data. Finally, it groups these
segmented regions into speaker homogeneous clusters.

\sphinxAtStartPar
An overview of a speaker diarization system.

\sphinxAtStartPar
Although there are many different approaches to perform speaker
diarization, most of them follow the following scheme:

\sphinxAtStartPar
Feature extraction: It extracts specific information from the audio
signal and allows subsequent speaker modeling and classification. The
extracted features should ideally maximize inter\sphinxhyphen{}speaker variability and
minimize intra\sphinxhyphen{}speaker variability, and represent the relevant
information.

\sphinxAtStartPar
Speaker segmentation: It partitions the audio data into acoustically
homogeneous segments according to speaker identities. It detects all
boundary locations within each speech region that corresponds to speaker
change points which are subsequently used for speaker clustering.

\sphinxAtStartPar
Speaker clustering: Speaker clustering groups speech segments that
belong to a particular speaker. It has two major categories based on its
processing requirements. Its two main categories are online and offline
speaker clustering. In the former, speech segments are merged or split
in consecutive iterations until the optimum number of speakers is
acquired. Since the entire speech file is available before decision
making in the later, it provides better results more than the online
speaker clustering. The most widely used and popular technique for
speaker clustering is Agglomerative Hierarchical Clustering (AHC). AHC
builds a hierarchy of clusters, that shows the relations between speech
segments, and merges speech segments based on similarity. AHC approaches
can be classified into bottom\sphinxhyphen{}up and top\sphinxhyphen{}down clustering.

\sphinxAtStartPar
Two items need to be defined in both bottom\sphinxhyphen{}up and top\sphinxhyphen{}down clustering:

\sphinxAtStartPar
1. A distance between speech segments to determine acoustic similarity.
The distance metric is used to decide whether or not two clusters must
be merged (bottom\sphinxhyphen{}up clustering) or split (top\sphinxhyphen{}down clustering).

\sphinxAtStartPar
2. A stopping criterion to determine when the optimal number of
clusters (speakers) is reached.

\sphinxAtStartPar
Bottom\sphinxhyphen{}up (Agglomerative): It starts from a large number of speech
segments and merges the closest speech segments iteratively until a
stopping criterion is met. This technique is the most widely used in
speaker diarization since it is directly applied on the output of speech
segments from speaker segmentation. A matrix of distances between every
possible pair of clusters is computed and the pair with highest BIC
value is merged. Then, the merged clusters are removed from the distance
matrix. Finally, the distance matrix table is updated using the
distances between the new merged cluster and all remaining clusters.
This process is done iteratively until the stopping criterion is met or
all pairs have a BIC value less than zero .

\sphinxAtStartPar
Top\sphinxhyphen{}down: Top\sphinxhyphen{}down Hierarchical Clustering methods start from a small
number of clusters, usually a single cluster that contains several
speech segments, and the initial clusters are split iteratively until a
stopping criterion is met. It is not as widely used as the bottom\sphinxhyphen{}up
clustering.

\sphinxAtStartPar
Bottom\sphinxhyphen{}Up and Top\sphinxhyphen{}down approaches to clustering


\subsection{2. Approaches to Speaker Diarization}
\label{\detokenize{Recognition/Speaker_Diarization:approaches-to-speaker-diarization}}
\sphinxAtStartPar
This section describes some of the state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art speaker diarization
systems.

\sphinxAtStartPar
HMM/GMM based speaker diarization system: Each speaker is represented by
a state of an HMM and the state emission probabilities are modeled using
GMMs. The initial clustering is performed initially by partitioning the
audio signal equally which generates a set of segments \{ \textbackslash{}( s\_i \textbackslash{}) \}.
Let  \textbackslash{}( c\_i \textbackslash{}) represent  \textbackslash{}( i\textasciicircum{}\{th\} \textbackslash{}) speaker cluster,  \textbackslash{}( b\_i \textbackslash{})
represent the emission probability of cluster  \textbackslash{}( c\_i \textbackslash{}) and  \textbackslash{}( f\_t
\textbackslash{}) denote a given feature vector at time \textbackslash{}( t \textbackslash{}) . Then, the
log\sphinxhyphen{}likelihood  \textbackslash{}( logb\_i(s\_t) \textbackslash{}) of the feature ft for cluster  \textbackslash{}(
c\_i \textbackslash{}) is calculated as follows:

\sphinxAtStartPar
\textbackslash{}{[} logb\_i(s\_t)=log \textbackslash{}sum\_\{(r)\} \{w\}\textasciicircum{}r\_i N
(f\_i,\{\textbackslash{}mu\}\textasciicircum{}r\_i,\textbackslash{}sum\_\{(i)\}\textasciicircum{}r) \textbackslash{}{]}

\sphinxAtStartPar
where  \textbackslash{}( N() \textbackslash{}) is a Gaussian pdf and  \textbackslash{}( \{w\}\textasciicircum{}r\_i,
\{\textbackslash{}mu\}\textasciicircum{}r\_i,\textbackslash{}sum\_\{(i)\}\textasciicircum{}r) \textbackslash{}) are the weights, means and covariance
matrices of the  \textbackslash{}( r\textasciicircum{}\{th\} \textbackslash{}) Gaussian mixture component of cluster 
\textbackslash{}( c\_i \textbackslash{}) , respectively.

\sphinxAtStartPar
The agglomerative hierarchical clustering starts by overestimating the
number of clusters. At each iteration, the clusters that are most
similar are merged based on the BIC distance. The distance measure is
based on modified delta Bayesian information criterion {[}Ajmera and
Wooters, 2003{]}. The modified BIC distance does not take into account
the penalty term that corresponds to the number of free parameters of a
multivariate Gaussian distribution and is expressed as:

\sphinxAtStartPar
\textbackslash{}{[} \textbackslash{}Delta BIC (c\_i,c\_j)= log \textbackslash{}sum\_\{f\_t \textbackslash{}in ( \{ci \textbackslash{}; \textbackslash{}cup \textbackslash{};
c\_j\})\} log b\_\{ij\}(f\_t) \sphinxhyphen{} log \textbackslash{}sum\_\{f\_t \textbackslash{}in ci\} log b\_\{i\}(f\_t) \sphinxhyphen{} log
\textbackslash{}sum\_\{f\_t \textbackslash{}in cj\} log b\_\{j\}(f\_t) \textbackslash{}{]}

\sphinxAtStartPar
where  \textbackslash{}( b\_\{ij\} \textbackslash{}) is the probability distribution of the combined
clusters  \textbackslash{}( c\_i \textbackslash{}) and  \textbackslash{}( c\_j. \textbackslash{})

\sphinxAtStartPar
The clusters that produce the highest BIC score are merged at each
iteration. A minimum duration of speech segments is normally constrained
for each class to prevent decoding short segments. The number of
clusters is reduced at each iteration. When the maximum  \textbackslash{}( \textbackslash{}Delta BIC
\textbackslash{})

\sphinxAtStartPar
distance among these clusters is less than threshold value 0, the
speaker diarization system stops and outputs the hypothesis.

\sphinxAtStartPar
Factor analysis techniques: Factor analysis techniques which are the
state of the art in speaker recognition have recently been successfully
used in speaker diarization. The speech clusters are first represented
by i\sphinxhyphen{}vectors and the successive clustering stages are performed based on
i\sphinxhyphen{}vector modeling. The use of factor analysis technique to model speech
segments reduces the dimension of the feature vector by retaining most
of the relevant information. Once the speech clusters are represented by
i\sphinxhyphen{}vectors, cosine\sphinxhyphen{}distance and PLDA scoring techniques can be applied to
decide if two clusters belong to the same or different speaker(s).

\sphinxAtStartPar
Deep learning approaches: Speaker diarization is crucial for many speech
technologies in the presence of multiple speakers, but most of the
current methods that employ i\sphinxhyphen{}vector clustering for short segments of
speech are potentially too cumbersome and costly for the front\sphinxhyphen{}end role.
Thus, it has been proposed by Daniel Povey an alternative approach for
learning representations via deep neural networks to remove the i\sphinxhyphen{}vector
extraction process from the pipeline entirely. The proposed architecture
simultaneously learns a fixed\sphinxhyphen{}dimensional embedding for acoustic
segments of variable length and a scoring function for measuring the
likelihood that the segments originated from the same or different
speakers.  The proposed neural based system matches or exceeds the
performance of state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art baselines.


\subsection{3. Evaluation Metrics}
\label{\detokenize{Recognition/Speaker_Diarization:evaluation-metrics}}
\sphinxAtStartPar
Diarization Error Rate (DER) is the metric used to measure the
performance of speaker diarization systems. It is measured as the
fraction of time that is not attributed correctly to a speaker or
non\sphinxhyphen{}speech.

\sphinxAtStartPar
The DER is composed of the following three errors:

\sphinxAtStartPar
Speaker Error: It is the percentage of scored time that a speaker ID is
assigned to the wrong speaker. Speaker error is mainly a diarization
system error (i.e., it is not related to speech/non\sphinxhyphen{}speech detection.)
It also does not take into account the overlap speeches not detected.

\sphinxAtStartPar
False Alarm: It is the percentage of scored time that a hypothesized
speaker is labelled as a non\sphinxhyphen{}speech in the reference. The false alarm
error occurs mainly due to the the speech/non\sphinxhyphen{}speech detection error
(i.e., the speech/non\sphinxhyphen{}speech detection considers a non\sphinxhyphen{}speech segment as
a speech segment). Hence, false alarm error is not related to
segmentation and clustering errors.

\sphinxAtStartPar
Missed Speech: It is the percentage of scored time that a hypothesized
non\sphinxhyphen{}speech segment corresponds to a reference speaker segment. The
missed speech occurs mainly due to the the speech/non\sphinxhyphen{}speech detection
error (i.e., the speech segment is considered as a non\sphinxhyphen{}speech segment).
Hence, missed speech is not related to segmentation and clustering
errors.

\sphinxAtStartPar
\textbackslash{}{[} DER = Speaker \textbackslash{}; Error + False \textbackslash{}; Alarm + Miss \textbackslash{}; Speech \textbackslash{}{]}

\sphinxstepscope


\section{Paralinguistic speech processing}
\label{\detokenize{Recognition/Paralinguistic_speech_processing:paralinguistic-speech-processing}}\label{\detokenize{Recognition/Paralinguistic_speech_processing::doc}}
\sphinxAtStartPar
Paralinguistic speech processing (PSP) refers to analysis of speech
signals with the aim of extracting information beyond the linguistic
content of speech (hence paralinguistic = alongside linguistic
content; see also Schuller and Batliner {[}\hyperlink{cite.References:id8}{2013}{]}). In other words, PSP does
not focus on what is the literal transmitted message but on what
additional information is conveyed by the signal. Speaker
{\hyperref[\detokenize{Recognition/Speaker_Diarization::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{diarization}}}},
{\hyperref[\detokenize{Recognition/Speaker_Recognition_and_Verification::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{recognition}}}}, and
{\hyperref[\detokenize{Recognition/Speaker_Recognition_and_Verification::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{verification}}}}, even though
focusing on non\sphinxhyphen{}linguistic aspects, are also traditionally considered as
separate problems that do not fall within the scope of PSP. A classical
example of PSP is speech emotion recognition, where the aim is to infer
the emotional state of a speaker based on a sample of his or her speech.
In a similar manner, information related to the health or age of a
speaker could be inferred from the speech signal.


\subsection{Coupling between speaker states and the speech signal}
\label{\detokenize{Recognition/Paralinguistic_speech_processing:coupling-between-speaker-states-and-the-speech-signal}}
\sphinxAtStartPar
The basic starting point for PSP systems is that the speech signal also
reflects the underlying cognitive and neurophysiological state of a
speaker. This is since speaking involves highly complicated cognitive
processing in terms of real\sphinxhyphen{}time communicative, linguistic, and
articulatory planning. In addition, execution of these plans requires
highly\sphinxhyphen{}precise motor control of articulators paired with real\sphinxhyphen{}time
monitoring of the resulting acoustic signal, and both of these tasks
take place in parallel with further speech planning. The mental state of
the speaker may also affect the manner that the speaker wishes to
express himself or herself.  Finally, the overall physiological
characteristics of the speech production apparatus also shape the
resulting signal, and details of these characteristics may also change
due to illnesses or habits. This means that many temporary or permanent
perturbations in the cognitive and physical machinery of a talker may
show up in the resulting speech.

\sphinxAtStartPar
To give some examples, substantial cognitive load (e.g., a concurrent
attention\sphinxhyphen{}requiring task) or neurodegenerative diseases affecting memory
(e.g., Alzheimer’s disease) may impact speech planning due to
compromised cognitive resources, resulting in speech output that differs
from the typical speech from the same person in non\sphinxhyphen{}stressful or healthy
conditons. In the same way, neurodegenerative diseases affecting the
brain’s motor system (e.g., Parkinson’s disease) may impact fluidity and
clarity of speech production, and the symptoms will become more
pronounced as the disease progresses. As for articulatory changes,
stress and emotional distress can cause increased tension in the muscles
of the larynx, which can result in tightening of the vocal folds and
therefore also causes increases in the fundamental frequency of speech.
Changes in the physical characteristics of the vocal tract may result
from, e.g., having a cold. In this case, mucus on tract surfaces may
affect resonance and damping characteristics of the vocal tract. In
addition, the mucus may prevent full closing of the velum, causing
nasalized speech often associated with a severe cold. The speaker may
also speak differently due to cognitive fatigue and throat soreness due
to the cold. Aging will also change the characteristics of the speech
production apparatus, not only in childhood but also in later years of
life. These changes are driven by physiological changes in the glottis
and in the vocal tract, where growth of the vocal tract length in early
childhood has an especially pronounced effect. The voice change in
puberty is also an example of quick growth of the larynx and vocal
folds, but small changes in the vocal folds and their control may also
take with later aging.

\sphinxAtStartPar
In addition to information that is not directly related to intended
communicative goals, speech also contains paralinguistic characteristics
related to communication. This is because speech has co\sphinxhyphen{}evolved with the
development of other social skills in humans over thousands of years.
Speech (and gestures) can thefore play different types of social
coordinative roles beyond the literal linguistic message transmitted.
For instance, prosody, and speaking style in general, can reflect
different social roles such as submissiveness, arrogance, or authority
in different interactions. Attitudes and emotions showing up in speech
can also be considered as communicative signals facilitating social
interaction and cohesion, not just being speaker\sphinxhyphen{}internal states that
inadvertently “leak out” for others to perceive. Demonstration of anger
or happiness through voice can transmit important information regarding
social dynamics even when visual contact between the interlocutors is
not possible. As a concrete example, consider having a telephone
conversation with someone close to you without access to anything else
than the literal message (e.g., substituting the original speech with
monotone but perfectly intelligible speech synthesis) while trying to
communicate highly sensitive and important personal information.

\sphinxAtStartPar
The basic aim of PSP is to use computational means to understand and
characterize the ways that different paralinguistic factors shape the
speech signal, and to build automatic systems for analyzing and
detecting the paralinguistic factors from real speech captured in
various settings.

\sphinxAtStartPar
Also note that the distinction between PSP systems and other established
areas of speech processing is not always clear\sphinxhyphen{}cut. For instance,
automated methods for speech \DUrole{xref,myst}{intelligibility
assessment} are also focusing on
extralinguistic factors, and the task of speaker recognition was already
mentioned at the beginning of this section. In addition, more flexible
control of speaking style is an ongoing topic of research in \DUrole{xref,myst}{speech
synthesis}, where the research focus is gradually
changing from the production of high\sphinxhyphen{}quality to speech to creation of
systems capable of richer vocalic expression. However, there is no need
for a strict distinction of PSP from other types of processing tasks,
but PSP can be viewed as an umbrella term for the increasingly many
analysis tasks focused on the various non\sphinxhyphen{}literal aspects of spoken
language, and where similar data\sphinxhyphen{}driven methodology is usually
applicable across a broad range of PSP phenomena (see also Schuller and Batliner {[}\hyperlink{cite.References:id8}{2013}{]}, for a discussion).


\subsection{Speaker traits and states}
\label{\detokenize{Recognition/Paralinguistic_speech_processing:speaker-traits-and-states}}
\sphinxAtStartPar
Schuller and Batliner {[}\hyperlink{cite.References:id8}{2013}{]}
use a distinction into two types of speaker
characteristics: traits and states. These are related to the temporal
properties of the analyzed phenomena. Speaker traits are long\sphinxhyphen{}term and
slowly\sphinxhyphen{}changing characteristics of the speaker, such as personality
traits (e.g., Big Five classification), gender, age, or dialect. On the
other hand, speaker states are short\sphinxhyphen{} to medium\sphinxhyphen{}term phenomena, such as
speaker’s emotional state, attitude in a conversation, (temporary)
health conditions, fatigue, or stress level. When collecting data for
PSP research and system development, it is important to consider the
time\sphinxhyphen{}scale of the phenomenon to be analyzed and how this relates to
practical needs of the analysis task (e.g., how much speech can be
collected and analyzed before classification decision; does the system
have to be real\sphinxhyphen{}time). For instance, quickly changing characteristics
such as emotional state should be analyzed from relatively short speech
recordings where the factor of interest can be assumed to be stable. For
example, recognition of speaker’s emotional state during a single
utterance is a widely adopted approach in speech emotion recognition. In
contrast, analysis of speaker health (e.g., COVID\sphinxhyphen{}19 symptoms) from just
one utterance is likely to be inaccurate, but speaker\sphinxhyphen{}dependent data
collection and analysis across longer stretches of speech will likely
produce more reliable analysis outcomes. Moreover, longitudinal
monitoring of a subject across longer periods of time is likely to be
more accurate in detecting changes in the speaker’s voice, as the system
can be adapted to the acoustic and linguistic characteristics of that
specific speaker. For instance, subject\sphinxhyphen{}specific monitoring of the
progression of a neurodegenerative disease based on speech is likely to
be more accurate than automatic classification of disease severity from
a bag of utterances from a random collection of speakers.


\subsection{Typical applications of PSP}
\label{\detokenize{Recognition/Paralinguistic_speech_processing:typical-applications-of-psp}}
\sphinxAtStartPar
Some possible applications of paralinguistic tasks include, but are not
limited to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Emotion classification

\item {} 
\sphinxAtStartPar
Personality classification (e.g., Big Five traits)

\item {} 
\sphinxAtStartPar
Sleepiness or intoxication detection

\item {} 
\sphinxAtStartPar
Analysis of cognitive or physical load

\item {} 
\sphinxAtStartPar
Health\sphinxhyphen{}related analyses (cold, snoring, neurodegenerative diseases
etc.)

\item {} 
\sphinxAtStartPar
Speech addressee analysis (e.g., adult\sphinxhyphen{} vs. infant\sphinxhyphen{}directed speech)

\item {} 
\sphinxAtStartPar
Age and gender recognition

\item {} 
\sphinxAtStartPar
Sincerity analysis

\item {} 
\sphinxAtStartPar
Attitude analysis

\end{itemize}


\subsection{Basic problem formulation and standard solutions}
\label{\detokenize{Recognition/Paralinguistic_speech_processing:basic-problem-formulation-and-standard-solutions}}
\sphinxAtStartPar
The basic goal of paralinguistic analysis is to extract paralinguistic
information of interest while ignoring the signal variability introduced
by other factors, such as linguistic content, speaker identity,
background noise or transmission channel characteristics (aka. \sphinxstyleemphasis{nuisance
factors}). However, for some tasks, it may also be useful to analyse the
lexical and grammatical content of speech in order to infer information
regarding the phenomena of interest.

\sphinxAtStartPar
Typical PSP systems follow the two classical tasks of machine learning:
classification and regression. In classification, the goal is to build a
system that can assign a speech sample (e.g., an utterance) into one of
two or more categories (e.g., intoxicated or not intoxicated). In
regression, the target is a continuous (or at least ordinal) measure
(e.g., blood alcohol concentration percentage).

\sphinxAtStartPar
Fig. 1 illustrates a standard PSP system pipeline, which follows a
typical supervised machine learning scenario. First, a number of
features are extracted from the speech signal. These features, together
with the corresponding class labels, are then used to train a classifier
model for the training dataset. During testing and actual use of the
system, the classifier is used to determine the most likely class of an
input waveform. Depending on the classifier architecture, the
classification result may or may not be associated with a confidence
measure of the classification decision. In regression tasks, the process
is otherwise the same, but the categorical class labels are replaced by
continous\sphinxhyphen{}valued measures and the classifier is replaced by a regression
model.

\sphinxAtStartPar
A central characteristic of many PSP tasks is that the analyzed
phenomenon (e.g., speaker emotion) is assumed to be fixed at a certain
time\sphinxhyphen{}scale, such as across one utterance, and therefore classification
decisions should also be made at time\sphinxhyphen{}scales longer than typical
frame\sphinxhyphen{}level signal features. In this case, it would desirable to obtain
a fixed\sphinxhyphen{}dimensional feature representation of the signal even if the
duration of the input waveform varies from case to case. This can be
achieved by a two\sphinxhyphen{}step process: \sphinxstylestrong{1)} first extracting regular
frame\sphinxhyphen{}level features (e.g., spectral features such as FFT) with high
temporal resolution (e.g., one frame every 10\sphinxhyphen{}ms), sometimes referred to
as low\sphinxhyphen{}level descriptors (LLDs), and then \sphinxstylestrong{2)} calculating statistical
parameters (“functionals”) of each of the features across all the
time\sphinxhyphen{}frames (illustrated in Fig. 2). Typical functionals of LLDs
include, e.g., \sphinxstyleemphasis{min, max, mean, variance, skewness,} and \sphinxstyleemphasis{kurtosis}, but
they can also be measures, such as centroids, percentiles, or different
types of means. In order to acquire a more complete picture of the
signal dynamics, first\sphinxhyphen{} and second\sphinxhyphen{}order time\sphinxhyphen{}derivatives of the
frame\sphinxhyphen{}level features (“deltas” and “delta\sphinxhyphen{}deltas”) are often included in
the feature set before calculating the functionals. In neural networks,
a varying\sphinxhyphen{}length signal can also be represented by a fixed\sphinxhyphen{}dimensional
embedding extracted from the input waveform, such as taking the output
of an LSTM\sphinxhyphen{}layer. Once the input signals are represented by
fixed\sphinxhyphen{}dimensional feature vectors, standard machine learning classifiers
can be applied to the data.

\sphinxAtStartPar
\sphinxincludegraphics{{180298884}.png}

\sphinxAtStartPar
\sphinxstylestrong{Figure 1:} An example of a classical PSP processing pipeline with
training (top) and usage (bottom). Speech features are first extracted
from the original speech waveform, followed by a classifier that has
been trained using supervised learning with labeled training samples.

\sphinxAtStartPar
\sphinxincludegraphics{{180298908}.png}

\sphinxAtStartPar
\sphinxstylestrong{Figure 2:} An example signal\sphinxhyphen{}level feature extraction process where
variable\sphinxhyphen{}duration utterances become represented by fixed\sphinxhyphen{}dimensional
feature vectors, consisting of statistical parameters (“functionals”)
calculated across frame\sphinxhyphen{}by\sphinxhyphen{}frame speech features (sometimes also
referred to as low\sphinxhyphen{}level descriptors, or “LLDs”).

\sphinxAtStartPar
In the both cases of classification and regression, the challenge is to
build a system that can discriminate the key features of the phenomenon
without being affected by the other sources of signal variability.
High\sphinxhyphen{}performance well\sphinxhyphen{}generalizing systems can be approached with three
basic strategies: \sphinxstyleemphasis{well\sphinxhyphen{}designed features}, \sphinxstyleemphasis{robust classifiers}, or
\sphinxstyleemphasis{end\sphinxhyphen{}to\sphinxhyphen{}end learning}, where signal representations (“features”) and
classifiers are jointly learned from the data. Selection of the approach
depends on two primary factors:  domain knowledge and availability of
representative large\sphinxhyphen{}scale training data (Fig. 3).

\sphinxAtStartPar
The basic principle is that well\sphinxhyphen{}designed or otherwise properly chosen
features require less training data for inference of the machine
learning model parameters, but this necessitates that the
acoustic/linguistic characteristics of the analyzed phenomenon are known
in order to design features that capture them. If there are only limited
data and limited domain knowledge, it is still possible to calculate a
large number of potentially relevant features and then use a classifier
such as Support Vector Machines (SVMs; Boser et al., 1992) that can
robustly handle high\sphinxhyphen{}dimensional features, including potentially
task\sphinxhyphen{}irrelevant or otherwise noisy features. For instance, the baseline
systems for annual Computational Paralinguistic Challenges (see also
below) have traditionally used a combination of more than 6000
signal\sphinxhyphen{}level features together with an SVM classifier, and often these
systems have been highly competitive with other solutions.
Alternatively, feature selection techniques may be applied in
conjunction with data labeling to find a more compact feature set for
the problem at hand (see, e.g., Pohjalainen \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.References:id9}{2015}{]}).

\sphinxAtStartPar
If there are plenty of training data available, multilayer neural
networks can be used to simultaneously learn useful signal
representations and a classifier for the given problem. This type of
representation learning can operate directly on the acoustic waveform or
using some standard spectral representation with limited additional
assumptions, such as FFT or log\sphinxhyphen{}Mel spectrum. The obvious advantage is
that feature design is no longer needed, and the features and the
classifier are seamlessly integrated and jointly optimized. In addition,
even if domain knowledge would be available, the assumptions built into
manually tailored features may lose some details of the modeled
phenomenon, whereas end\sphinxhyphen{}to\sphinxhyphen{}end neural networks can potentially use all
the information available in the input signal. Similarly to many other
applications of machine learning, neural networks can therefore be
expected to outperform the “more classical” approaches if sufficient
training data are available for the task (see also sub\sphinxhyphen{}section below for
data in PSP). However, besides the data requirement drawback, the
standard problems and principles of neural network design and training
apply, including the lack of access to the global optimum during the
parameter optimization process.

\sphinxAtStartPar
If sufficient training data and domain knowledge are both available, one
may also combine representation learning with good initial features or
some type of model priors or constraints. This may speed\sphinxhyphen{}up the learning
process or improve model convergence to a more effective solution due to
a more favorable starting point for the optimization process. One
particularly interesting new research direction is the use of
differentiable computational graphs in feature extraction. In this case,
prior task\sphinxhyphen{}related knowledge could be incorporated into digital signal
processing (DSP) steps used to extract some initial features, but the
feature extraction algorithm would be implemented together with the rest
of the network as one differentiable computational graph (e.g., using a
framework such as TensorFlow). This would allow error backpropagation
through the entire pipeline from classification decisions to the feature
extractor, thereby enabling task\sphinxhyphen{}optimized adaptation of the feature
extraction or pre\sphinxhyphen{}processing steps. As an example, Discrete Fourier
Transform is a differentiable function, allowing error gradients to pass
through it, although by default it does not have any free parameters to
optimize. However, similar signal transformations but with parametrized
basis functions could be utilized and optimized jointly with the rest of
the model.

\sphinxAtStartPar
\sphinxincludegraphics{{180298840}.png}

\sphinxAtStartPar
\sphinxstylestrong{Figure 3:} Basic strategies for PSP system development given the two
main considerations: availability of training data (x\sphinxhyphen{}axis) and domain
knowledge (y\sphinxhyphen{}axis).

\sphinxAtStartPar
Naturally, the division into the four categories described in Fig. 3 is
by no means definite. For instance, recent developments in
self\sphinxhyphen{}supervised representation learning (van den Oord et al., 2018;
Chung et al., 2019; Baevski et al., 2020) or multitask learning (e.g.,
Wu et al., 2015) are still largely unexplored in PSP. Such approaches
could enable effective utilization of large amounts of unlabeled speech
data with fewer labeled examples in the target domain of interest.


\subsection{Data collection and data sparsity}
\label{\detokenize{Recognition/Paralinguistic_speech_processing:data-collection-and-data-sparsity}}
\sphinxAtStartPar
Since modern PSP largely relies on machine learning, representative
training data will be required for the phenomenon of interest. However,
access to high\sphinxhyphen{}quality labeled data is often be limited for many PSP
tasks. First of all, the data collection itself is often challenging and
may include important ethical considerations, such as collecting data
from intoxicated speakers or subjects with rare diseases, or data where
some type of objective (physiological) measurements of emotional state
are captured simultaneously with the speech audio. Availability of
reliable ground\sphinxhyphen{}truth labels for the speech data can also be difficult.
For instance, there is no direct way to measure the underlying emotional
states of speakers, whereas induced emotional speech by professional
actors may not properly reflect the variability of real\sphinxhyphen{}world emotional
expression. In a similar manner, assessment of severity of many diseases
is based on various indirect measurements and clinical diagnostic
practices, not on some type of oracle knowledge on a universally
standardized scale. Every time humans are used for data labelling (e.g.,
assessing emotions), there is a certain degree of inter\sphinxhyphen{}annotator
inconsistency due to differing opinions and general variability in human
performance. This is the case even when domain experts are used for the
task. Naturally, the more difficult the task or more ambiguous the
phenomenon, the more there will be noise and ambiguity in the labeling.

\sphinxAtStartPar
Another limitation hindering PSP progress is that many PSP datasets
cannot be freely distributed to the research community due to data
ownership and human participant privacy protection considerations. As a
clear example, speech with metadata related to factors such as health or
IQ of the speakers is highly sensitive in nature, and not all speakers
consent to open distribution of their identifiable voice together with
such private data of themselves. The data ownership considerations
inherently limit the pooling of different speech corpora in order to
build more comprehensive databases of speech related to different
phenomena of interest, and generally slow down replicable open science.
On the other hand, it is of utmost importance to respect the privacy of
human participants in PSP (or any other) research—not only due to
ethical considerations, but also since the entire field depends on
access to data from voluntary human participants. Commercial interests
to data ownership are also often unavoidable.

\sphinxAtStartPar
In total, this means that data is often a limiting factor in system
performance, and therefore deep neural networks have still not become
the only off\sphinxhyphen{}the\sphinxhyphen{}shelf\sphinxhyphen{}solution for many PSP problems. Efforts for more
flexible (but ethical) data sharing and pooling is therefore an
important challenge for future research. More powerful technical
solutions and deeper understanding of the PSP phenomena could be
acquired by combining rich data (and metadata) from various sources,
including different languages, cultural environments, and recording
conditions.


\subsection{Computational Paralinguistic Challenge}
\label{\detokenize{Recognition/Paralinguistic_speech_processing:computational-paralinguistic-challenge}}
\sphinxAtStartPar
Research in PSP has been strongly advanced by an annual Computational
Paralinguistic Challenge (ComParE) held in the context of ISCA
Interspeech conferences (see \sphinxurl{http://www.compare.openaudio.eu/}). Every
year since 2009, ComParE has included a number of paralinguistic
analysis tasks with pre\sphinxhyphen{}defined datasets in which participants can
compete with each other. Competitive baseline systems, evaluation
protocols and results are always provided to the participants as a
starting point, enabling low\sphinxhyphen{}barrier access to the world of PSP for
researchers with various backgrounds. In addition, new tasks and
datasets can be proposed to challenge organizers, providing a useful
channel for data owners and researchers to obtain competitive solutions
to their analysis problems.


\subsection{Further reading and materials on PSP}
\label{\detokenize{Recognition/Paralinguistic_speech_processing:further-reading-and-materials-on-psp}}
\sphinxAtStartPar


\sphinxAtStartPar
Schuller, B. et al.: Computational Paralinguistic Challenge. WWW\sphinxhyphen{}site:
\sphinxurl{http://www.compare.openaudio.eu/}, last accessed 11th October 2020.

\sphinxstepscope


\chapter{Speech Synthesis}
\label{\detokenize{Speech_Synthesis:speech-synthesis}}\label{\detokenize{Speech_Synthesis::doc}}
\sphinxAtStartPar
Speech synthesis systems aim at producing intelligible speech signals
from some type of input language representation. Synthesis systems are
also often referred to as text\sphinxhyphen{}to\sphinxhyphen{}speech (TTS) systems, as written text
is a natural way to instruct what type of utterances should be produced.

\sphinxAtStartPar
A typical TTS system consists of two basic processing modules: text
analysis module and a speech synthesis module.

\sphinxAtStartPar
\sphinxstylestrong{Text analysis}

\sphinxAtStartPar
The first module of a TTS pipeline, the text analysis module, is
responsible for converting the incoming text into a linguistic
representation that encodes the information of how the input text should
be spoken.

\sphinxAtStartPar
As the first step, the module must process the text into a standardized
format by taking care of any special characters or other
inconsistencies. Then the text must be structurally analyzed to identify
syntactical components of the sentences. Any inconsistencies between the
written and spoken language (e.g., abbreviations and acronyms, proper
names, numbers) must be detected and converted into a correct format
(e.g., to map the string “100 sq ft” into “\sphinxstyleemphasis{one hundred} \sphinxstyleemphasis{square feet}”,
not “\sphinxstyleemphasis{one\sphinxhyphen{}zero\sphinxhyphen{}zero sq ft}”) .

\sphinxAtStartPar
The second step consists of inferring the correct pronunciation of the
words, also known as grapheme\sphinxhyphen{}to\sphinxhyphen{}phoneme conversion. Since the
relationship between written and spoken language is not always
straightforward, the text analysis module must convert strings of input
letters into strings of phonemes based on the pronunciation conventions
of the given language. This involves resolving many ambiguities
prevalent in languages, such as how to deal with homographs, that is,
words with the same written form but different pronunciations. Foreign
language words and loan words have to be handled as well.

\sphinxAtStartPar
As the final stage, suprasegmental characteristics of the
speech\sphinxhyphen{}to\sphinxhyphen{}be\sphinxhyphen{}produced must be introduced to the linguistic
representation. Durations of the phonemes and intermediate pauses must
be defined, even though the information does not exist in the text. In
order to make the speech natural sounding and intelligible, the process
also includes introduction of any potential rhythmic structure, stress
patterns, and intonational cues to the linguistic representation. For
this purpose, the text analysis module must interpret syntactic
properties of the input text. For instance, the system must be able to
differentiate different sentence types such as questions from statements
and to infer which word should receive focus in the given sentential
context.

\sphinxAtStartPar
\sphinxstylestrong{Synthesis algorithms}

\sphinxAtStartPar
The second key module consists of the speech synthesizer module. Input
to the synthesizer is the linguistic representation produced by the text
analysis block while the output consists of acoustic waveforms of
synthesized speech. There are several potential technical approaches to
the creation of a speech waveform from linguistic instructions.
Historically, methods such as formant synthesis or articulatory
synthesis have been utilized (where the latter is still used in speech
research). However, modern commercial speech synthesizers are based on
one of the two alternative techniques: {\hyperref[\detokenize{Synthesis/Concatenative_speech_synthesis::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{concatenative
synthesis}}}} or {\hyperref[\detokenize{Synthesis/Statistical_parametric_speech_synthesis::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{statistical parametric speech synthesis}}}}. Both methods
are described in more detail in their respective sub\sphinxhyphen{}sections.

\sphinxAtStartPar
\sphinxincludegraphics{{175517689}.png}
\sphinxstylestrong{Figure 1:} The basic structure of a speech synthesis system.**

\sphinxAtStartPar
In general, synthesis algorithms aim at speech output that maximally
resembles natural speech, is free of noise and artifacts, and has high
intelligibility. Other characteristics may include possibility to use
different speaker voices or to speaking styles to account for different
use contexts and user preferences. In practical use, computational
complexity of the system may also become a relevant design factor. This
is especially true if the system must support real\sphinxhyphen{}time speech
production and/or serve multiple users simultaneously. For instance,
speech synthesis used on a standard mobile device or in a car
entertainment system must operate with strict latency computational
complexity constraints.

\sphinxAtStartPar
Speech quality and intelligibility a speech synthesizer are typically
evaluated using \DUrole{xref,myst}{subjective listening
tests.}

\sphinxAtStartPar
\sphinxstylestrong{Further readings \& material}

\sphinxAtStartPar
Simon King \sphinxhyphen{} Using Speech Synthesis to give Everyone their own Voice,
University of Edinburgh. Youtube video
(\sphinxhref{https://www.youtube.com/watch?v=xzL-pxcpo-E}{link}). \sphinxstyleemphasis{Includes an
overview of unit selection and statistical parametric synthesis with
sound demonstrations.}

\sphinxAtStartPar
Kim Silverman \sphinxhyphen{} Speech synthesis lecture at ICSI, Berkeley. Youtube
video (\sphinxhref{https://www.youtube.com/watch?v=7mjh0PSUv0M}{link}).

\sphinxAtStartPar
Sai Krishna Rallabandi’s on\sphinxhyphen{}line list of introductory resources for
speech synthesis
(\sphinxhref{http://www.cs.cmu.edu/~srallaba/Learn\_Synthesis/intro.html}{link})

\sphinxstepscope


\section{Concatenative speech synthesis}
\label{\detokenize{Synthesis/Concatenative_speech_synthesis:concatenative-speech-synthesis}}\label{\detokenize{Synthesis/Concatenative_speech_synthesis::doc}}
\sphinxAtStartPar
\sphinxstyleemphasis{Concantenative speech synthesis} (CSS), also known as \sphinxstyleemphasis{unit selection
speech synthesis}, is one of the two primary modern speech synthesis
techniques together with {\hyperref[\detokenize{Synthesis/Statistical_parametric_speech_synthesis::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{statistical parametric speech
synthesis}}}}. As the name
suggests, CSS is based on concatenation of pre\sphinxhyphen{}recorded speech segments
in order to create intelligible high\sphinxhyphen{}quality speech. The advantage of
this approach is extremely high naturalness of the produced speech, as
long as the system is well\sphinxhyphen{}designed and suitable speech data are
available for its development. The drawback is limited flexibility as
all the used speech segments have to be pre\sphinxhyphen{}recorded, limiting the
choice of speaker voices or other modifications to the verbal
expression.

\sphinxAtStartPar
The most simple CSS system imaginable could be developed using
concatenation of pre\sphinxhyphen{}recorded word waveforms. However, as {[}\hyperlink{cite.Synthesis/Concatenative_speech_synthesis:id55}{Rabiner and Schafer, 2007}{]} note, such an approach would suffer from two primary
problems. First, concatenation of word\sphinxhyphen{}level waveforms would sound
unnatural, as coarticulatory effects between words would be absent from
the data. In addition, the system would be limited to very restrictive
scenarios only, as there can be tens or hundreds of thousands of lexical
items and millions of proper names in any language—way more than what
can be reasonably pre\sphinxhyphen{}recorded by any individual speaker. The problem is
even worse for agglutinative languages such as Finnish, where word
meanings are constructed and adjusted by extending word root forms with
various suffixes. Since words can also participate to utterances in
various positions and roles, prosodic characteristics (e.g., F0) of the
same word can differ from context to another. This means that
pre\sphinxhyphen{}recording all possible words is not a practical option.

\sphinxAtStartPar
To solve the issues of scalability and coarticulation, practical modern
CSS systems are based on sub\sphinxhyphen{}word units. In principle, there are only
few phones per language (e.g., around 40–50 for English), but their
acoustic characteristics are also highly dependent on the surrounding
context due to coarticulation. Context\sphinxhyphen{}dependent phones such as diphones
(pairs of phones) or triphones (phone triplets) are therefore utilized.
In order to build a CSS system, speech dataset has to be first carefully
annotated and segmented for the units of interest. These segments can
then be stored as acoustic parameters (e.g., speech codec parameters) to
save space and to allow easy characterization and manipulation.


\subsection{Steps of concatenative synthesis}
\label{\detokenize{Synthesis/Concatenative_speech_synthesis:steps-of-concatenative-synthesis}}
\sphinxAtStartPar
Once a database of units exists, synthesis with CSS consists of the
following basic steps: \sphinxstylestrong{1)} \sphinxstyleemphasis{conversion of input text to a target
specification}, which includes the string of phones to be synthesized
together with additional prosodic specifications such as pitch,
duration, and power, \sphinxstylestrong{2)} \sphinxstyleemphasis{unit selection} for each phone segment
according to the specification, and \sphinxstylestrong{3)} \sphinxstyleemphasis{post\sphinxhyphen{}processing} to reduce
the impact of potential concatenation artefacts.

\sphinxAtStartPar
While the text processing stage is largely similar to pre\sphinxhyphen{}processing in
{\hyperref[\detokenize{Synthesis/Statistical_parametric_speech_synthesis::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{statistical parametric speech synthesis systems}}}}, the main part of CSS
is to perform unit selection in such a manner that the output speech
matches the specification with high naturalness of the sound. As
described by {[}\hyperlink{cite.Synthesis/Concatenative_speech_synthesis:id54}{Hunt and Black, 1996}{]}, unit selection is achieved by cost
minimization using two cost functions (Fig. 1): \sphinxstyleemphasis{target cost}
\(C^{t}(u_{i},t_{i})\) and \sphinxstyleemphasis{concatenation cost}
\(C^{c}(u_{i-1},u_{i})\). Target cost
describes the mismatch between the target speech unit specification
\(t_{i}\) and a candidate unit \(u_{i}\) from the database.
Concatenation cost describes the mismatch (e.g., acoustic or perceptual)
of the join between the candidate unit \(u_{i}\) and the preceding
unit \(u_{i-1}\). In other words, an ideal solution
would find all the target units according to the specification without
introducing acoustic mismatches at the edges of concatenated units.

\sphinxAtStartPar
\sphinxincludegraphics{{CSS_cost_schematic}.png}
Figure 1: An illustration of the selection cost Ct and concenation cost Cc in diphone\sphinxhyphen{}based unit selection for synthesis of word “cat” {[}k ae t{]}. In practice, diphones with different initial phones but similar coarticulatory effects on the target phone can be considered in the selection process to overcome the issue of data sparsity. Adapted from {[}\hyperlink{cite.Synthesis/Concatenative_speech_synthesis:id54}{Hunt and Black, 1996}{]}.

\sphinxAtStartPar
Since the target specification consists of many characteristics such as
target and context phone(s) identity, pitch, duration, and power, the
target cost can be divided into multiple subcosts \(j\) as
\( C_{j}^{t}(t_{i},u_{i}) \) . For instance, the contextual
phone(s) (e.g., {[}ae{]} in the last unit {[}ae t{]} of “cat” in Fig. 1) can
be represented by a number of features describing the manner and place
of articulation, so that the specification can be compared to different
candidate units in the database {[}\hyperlink{cite.Synthesis/Concatenative_speech_synthesis:id54}{Hunt and Black, 1996}{]}. This enables the
use of different context phones with similar coarticulatory effects on
the target phone. Similarly, costs for segment power and pitch can be
measured, e.g., in terms of the differences in mean log\sphinxhyphen{}power and mean
F0. Cost for the target phone ({[}t{]} in {[}ae t{]} of Fig. 1) is usually a
binary indicator that forces the phonemic identity of the chosen unit to
match with that of the target specification. The total cost can then be
written as
\begin{equation*}
\begin{split}
C^{t}(t_i,u_i)=\sum_{j=1}^{P}w_{j}^{t}C_{j}^{t}(t_{j},u_{i}) \end{split}
\end{equation*}
\sphinxAtStartPar
(1)

\sphinxAtStartPar
where \(w^{t} = [w^{t}_{1},
w^{t}_{2}, ..., w^{t}_{P}]\) are the
relative weights of each subcost.

\sphinxAtStartPar
Concatenation cost \(C^{c}(u_{i-1},u_{i})\)
can be derived in a similar manner to Eq. (1) by decomposing the the
total cost to \(Q\) subcosts, and then calculating a weighted sum of the
subcosts:
\begin{equation*}
\begin{split}
C^{c}(u_{i-1},u_i)=\sum_{j=1}^{Q}w_{j}^{c}C_{j}^{c}(u_{i-1},u_{i})
\end{split}
\end{equation*}
\sphinxAtStartPar
(2)

\sphinxAtStartPar
Note that the subcosts and their weights \(w^{c}\) for
\(C^{c}\) do not need match those of \(C^{t}\), as the
concatenation cost specifically focuses on the acoustic compatibility of
the subsequent units. Therefore
subcosts  \(C^{c}_{j}(u_{i-1},u_{i})\)
associated with continuity of the spectrum (or cepstrum), segment power,
and pitch in the segment and/or at the concenation point should be
considered.

\sphinxAtStartPar
The total cost of the selection process is the sum of the target and
concenation costs across all \(n\) units:
\begin{equation*}
\begin{split}
                  \begin{split}
C(t_{1}^{n},u_{1}^{n})&=\sum_{i=1}^{n}C^t(t_i,u_i)+\sum_{i=2}^{n}C^c(u_{i-1},u_i)+C^c(\#,u_1)+C^c(u_n,\#)
\\&
                   =
\sum_{i=1}^{n}\sum_{j=1}^{P}w_{j}^{t}C_{j}^{t}(t_{i},u_{i})+\sum_{i=2}^{n}\sum_{j=1}^{Q}w_{j}^{c}C_{j}^{c}(u_{i-1},u_{i})+C^c(\#,u_1)+C^c(u_n,\#)
\end{split}
\end{split}
\end{equation*}
\sphinxAtStartPar
(3)

\sphinxAtStartPar
where \( t_{1}^{n} \) are the targets,  \( u_{1}^{n} \) are the
selected units, and \# denotes silence {[}\hyperlink{cite.Synthesis/Concatenative_speech_synthesis:id54}{Hunt and Black, 1996}{]}. The two
extra terms stand for transition from preceding silence to the utterance
and from utterance to the trailing silence. The aim of the selection
process is then to find units \( \bar{u}_{1}^{n} \) that minimize
the total cost in Eq. (3). The selection process can be represented as a
fully connected trellis, as shown in Fig. 3, where each edge to a node
has a basic cost of the given node to be chosen (the target cost) and an
additional cost depending on the previous unit (the concatenation cost).
Given the trellis, the optimal selection can be carried out with
\sphinxstyleemphasis{\sphinxhref{https://en.wikipedia.org/wiki/Viterbi\_algorithm}{Viterbi search}}—a
dynamic programming algorithm that calculates the least cost path
through the trellis. To make the search computationally feasible for
large databases, less likely candidates for each target can be pruned
from the trellis. In addition, \sphinxhref{https://en.wikipedia.org/wiki/Beam\_search}{\sphinxstyleemphasis{beam
search}} with only a fixed
number of most likely nodes for each step can be applied for further
speedup.

\sphinxAtStartPar
\sphinxincludegraphics{{180303620}.png}

\sphinxAtStartPar
\sphinxstylestrong{Figure 3:} An example of unit selection search trellis for word
“\sphinxstyleemphasis{cat}” {[}\sphinxstyleemphasis{k ae t}{]}. Each edge is associated with basic target cost of
the selected unit and concatenation cost dependent on the previous unit.
Adapted from {[}\hyperlink{cite.Synthesis/Concatenative_speech_synthesis:id55}{Rabiner and Schafer, 2007}{]}

\sphinxAtStartPar
After the speech units have been concatenated to form the intended
utterance, postprocessing techniques can be used to smooth the potential
discontinuities in F0, energy and spectrum at the unit boundaries. Note
that aggressive signal processing based modification of the segments
also often tends to decrease the naturalness of the sound.
Straightforward modification of the segments’ acoustic parameters (e.g.
with vocoding) is not therefore a recommended strategy to overcome the
issues of poor unit selection or low quality source data.


\subsection{CSS training}
\label{\detokenize{Synthesis/Concatenative_speech_synthesis:css-training}}
\sphinxAtStartPar
The above formulation enables mathematically principled and optimal unit
selection process from a given speech database. However, the synthesis
output is highly dependent on the choice of features and functions used
for the subcosts, and also on the weights chosen for each feature. While
the cost functions and their underlying features can be largely designed
based on knowledge in signal processing and speech processing, the
weights need to be either adjusted through trial and error, or they can
be automatically optimized using some kind of quality criterion.

\sphinxAtStartPar
As examples of automatic weight estimation, {[}\hyperlink{cite.Synthesis/Concatenative_speech_synthesis:id54}{Hunt and Black, 1996}{]} propose
two alternative ways to automatically acquire cost function weights:

\sphinxAtStartPar
1) Using a \sphinxhref{https://en.wikipedia.org/wiki/Hyperparameter\_optimization\#Grid\_search}{grid
search}
across different weight values by synthesizing utterances using the
target specifications of held\sphinxhyphen{}out utterances from the training database,
and then comparing the synthesized waveform to the real waveform of the
held\sphinxhyphen{}out utterance using an objective metric. The weights that lead to
the best overall performance are then chosen.

\sphinxAtStartPar
2) Using regression models to predict best values for the weight. {[}\hyperlink{cite.Synthesis/Concatenative_speech_synthesis:id54}{Hunt and Black, 1996}{]} report that cepstral distance and power difference
across the concanation point can be used as predictors for reported
perceptual quality of the concatenation in a linear regression model,
and therefore the linear regression weights can be directly used as
perceptually motivated the cost weights.  For the target weights, they
propose and approach where each unit in the database is considered as
the target specification at a time, and \(K\) best matching other units
are then searched for the target using an objective distance measure.
Then the sub\sphinxhyphen{}costs between the target and the \(K\) matches are calculated
and recorded. This process is repeated for all exemplars of the same
phonetic unit in the database, recording the \(K\times Q\) subcosts and the
related \(K\) distances for each exemplar. Linear regression is then
applied to predict the recorded objective perceptual distances using the
associated sub\sphinxhyphen{}costs, linear regression coefficients again revealing the
optimal weights for each of the subcosts. A specific advantage of the
regression approach for subcost weight estimation is that it allows
estimation of phoneme\sphinxhyphen{}specific weights for each subcost, as the
perceptually critical cues may differ from a phonetic context to
another.


\subsection{Further reading}
\label{\detokenize{Synthesis/Concatenative_speech_synthesis:further-reading}}


\sphinxstepscope


\section{Statistical parametric speech synthesis}
\label{\detokenize{Synthesis/Statistical_parametric_speech_synthesis:statistical-parametric-speech-synthesis}}\label{\detokenize{Synthesis/Statistical_parametric_speech_synthesis::doc}}
\sphinxAtStartPar
While concatenative synthesis can reach highly natural synthesized
speech, the approach is inherently limited by properties of the speech
corpus used for the unit selection process. Concatenative systems can
only produce speech whose constituent segments (e.g., diphones) have
been pre\sphinxhyphen{}recorded. In order to make the synthesis sound natural, large
amounts of speech from a single speaker must therefore be available.
This limits the flexiblity of concatenative systems in producing
different voices, speaking styles, emotional expressions, or other
modifications to the sound that are common in everyday human
communication.

\sphinxAtStartPar
As an alternative to the concatenative approach, statistical parametric
speech synthesis (SPSS) is another TTS approach that has become highly
popular in the speech technology field. This is because it addresses the
main limitation of the concatenative systems — the lack of flexibility —
by generating the speech using statistical models of speech instead of
relying on pre\sphinxhyphen{}recorded segments. These statistical models are learned
from speech corpora using machine learning techniques, and they encode
information of how speech evolves as a function of time in the context
of a given input text.  In this respect, SPSS systems can be viewed as a
mirror image of \DUrole{xref,myst}{ASR} systems: while an ASR system
tries to convert speech from acoustic features to a string of words
using machine learning models, an SPSS system tries to convert a string
of words into acoustic features or directly to the acoustic waveform
using machine learning models. Both ASR and SPSS systems are typically
trained on a large amount of speech data with their transcriptions,
resulting in a set of \sphinxstyleemphasis{parameters} that describe \sphinxstyleemphasis{statistical
characteristics} of the speech data (hence “statistical parametric”
speech synthesis).

\sphinxAtStartPar
\sphinxincludegraphics{{175517696}.png}
\sphinxstylestrong{Figure 1:} A schematic view of an SPSS system.

\sphinxAtStartPar
A full SPSS system consists of text analysis, feature generation, and
waveform generation modules. The classical approach to SPSS is based on
a combination of a \sphinxstyleemphasis{hidden\sphinxhyphen{}Markov model Gaussian mixture model}
(HMM\sphinxhyphen{}GMM) architecture for feature generation and a \sphinxstyleemphasis{vocoder} for
waveform generation, and these will be discussed in more detail below.
Recent advances in neural network\sphinxhyphen{}based SPSS are then reviewed at the
end.


\subsection{Feature generation}
\label{\detokenize{Synthesis/Statistical_parametric_speech_synthesis:feature-generation}}
\sphinxAtStartPar
Given the linguistic description of the text\sphinxhyphen{}to\sphinxhyphen{}be\sphinxhyphen{}synthesized, the
purpose of the feature generation is to transform the linguistic
features into a corresponding description of the acoustic signal.
Similarly to ASR, this component mediating the two levels is called \sphinxstyleemphasis{an
acoustic model}. Technically speaking, the acoustic model converts the
linguistic input into a series of acoustic features at a fixed
frame\sphinxhyphen{}rate (e.g., one feature frame every 10 ms) using a probabilistic
mapping between the two. The mapping is learned from a training speech
corpus.

\sphinxAtStartPar
A standard approach for the probabilistic mapping has been to use a
HMM\sphinxhyphen{}GMM as the statistical parametric model. Similarly to an HMM\sphinxhyphen{}GMM ASR
system, the states \(s\) of the HMM correspond to parts of subword units
(e.g., parts of a phone, diphone, or triphone). Transition probabilities
P(s | st\sphinxhyphen{}1) between the states describe how the speech evolves through
each subword unit and from a unit to another. Acoustic characteristics
associated with each state are modeled with a GMM, where the GMM
describes a probability distribution \(P(y | s)\) over the possible
acoustic feature vectors in that state. Given a sequence of desired
subword units (as instructed by the linguistic features), the model can
be stochastically or deterministically sampled to produce a sequence of
acoustic features. These are then fed to \sphinxstyleemphasis{a waveform generation module}
to produce the actual speech signal. In the most basic form,
self\sphinxhyphen{}transitions from an HMM state to itself account for the duration
spent in that state (i.e., how many frames should the same acoustic
content be repeated). However, separate more advanced duration models
are often used to overcome the limitations of a first\sphinxhyphen{}order Markov chain
in modeling thetemporal dependencies and durational characteristics of
speech.

\sphinxAtStartPar
\sphinxincludegraphics{{175518368}.png}

\sphinxAtStartPar
\sphinxstylestrong{Figure 2:} A visual illustration of HMM\sphinxhyphen{}GMM\sphinxhyphen{}based speech feature
generation. State sequence \(s = \{s_1, s_2,...,s_{10}\}\) required for word “cat” (/k ae t/) is shown on
top, where each phoneme consists of three states: initial, center and
final state (e.g., \(k_{1}\), \(k_{2}\), and \(k_{3}\)). 
Each state is associated with an \(N\)\sphinxhyphen{}dimensional Gaussian mixture model
(GMM), where \(N\) is the dimensionality of the speech features \(y\). At
each time step, the GMM of the active state is sampled for a feature
vector \(y_{t}\).  After this, a state transition can occur to
a next state or back to the current state, controlling the durational
aspects of the speech.


\subsection{Waveform generation with vocoders}
\label{\detokenize{Synthesis/Statistical_parametric_speech_synthesis:waveform-generation-with-vocoders}}
\sphinxAtStartPar
A typical high\sphinxhyphen{}quality speech waveform consists of “continuous” (e.g.,
16\sphinxhyphen{}bit quantized) amplitude values sampled at 16 kHz. In addition, the
shape of the waveform is affected by several factors that do not
directly contribute to the naturalness or intelligibility of speech,
such as signal gain or phase and amplitude characteristics of the
recording and transmission chain. This means that mere 80 milliseconds
of a raw waveform — a typical length of one vowel — would correspond to
0.08 s\textbackslash{}16 kHz = 1280\sphinxhyphen{}dimensional amplitude vector, and that this
vector could take countless of shapes for perceptually highly similar
sounds. Moreover, the values encoded in this vector would be highly
correlated with each other (see \DUrole{xref,myst}{LPC}). Given the
high dimensionality, variability, and redundant nature of the waveform
signal representation, it is not an attractive target for statistical
parametric modeling with classical machine learning techniques (but see
also Neural SPSS below).

\sphinxAtStartPar
However, as we remember from speech feature extraction (see, e.g.,
\DUrole{xref,myst}{SFFT}), speech signal can be considered as
quasi\sphinxhyphen{}stationary in short windows of approx. 10–30 ms in duration.
Speech contents of the signal within these short windows can be
described using a set of spectral and source features (such as
\DUrole{xref,myst}{MFCCs} and \DUrole{xref,myst}{F0}) that are
assumed to be fixed for that window. When extracting the features in a
sliding window with short (e.g., 10 ms) window steps, the overall
structure of the signal can be captured with a much lower dimensional
and less variable representation than what the actual waveform would
be.  \sphinxstylestrong{A vocoder, then, is an algorithm that can 1) parametrize a
speech waveform into a more compact set of descriptive features as a
function of time, but also to 2) synthesize the speech back from the
features with minimal loss in speech quality}. In addition, many
vocoders use features that are interpretable in terms of speech
production or speech acoustics, enabling analysis and manipulation of
the speech signal to observe or cause certain phenomena in the speech
signal.

\sphinxAtStartPar
Compactness and invariance of the acoustic signal representation is also
why vocoding is used in SPSS systems:  instead of generating the speech
waveform directly, the feature generation module first generates a
lower\sphinxhyphen{}dimensional set of vocoder features that characterize the speech
signal with its essential properties. A vocoder then takes these
features as input and generates the corresponding waveform using a
series of signal processing operations. These operations are essentially
an inverse of the original feature extraction process, combined with
some additional mechanisms for re\sphinxhyphen{}introducing (or inventing) information
lost during the feature extraction process (such as signal phase that is
discarded from standard spectral features).

\sphinxAtStartPar
For instance, when using the popular STRAIGHT vocoder (Kawahara et al.,
1999), the HMM\sphinxhyphen{}GMM model first generates a sequence of feature vectors
that encode spectral envelope, F0, and periodicity characteristics of
the speech signal to\sphinxhyphen{}be\sphinxhyphen{}produced, as instructed by the text analysis
module. These features are then fed to STRAIGHT that synthesizes the
final speech waveform based on the features.

\sphinxAtStartPar
\sphinxincludegraphics{{175517700}.png}
\sphinxstylestrong{Figure 3:}
A schematic view of a vocoder and typical uses for vocoder features.When used as a part of an SPSS system, vocoder features are generated by
the parametric statistical model during the synthesis process.


\subsection{SPSS system training}
\label{\detokenize{Synthesis/Statistical_parametric_speech_synthesis:spss-system-training}}
\sphinxAtStartPar
Training of an SPSS system refers to estimation of the parametric
acoustic model (e.g., a HMM\sphinxhyphen{}GMM) that is responsible for mapping the
linguistic features to the corresponding waveform generation (vocoder)
features. This is achieved using a corpus of speech data, where each
utterance comes with the corresponding text of what was said, and
optionally with phonetic annotation describing the phonetic units and
their temporal positions in the waveform. First, the text analysis
module is used to create linguistic features of a training utterance
while the vocoder is used to extract vocoder features from the
corresponding speech waveform. Then the statistical model is trained to
minimize prediction error of the given vocoder features when the
linguistic features are used as inputs. Access to phonetic annotation
allows more accurate temporal alignment between the linguistic features
and the speech signal. Since the widely utilized HMM architecture for
acoustic modeling is not ideal for modeling speech segment durations, a
separate \sphinxstyleemphasis{duration model} is often trained to align the linguistic
features (which are agnostic of speaking rate and rhythm in the actual
speech data) with the phonetic units realized in the acoustic speech
signal. In the figure below, both the acoustic model and the duration
model are denoted jointly by the parametric statistical model block.

\sphinxAtStartPar
\sphinxincludegraphics{{175517698}.png}
\sphinxstylestrong{Figure 4:} A schematic view of SPSS system training.


\subsection{Advantages and disadvantages of the HMM\sphinxhyphen{}GMM SPSS compared to concatenative synthesis}
\label{\detokenize{Synthesis/Statistical_parametric_speech_synthesis:advantages-and-disadvantages-of-the-hmm-gmm-spss-compared-to-concatenative-synthesis}}
\sphinxAtStartPar
Since the “instructions” for speech generation are encoded by parameters
of the SPSS model, the model can easily be adapted to produce speech
with different characteristics. For instance, the vocal tract
characteristics of the training speaker are encoded by the means and
variances of the Gaussian distributions in each HMM state whereas
durational characteristics are encoded by the transition probabilty
matrix of the HMM. Therefore, the system can be adapted to other
speakers by simply adapting the pre\sphinxhyphen{}trained HMM\sphinxhyphen{}GMM using speech from a
new talkers. In this case, standard techniques such as
Maximum\sphinxhyphen{}a\sphinxhyphen{}posteriori (MAP) adaptation or Maximum\sphinxhyphen{}likelihood linear
regression (MLLR) can be used to update the model parameters. In
addition, since the parameters of the HMM\sphinxhyphen{}GMM are often interpretable in
terms of speech spectral envelope or phonation characteristics, it is
possible to either modify the models or to post\sphinxhyphen{}process the resulting
acoustic features in order to achieve desired effects. For example,
changing of the speech pitch can be done by simply adjusting the F0
parameter, whereas reduction of some synthesis artifacts such as muffled
sound quality due to statistical averaging can be attempted by adjusting
the GMM parameters with a chosen transformation.

\sphinxAtStartPar
The potential disadvantages of the statistical approach include sound
quality issues (e.g., muffledness) due to statistical smoothing taking
place in a stochastic generative model, sound quality of the used
vocoders, and potential problems in robust statistical model estimation
from finite data.


\subsection{Neural SPSS}
\label{\detokenize{Synthesis/Statistical_parametric_speech_synthesis:neural-spss}}
\sphinxAtStartPar
Recent advances in artificial neural networks (ANNs) have also led to
new developments in SPSS beyond the classical HMM\sphinxhyphen{}GMM framework. In
terms of vocoding, WaveNet (van den Oord et al., 2016) is a highly
influential neural network waveform generator that can produce
high\sphinxhyphen{}quality speech. It is based on an autoregressive convolutional
neural network (CNN) architecture and it operates directly on the speech
waveforms.  Given a history of previous waveform samples and some
“conditioning” information on what type of signal should be produced,
the model predicts the most likely next speech waveform sample at each
time step. For instance, WaveNet can be trained to produce speech from
spectral features such as log\sphinxhyphen{}Mel energies and F0 information. Although
WaveNet can reach near\sphinxhyphen{}human naturalness of the produced speech (with
certain limitations), waveform\sphinxhyphen{}level autoregressive processing is also
computationally extremely expensive. Development of computationally
flexible high\sphinxhyphen{}quality neural vocoders is therefore still an active
research area.

\sphinxAtStartPar
In addition to vocoding, neural networks have become commonplace
replacements for the HMM\sphinxhyphen{}GMM in the feature generation stage. For
instance, deep feed\sphinxhyphen{}forward networks or LSTMs can be utilized in the
feature generation. Since LSTMs are especially good at modeling temporal
dependencies, they can theoretically handle larger temporal ambiguity
and variability between the input linguistic specifications and the
target vocoder features.

\sphinxAtStartPar
Given sufficient training data, it is also possible to implement the
entire chain from written text to the synthesized waveform using a
neural network system. Tacotron 2 is an example of such a system, where
the input text is processed by a sequence\sphinxhyphen{}to\sphinxhyphen{}sequence ANN model to
directly create a log\sphinxhyphen{}Mel spectrogram corresponding to the input text
(i.e., without a dedicated text analysis module). The spectrogram is
then fed to the WaveNet module (see above) to produce the speech signal.
As a result, Tacotron 2 and the Wavenet vocoder can together achieve
highly impressive speech quality. The advantage of these type end\sphinxhyphen{}to\sphinxhyphen{}end
approaches is that there are fewer assumptions regarding what kind of
intermediate representations are good for the task at hand, reducing the
risk that the pre\sphinxhyphen{}specified operations and representations cause a loss
of relevant information in the pipeline. There is also no need for deep
understanding of the linguistic structure underlying written and spoken
language or access to pre\sphinxhyphen{}existing text analysis tools, making
deployment of the systems possible for any language with sufficient
training data (text and corresponding speech). Since all the components
are based on differentiable neural network operations, it is also
possible to jointly optimize the entire chain from waveform generation
to text processing. In principle, neural SPSS systems are also highly
flexible, as basically any type of side information can be injected to
the system to adjust the characteristics of the produced speech.

\sphinxAtStartPar
Neural systems, however, also have their drawbacks. The amount of data
and computational resources required to train these systems can be high.
Runtime computational requirements of neural vocoders may also be
problematic in some applications, although recent advances in vocoders
and in parallelization of the computations has already led to
significant advances in this respect. Another issue is the lack of
interpretability and transparency of model parameters: while parameters
of classical models such as HMMs and GMMs have relatively clear
relationship with what are the inputs and outputs of the system, the
same is not true for ANNs with multiple layers. This makes it much more
difficult to understand the behavior of the model, especially when
trying to overcome problems in model performance. Lack of transparency
and interpretability also means that manual control of characteristics
of the produced speech is more difficult. Finally, adaptation of the
models to new data (e.g., a new speaker or speaking style) cannot make
use of the well\sphinxhyphen{}understood mathematical solutions available to the
classical models. In contrast, the design, training, and adaptation of
ANNs are much more heuristics\sphinxhyphen{}driven, similarly to the use of ANNs in
any other machine learning domain.


\subsection{Further reading}
\label{\detokenize{Synthesis/Statistical_parametric_speech_synthesis:further-reading}}
\sphinxAtStartPar
Kawahara, K., Masuda\sphinxhyphen{}Katsuse, I., and de Cheveigné , A. (1999).
Restructuring speech representations using a pitch\sphinxhyphen{}adaptive
time–frequency smoothing and an instantaneous\sphinxhyphen{}frequency\sphinxhyphen{}based F0
extraction: Possible role of a repetitive structure in sounds, Speech
Communication, 27, 187–207. (STRAIGHT vocoder)

\sphinxAtStartPar
Yamagishi, J. (2006). An introduction to HMM\sphinxhyphen{}based speech synthesis. 
\sphinxurl{https://wiki.inf.ed.ac.uk/pub/CSTR/TrajectoryModelling/HTS-Introduction.pdf}  
(introduction to HMM\sphinxhyphen{}based SPSS)

\sphinxAtStartPar
Shen, J. et al. (2017). Natural TTS synthesis by conditioning WaveNet on
Mel spectrogram predictions. ArXiV pre\sphinxhyphen{}print:
\sphinxurl{https://arxiv.org/abs/1712.05884}   (Tacotron 2)

\sphinxAtStartPar
Tokuda, K., Nankaku, Y., Toda, T., Zen, H., Yamagishi, Y., and Oura, K.
(2013). Speech synthesis based on hidden Markov models. \sphinxstyleemphasis{Proceedings of
the IEEE}, 101, 1234–1252. (introduction to SPSS)**

\sphinxAtStartPar
van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O.,
Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. (2016).
\sphinxstyleemphasis{WaveNet: A generative model for raw audio.} * ArXiV pre\sphinxhyphen{}print:
\sphinxurl{https://arxiv.org/pdf/1609.03499.pdf}  (WaveNet original paper)**

\sphinxAtStartPar
Wu, Z., Watts, O., and King, S. (2016). Merlin: An Open Source Neural
Network Speech Synthesis System. \sphinxstyleemphasis{In Proc. 9th ISCA Speech Synthesis
Workshop (SSW9)}, September 2016, Sunnyvale, CA,
USA  \sphinxurl{https://github.com/CSTR-Edinburgh/merlin} (Merlin toolkit for
synthesis)**

\sphinxAtStartPar
Zen, H., Tokuda, K., and Black, A. W. (2009). Statistical parametric
speech synthesis. \sphinxstylestrong{Speech Communication}, 51, 1039–1064.
\sphinxurl{https://www.sciencedirect.com/science/article/pii/S0167639309000648}
(introduction to SPSS)**

\sphinxstepscope


\chapter{Transmission, storage and telecommunication}
\label{\detokenize{Transmission_storage_and_telecommunication:transmission-storage-and-telecommunication}}\label{\detokenize{Transmission_storage_and_telecommunication::doc}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Transmission/Design_goals::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Design goals}}}}

\item {} 
\sphinxAtStartPar
Basic tools
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Transmission/Modified_discrete_cosine_transform_MDCT::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Modified\sphinxhyphen{}discrete cosine transform
MDCT}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Transmission/Entropy_coding::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Entropy coding}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Transmission/Perceptual_modelling_in_speech_and_audio_coding::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Perceptual modelling in speech and audio
coding}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Modelling/Vector_quantization_VQ::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Vector quantization (VQ)}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Representations/Linear_prediction::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Linear prediction}}}}

\end{enumerate}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Transmission/Code-excited_linear_prediction_CELP::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Code\sphinxhyphen{}excited linear prediction
(CELP)}}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Transmission/Frequency-domain_coding::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Frequency\sphinxhyphen{}domain coding}}}}

\end{enumerate}

\sphinxstepscope


\section{Design goals}
\label{\detokenize{Transmission/Design_goals:design-goals}}\label{\detokenize{Transmission/Design_goals::doc}}
\sphinxAtStartPar
In short, the aim of speech coding methods is primarily to enable
natural and efficient spoken communication over a geographical distance,
given constraints on available resources. In other words, we want to be
able to talk with a distant person with the aid of technology. Usually
distance refers to location, but speech coding can be (and is often)
used for storing speech signals (such that distance refers to distance
in \sphinxstyleemphasis{time}). {[}\hyperlink{cite.Transmission/Modified_discrete_cosine_transform_MDCT:id43}{Bäckström \sphinxstyleemphasis{et al.}, 2017}{]}

\sphinxAtStartPar
In particular, aspects of quality which we can be included in our design
goals are for example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Acoustic quality} in the sense that the the reproduced acoustical
signal should be similar to the original signal (measured for
example in terms of signal to noise ratio or a perceptually weighted
variant thereof).

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Perceptual transparency} refers to a property of high\sphinxhyphen{}accuracy
coding systems, where a human listener cannot perceive a difference
between the original and reconstructed signals. When discussing
transparency, we however need to accurately define the methodology
with which we measure transparency. Namely, if we compare small
segments of an original and reconstructed signals, we can easily
hear minute differences, which would never be perceived in a
realistic use\sphinxhyphen{}case.

\item {} 
\sphinxAtStartPar
*Intelligibility *such that a listener can interpret the linguistic
meaning of the reproduced signal.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Delay} in the communication path, end\sphinxhyphen{}to\sphinxhyphen{}end, should be within
reasonable limits (e.g. below 150 ms). A higher delay can impede the
naturalness of a dialogue.

\item {} 
\sphinxAtStartPar
*Noisyness *caused by low\sphinxhyphen{}accuracy quantization and background
noises should be minimized.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Distortions} of the speech signal to the amount the original signal
is perceived to be changed by the processing.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Naturalness} of the speech signal refers to high natural vs.
non\sphinxhyphen{}natural a speech signal sounds. For example, some types of
non\sphinxhyphen{}natural speech signals could be such which sound robotic,
metallic or muffled.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Listening effort} refers to the effort a listener needs to use a
telecommunications system, and it should be minimized. Effort can be
both due to properties of the user\sphinxhyphen{}interface, but importantly also,
the listener might have to exert effort to understand a distorted
speech signal.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Annoyance} is closely related to listening effort, in that a signal
which is intelligible can have so severe distortions, noisiness or
transmission delays that it is really annoying. Usually annoyance
thus also increase listening effort.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Perception of distance} is the a feeling that the participant has
of the distance between participants. The main contributor to the
perception of distance is probably room acoustics, such that if the
distance between speaker and microphone is large, than the listener
feels distant to the speaker. It affects for example the intimacy of
a discussion, such that it is hard to feel intimate if the distance
is large.

\end{itemize}

\sphinxAtStartPar
It is clear that different types of quality are prominent at different
levels of coding\sphinxhyphen{}accuracy, which in turn is a function of available
bitrate (bandwidth). As a rough characterization, with current
technology, the quality\sphinxhyphen{}issues we optimize at different bitrates are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
At extremely low bitrates (below 1kbp/s), we cannot hope to encode
speech at high quality. At best, we can hope to retain
\sphinxstyleemphasis{intelligibility}.

\item {} 
\sphinxAtStartPar
At very low bitrates (1\sphinxhyphen{}2 kbp/s), intelligibility can usually be
preserved, but speech signals can still be distorted and noisy, such
that we want to minimize \sphinxstyleemphasis{listening effort}.

\item {} 
\sphinxAtStartPar
At low rates (3\sphinxhyphen{}8 kbps/s), we often have to balance between avoiding
noisyness, distortions, annoyance and naturalness. In other words,
we can often reduce noisyness by making the signal more muffled, but
that would reduce naturalness. It is then very much a question of
individual taste to choose which balance of distortions is best.

\item {} 
\sphinxAtStartPar
At medium rates (8\sphinxhyphen{}16 kbp/s), speech signals can already be coded
with a high quality such that we can try to minimize the number of
audible (perceivable) distortions. At these rates telecommunication
systems can often, in practice, be transparent in the sense that
users are not actively aware of any distortions, even if they would
clearly notice distortions in a comparison with the original signal.

\item {} 
\sphinxAtStartPar
At high rates (above 16 kbp/s), it should in general be possible to
encode speech signals at a perceptual transparent level. However, if
computational resources are limited and in applications which
require extremely low delay, distortions can still be audible.

\end{itemize}

\sphinxAtStartPar
Performance of a codec is however always a compromise between quality
and resources. By increasing the amount of computational resources (or
bandwidth) we can improve quality ad infinitum. The most important
limited resources are
\begin{itemize}
\item {} 
\sphinxAtStartPar
Bandwidth, that is, the bit\sphinxhyphen{}rate at which we can transmit data. It
is limited by
\begin{itemize}
\item {} 
\sphinxAtStartPar
physical constraints such as available radio channels,

\item {} 
\sphinxAtStartPar
power consumption (battery, ecology and price) and

\item {} 
\sphinxAtStartPar
infrastructure capacity (investment, complexity, power).

\end{itemize}

\item {} 
\sphinxAtStartPar
CPU, that is, the amount of operations per second that can be
performed, which is further limited by
\begin{itemize}
\item {} 
\sphinxAtStartPar
investment cost and

\item {} 
\sphinxAtStartPar
power consumption (battery, ecology and price).

\end{itemize}

\item {} 
\sphinxAtStartPar
Memory, that is, the amount of RAM and ROM which is needed for the
system, which are limited by
\begin{itemize}
\item {} 
\sphinxAtStartPar
investment cost and

\item {} 
\sphinxAtStartPar
power consumption (battery, ecology and price).

\end{itemize}

\end{itemize}

\sphinxAtStartPar
Furthermore, the use\sphinxhyphen{}case of the intended speech (and audio) codec has
many important effects on the overall design. For example, the systems
configuration can be one of the following:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{One\sphinxhyphen{}to\sphinxhyphen{}one}; the classic telephony conversation, where two phones
transmit speech between them.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{One\sphinxhyphen{}to\sphinxhyphen{}many}; could be applicable for example in a setting like a
radio\sphinxhyphen{}broadcast or in a storage application, where we encode once
and have potentially multiple receivers/decoders. Since there are
many receivers, we would then prefer that decoding the signal does
not require much resources. In practice that means that the sender
side (encoder) can use proportionally more resources.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Many\sphinxhyphen{}to\sphinxhyphen{}many;} the typical teleconferencing application, often
implemented with a cloud\sphinxhyphen{}server, such that merging of individual
speakers can be done centrally, such that bandwidth to the many
receivers can be saved.

\item {} 
\sphinxAtStartPar
*Many\sphinxhyphen{}to\sphinxhyphen{}one; *could be a distributed sensor\sphinxhyphen{}array scenario, where
multiple devices in a room jointly record speech. Since we then have
many encoders, they should be very simple and a majority of the
intelligence and computational resources should be at the receiver
end.

\end{itemize}

\sphinxAtStartPar
The overall design is also influenced by the type of transmission link.
In particular, the first few generations of digital mobile phones
operated with
\sphinxhref{https://en.wikipedia.org/wiki/Circuit\_switching}{circuited\sphinxhyphen{}switched}
networks, where a fixed amount of bandwidth is allocated to every
connection. Newer networks are however based on
\sphinxhref{https://en.wikipedia.org/wiki/Packet\_switching}{packet\sphinxhyphen{}switched}
designs, where data is transmitted essentially over the internet and
capacity and routing is optimized on the fly. Packet\sphinxhyphen{}switched networks
can in practice be much better optimized for overall cost and
performance. However, a packet\sphinxhyphen{}switched network cannot guarantee a
steady flow of packets, such that the receiver has to tolerate both
delayed or missing packets as well as packets which arrive in the wrong
order. Clearly this has an impact on both overall transmission delay of
the system, as well as increases the computational complexity of the
receiver. The costs are however usually balanced by the savings gained
in network optimization.

\sphinxAtStartPar
A further important related aspect are assumptions about lost packets in
general. In many storage and broadcast applications we can assume that
packets are not lost and that all data is available at the receiver. It
however much more common that we must assume that some packets are lost.
Among the most important consequences of lost packets for the design is
that in decoding the signal, we cannot assume that we have access to
previous packets. Specifically, if decoding of the current packet
depends on the previous packet, then a single lost packet would make us
unable to decode any of the following packets. Clearly such sensitivity
to lost packets is unacceptable in most real\sphinxhyphen{}world transmission systems.
However, we could encode speech with much higher efficiency, if we were
allowed to use previous packets to predict the current packet. The
likelihood of lost packets thus dictates the compromise between
sensitivity to lost packets and coding (compression) efficiency.


\subsection{References}
\label{\detokenize{Transmission/Design_goals:references}}
\sphinxstepscope


\section{Modified discrete cosine transform (MDCT)}
\label{\detokenize{Transmission/Modified_discrete_cosine_transform_MDCT:modified-discrete-cosine-transform-mdct}}\label{\detokenize{Transmission/Modified_discrete_cosine_transform_MDCT::doc}}

\subsection{Introduction}
\label{\detokenize{Transmission/Modified_discrete_cosine_transform_MDCT:introduction}}
\sphinxAtStartPar
In processing speech and audio, we often need to split the signal into
segments or {\hyperref[\detokenize{Representations/Windowing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{windows}}}}, because
\begin{itemize}
\item {} 
\sphinxAtStartPar
audio signals are relatively slowly changing over time, such that by
segmenting the signal in short windows allows assuming that the
signal is stationary, which is a pre\sphinxhyphen{}requisite of many efficient
methods such as {\hyperref[\detokenize{Representations/Spectrogram_and_the_STFT::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{spectral analysis}}}},

\item {} 
\sphinxAtStartPar
transform\sphinxhyphen{}domain analysis, such as spectral analysis, requires that
we transform the entire signal in one operation, which in turn
requires that we have received the entire signal, before we can
start processing. In for example telecommunications, this would mean
that we have to wait for a sentence to finish before we can begin
sending, and thus reconstruction the sentence at the receiver would
have to wait until the whole sentence has finished, incurring a
significant delay in communication.

\end{itemize}

\sphinxAtStartPar
In practical scenarios, we thus always apply windowing before
processing.

\sphinxAtStartPar
Applying spectral processing, such as processing in the
{\hyperref[\detokenize{Representations/Spectrogram_and_the_STFT::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{STFT}}}} domain, in \sphinxstyleemphasis{telecommunications
applications} however has a significant drawback. We need windows which
are \sphinxstyleemphasis{overlapping} to allow \sphinxstyleemphasis{perfect reconstruction}, that is, we require
that we can perfectly reconstruct the original signal from the windowed
signal. Overlaps however mean that consecutive windows share information
from the same samples (see Figure on the right). The same information
thus has to be encoded in \sphinxstyleemphasis{both windows}, which is inefficient. With
coding a signal, our target is to compress the information to the fewest
possible bits, but if we encode a part of the information twice, we
increase the amount of information we need to encode. The STFT domain
thus causes \sphinxstyleemphasis{over\sphinxhyphen{}coding}, where in effect, two different bit\sphinxhyphen{}streams
could represent the same output signal.

\sphinxAtStartPar
The \sphinxstyleemphasis{modified discrete cosine transform} (MDCT) is a solution to the
over\sphinxhyphen{}coding problem, which uses projection\sphinxhyphen{}operators at the overlaps
such that the information in consecutive windows are orthogonal to each
other. Since information is therefore perfectly retained, the amount of
information is not increased nor decreased, and we say that this
operator is \sphinxstyleemphasis{critically sampled}.

\sphinxAtStartPar
The MDCT thus provides a time\sphinxhyphen{}frequency representation of the input
signal, where we can analyse and process the time\sphinxhyphen{}evolution of
frequency\sphinxhyphen{}components. It shares most of the beneficial properties of the
STFT, wherein a signal processed in the MDCT\sphinxhyphen{}domain, remains continuous
in the time\sphinxhyphen{}domain and we can use FFT\sphinxhyphen{}based operations for efficient
implementations. Perhaps the most significant difference between the
STFT and MDCT, other than perfect sampling of the MDCT, is that where
the STFT is a complex\sphinxhyphen{}valued representation, the MDCT is a real\sphinxhyphen{}valued
representation when the input is real\sphinxhyphen{}valued.

\sphinxAtStartPar
Due to these beneficial properties of the MDCT, it is the most commonly
used time\sphinxhyphen{}frequency transform in speech and audio coding and employed in
standardized codecs such as MPEG USAC, 3GPP EVS and Bluetooth LC3.

\sphinxAtStartPar
\sphinxincludegraphics{{1482950021}.png}


\subsection{Executive summary of algorithm}
\label{\detokenize{Transmission/Modified_discrete_cosine_transform_MDCT:executive-summary-of-algorithm}}
\sphinxAtStartPar
The MDCT is based on taking the symmetric and anti\sphinxhyphen{}symmetric parts of
the signal at the overlap, such that the symmetric part goes to the one
window and the antisymmetric to the other. We furthermore window those
symmetric and antisymmetric parts such that they converge smoothly to
zero at the borders. Consecutive windows are thus orthogonal at the
overlap such that information is split exactly in half and we obtain
perfect reconstruction and critical sampling. Moreover, since the
segments are windowed, we do not have any difficulties with
discontinuities.

\sphinxAtStartPar
Secondly, we take the discrete cosine transform (DCT) of the windowed
signal (=of the orthogonal projection). The conventional DCT has to,
however, be adjusted such that its phase and symmetries match with the
symmetric and anti\sphinxhyphen{}symmetric parts. Hence the name \sphinxstyleemphasis{modified DCT or
MDCT}. The matching of symmetries is possible only through this
modification of the DCT and cannot be generalized to the discrete
Fourier transform (DFT).

\sphinxAtStartPar
The (anti)symmetric part of signal then “looks like the signal” which
would have this symmetry. In other words, we introduce a \sphinxstyleemphasis{deviation}
from the original signal and this deviation is known as a \sphinxstyleemphasis{time\sphinxhyphen{}domain
aliasing} component, since it is similar to the aliasing effects which
occur with the DFT. However, in reconstruction, we add the symmetric and
anti\sphinxhyphen{}symmetric parts together, such that they cancel each other, which
is known as \sphinxstyleemphasis{time\sphinxhyphen{}domain aliasing cancellation (TDAC)}. The TDAC is
equivalent with the orthogonal projection laying at the basis of the
MDCT, and thus a central property of the MDCT.


\subsection{Definition}
\label{\detokenize{Transmission/Modified_discrete_cosine_transform_MDCT:definition}}
\sphinxAtStartPar
Suppose \(x\) is a \(N\times1\) vector representing the overlapping region shared
by two consecutive windows, such as the are between 15 and 30 ms in the
figure above and right. Our task is to design a projection matrix \(P\) of
size \(N\times N\), such that we can split \(x\) into two orthogonal parts as
\begin{equation*}
\begin{split} \begin{cases} x_L &= Px\\ x_R &= P_0x \end{cases} \end{split}
\end{equation*}
\sphinxAtStartPar
where the \(P_{0}\) is the kernel of \(P\) and thus \( P_0 P^T = 0,
\) such that we can reconstruct \(x\) as
\begin{equation*}
\begin{split} P^T x_L+P_0^Tx_R = P^T P x + P_0^T P_0 x = (P^T P + P_0^T P_0)x =
x, \end{split}
\end{equation*}
\sphinxAtStartPar
since \( P^T P + P_0^T P_0 = I \) by definition.

\sphinxAtStartPar
The vectors \(x_{L}\) and \(x_{R},\) corresponding to the
left and right windows, are thus both of length N/2 and contain each
exactly half the information of \(x\).

\sphinxAtStartPar
Though this is a mathematically clean solution, it is not practical,
since after processing the reconstructed signal could have
discontinuities. For example, suppose that we define \(P\) as the operator
which extracts the symmetric part of \(x\), that is, \( P = [I,
J]\sqrt{1/2}, \) where \(I\) is the \((N/2)\times(N/2)\) identity matrix and
\(J\) its left\sphinxhyphen{}right flipped counterpart (reverse diagonal). The kernel is
then \( P_0 = [I, -J]\sqrt{1/2}. \) If we make a small modification
to the signal as \( x_L':=x_L+d, \) then the reconstruction is \(
x':=P^Tx_L+P^Td+P_0^Tx_R=x+P^Td, \) which means that original signal is
changed by \(P^Td\). Unfortunately, however, this component does
not go to zero at the border such that processed signal is not
continuous. Specifically, for example, consider a vector \( d =
[1,\,1,\,\dotsc,\,1]^T, \) which gives \( P^Td =
\sqrt{1/2}[1,\,1,\,\dotsc,\,1]^T, \) which is clearly non\sphinxhyphen{}zero
at both ends of the vector.

\sphinxAtStartPar
The reason for this problem is that the definition did not include a
windowing function. Recall that windowing is sample\sphinxhyphen{}by\sphinxhyphen{}sample
multiplication of the signal with a windowing function. In matrix
notation, we can implement this by multiplying the input signal \(x\) with
a diagonal matrix \(W\), where the diagonal elements correspond to the
windowing function. The windowed signal is then \(Wx\). The symmetric
projection \(P\) can then be applied on the windowed signal such that the
windowed left and right signals are
\begin{equation*}
\begin{split} 
\begin{cases} x_L &= PWx := P'x\\ x_R &= P_0'x = P_0JWJx
\end{cases} 
\end{split}
\end{equation*}
\sphinxAtStartPar
where the \(P_{0}'\) is the kernel of \(P'\), and \(JWJ\) is simply
the backwards part of the window (if \(W\) is the increasing left part of
the window, then \(JWJ\) is the decreasing right part of the window). Note
that the relationship between the two kernels, \( P_0'=P_0JWJ \)
applies only to the symmetric operator defined above, but generally not.
In any case, the above definition now includes windowing and the
orthonormal projection such that the beneficial properties of perfect
reconstruction and critical sampling are retained, but now the signal is
also going smoothly to zero at the edges, because we applied a windowing
function.

\sphinxAtStartPar
Above we have thus developed a windowing operation which has the
properties we need. However, our goal was to obtain a time\sphinxhyphen{}frequency
transform of the signal, so we are still missing the time\sphinxhyphen{}frequency
transform.

\sphinxAtStartPar
Our objective is to find a spectral representation of the windowed input
signal. Importantly, note that above we studied the properties at the
overlap, where we took an orthonormal projection of the overlap to
obtain the left\sphinxhyphen{}part of the current window \(x_{L}\) and
right\sphinxhyphen{}part of the preceeding window \(x_{R}\). To avoid confusion,
we here have to include the frame indices, such that the overlapping
parts are now, respectively, \(x_{L,k}\) and \(x_{R,k-1}\).
The spectral representation, however, should come from a single frame,
that is, we define frame \(k\) as
\begin{equation*}
\begin{split} \widehat x_k := \begin{bmatrix}x_{L,k} \ x_{R,k}
\end{bmatrix}. \end{split}
\end{equation*}
\sphinxAtStartPar
To obtain the spectrum of \( \widehat x_k \) we can then multiply it
with a time\sphinxhyphen{}frequency transform matrix \(D\), such that the frequency
representation is  \( y_k=D\widehat x_k \) and the reconstruction
is  \( \widehat x_k = D^T y_k, \) assuming that \(D\)
is orthogonal (as time\sphinxhyphen{}frequency transforms usually are).

\sphinxAtStartPar
In principle, the transform matrix \(D\) could be any time\sphinxhyphen{}frequency
transform (DFT, DCT\sphinxhyphen{}I, DCT\sphinxhyphen{}II, DST\sphinxhyphen{}I etc.), but we need to choose one
specific. To evaluate the sanity of a particular choice, we should check
how well the transform works together with the prior orthogonal
projections. In particular, let us substitute the defitions of the left
and right parts into the above equation to obtain
\begin{equation*}
\begin{split} 
\widehat x_k := \begin{bmatrix}x_{L,k} \\ x_{R,k}
\end{bmatrix} = \begin{bmatrix}PWx_k \\ P_0JWJx_{k+1}
\end{bmatrix} = \begin{bmatrix}PW & P_0JWJ \end{bmatrix}
\begin{bmatrix}x_k \\ x_{k+1} \end{bmatrix} := \hat P
\begin{bmatrix}x_k \\ x_{k+1} \end{bmatrix}. 
\end{split}
\end{equation*}
\sphinxAtStartPar
The combined windowing+projection+spectral transform is thus  
\(y_k=D\hat P \begin{bmatrix}x_k \\ x_{k+1} \end{bmatrix} = M \begin{bmatrix}x_k \\ x_{k+1} \end{bmatrix}. \)
To evaluate the
sanity of a particular time\sphinxhyphen{}frequency transform, we thus have to study
the basis functions of matrix \(M\). By exhaustively going through
different options, it is simple to see that the implicit even/odd
extensions of DCTs align with the symmetric/antisymmetric projections
only for DCT\sphinxhyphen{}III. For the other options, we always get discontinuities
in the basis functions.

\sphinxAtStartPar
Heuristically speaking, we want the basis functions to “look like”
plausible signals, which could appear in a naturally appearing physical
system. More formally, such “nice looking” basis functions have less
leakage between spectral bins, and thus offer a more accurate
description of the spectral characteristics of the input signal.
{[}\hyperlink{cite.Transmission/Modified_discrete_cosine_transform_MDCT:id43}{Bäckström \sphinxstyleemphasis{et al.}, 2017}{]}

\sphinxAtStartPar
See also \sphinxurl{https://github.com/audiolabs/lapped-transforms}


\subsection{References}
\label{\detokenize{Transmission/Modified_discrete_cosine_transform_MDCT:references}}
\sphinxstepscope


\subsection{Entropy coding}
\label{\detokenize{Transmission/Entropy_coding:entropy-coding}}\label{\detokenize{Transmission/Entropy_coding::doc}}
\sphinxAtStartPar
In transmission and storage of data, it is useful if we can minimize the
number of bits needed to uniquely represent the input. With \sphinxstyleemphasis{entropy
coding}, we refer to methods which use statistical methods to compress
data. The target is \sphinxstyleemphasis{lossless} encoding, where the original data can be
perfectly reconstructed from the compressed representation. With \sphinxstyleemphasis{lossy}
coding, similarly, we refer to compression where, for example, we have a
limited number of bits to use and we try to reproduce a signal as
similar as possible to the original, but not necessarily exactly the
same. In speech and audio, coding usually refers to lossy coding. The
objective is to compress the coded signal such that it remains
perceptually indistinguishable from the original or such that the
perceptual effect of quantization is minimized. Such coding is known as
\sphinxstyleemphasis{{\hyperref[\detokenize{Transmission/Perceptual_modelling_in_speech_and_audio_coding::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{perceptual coding}}}}}.
Often, perceptual coding is performed in two steps; 1) quantization of
the signal such that the perceptually degrading effect of quantization
is minimized and 2) lossless coding of the quantized signal. In this
sense, even if lossless and lossy coding are clearly different methods,
a lossless coding module is often included also in lossy codecs.

\sphinxAtStartPar
Entropy coding operates on an abstract level such that it can operate on
any set of symbols as long as we have information about the
probabilities of each symbol. For example, consider a integer\sphinxhyphen{}valued
scalar \(x\), which can attain values \sphinxhyphen{}1, 0, and +1, with respecitve
probabilities 0.25, 0.5, 0.25. That is, if we repeatedly draw scalars
\(x\) from this distribution, then on average, 25\% of them are \sphinxhyphen{}1’s. It is
then irrelevant what the numerical values of \(x\) are, we can
equivalently name the distinct elements according to symbols of the
alphabet as \(a, b\) and \(c\).The table on the right demonstrates a possible encoding of these
numbers. Clearly we need more than one bit to encode three symbols, and
hence this encoding uses 2 bits per symbol. Observe, however, that the
bit\sphinxhyphen{}string 11 is not used, which means that the encoding is inefficient.

\sphinxAtStartPar
A naive encoding of a set of numbers, with 2 bits per symbol.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
number
&\sphinxstyletheadfamily 
\sphinxAtStartPar
symbol
&\sphinxstyletheadfamily 
\sphinxAtStartPar
bit\sphinxhyphen{}string
&\sphinxstyletheadfamily 
\sphinxAtStartPar
length
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
a
&
\sphinxAtStartPar
00
&
\sphinxAtStartPar
2
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
b
&
\sphinxAtStartPar
01
&
\sphinxAtStartPar
2
\\
\hline
\sphinxAtStartPar
+1
&
\sphinxAtStartPar
c
&
\sphinxAtStartPar
10
&
\sphinxAtStartPar
2
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsubsection{Vector coding}
\label{\detokenize{Transmission/Entropy_coding:vector-coding}}
\sphinxAtStartPar
To take better use of all bits, we can instead of single symbols,
consider a vector of symbols \( x_1,x_2,x_3 \) . With 3 possible
symbols for each element, we have \(3^{3}=27\) possible
combinations. To encode it we thus need \({\mathrm{ceil}}(\log_2(27))=5 \) bits, or 1.66 bits per sample. In
comparison to the original 2 bits per sample above, this is a clear
improvement. However, we still have 5 unused bit\sphinxhyphen{}strings, which shows
that this encoding is sub\sphinxhyphen{}optimal.

\sphinxAtStartPar
Note that there are immediate parallels with \sphinxstyleemphasis{\DUrole{xref,myst}{vector quantization
(VQ)}} though the two methods are not the same.
In short, vector quantization is lossy coding, which finds the best
quantization with a given set of symbols, whereas vector coding is
lossless coding of vectors of symbols.

\sphinxAtStartPar
A naive encoding of a set of numbers, with 2 bits per symbol.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
numbers
&\sphinxstyletheadfamily 
\sphinxAtStartPar
symbols
&\sphinxstyletheadfamily 
\sphinxAtStartPar
bit\sphinxhyphen{}string
&\sphinxstyletheadfamily 
\sphinxAtStartPar
length
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{}1, \sphinxhyphen{}1, \sphinxhyphen{}1
&
\sphinxAtStartPar
aaa
&
\sphinxAtStartPar
00000
&
\sphinxAtStartPar
5
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{}1, \sphinxhyphen{}1, 0
&
\sphinxAtStartPar
aab
&
\sphinxAtStartPar
00001
&
\sphinxAtStartPar
5
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{}1, \sphinxhyphen{}1, +1
&
\sphinxAtStartPar
aac
&
\sphinxAtStartPar
00010
&
\sphinxAtStartPar
5
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{}1, 0 \sphinxhyphen{}1
&
\sphinxAtStartPar
aba
&
\sphinxAtStartPar
00011
&
\sphinxAtStartPar
5
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{}1, 0, 0
&
\sphinxAtStartPar
abb
&
\sphinxAtStartPar
00100
&
\sphinxAtStartPar
5
\\
\hline
\sphinxAtStartPar
…
&
\sphinxAtStartPar
…
&
\sphinxAtStartPar
…
&
\sphinxAtStartPar
…
\\
\hline
\sphinxAtStartPar
+1, +1, +1
&
\sphinxAtStartPar
ccc
&
\sphinxAtStartPar
11011
&
\sphinxAtStartPar
5
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsubsection{Variable length and Huffman coding}
\label{\detokenize{Transmission/Entropy_coding:variable-length-and-huffman-coding}}
\sphinxAtStartPar
A central concept in entropy coding \sphinxstyleemphasis{variable length} coding, where
symbols can be encoded with bit\sphinxhyphen{}strings of varying lengths. In our above
example, an optimal coding (i.e. optimal bit\sphinxhyphen{}strings) is listed in the
table on the right.

\sphinxAtStartPar
The average number of bits per symbol is then the sum of the length of
each bit\sphinxhyphen{}string multiplied with the corresponding probability, that is,
\begin{equation*}
\begin{split} E[bits/symbol] = \sum_{k\in\{a,b,c\}}P_k L_k = 0.25\times
2 + 0.5\times 1 + 0.25\times 2 = 1.5. \end{split}
\end{equation*}
\sphinxAtStartPar
From the bit\sphinxhyphen{}strings 00, 01 and 1, we can clearly decode the original
symbols; if the first bit is one, then the symbol is \(b\), otherwise the
second bit determines whether the symbol is \(a\) or \(c.\)

\sphinxAtStartPar
Such variable length codes can be readily constructed when the
probabilities are negative powers of 2. This is a classic approach known
as \sphinxhref{https://en.wikipedia.org/wiki/Huffman\_coding}{\sphinxstyleemphasis{Huffman} coding}. It
is very simple to implement, which makes it an attractive choice when
probabilities are negative powers of 2. However, when the probabilities
of the symbols are arbitrary, then Huffman coding is no longer
applicable without approximations of probabilities, which make the
coding suboptimal.

\sphinxAtStartPar
An illustrative encoding of a set of numbers, with 1.5 bits per symbol.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
number
&\sphinxstyletheadfamily 
\sphinxAtStartPar
symbol
&\sphinxstyletheadfamily 
\sphinxAtStartPar
probability \(P_{k}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
bit\sphinxhyphen{}string
&\sphinxstyletheadfamily 
\sphinxAtStartPar
length \(L_{k}\)
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
a
&
\sphinxAtStartPar
0.25
&
\sphinxAtStartPar
00
&
\sphinxAtStartPar
2
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
b
&
\sphinxAtStartPar
0.5
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
+1
&
\sphinxAtStartPar
c
&
\sphinxAtStartPar
0.25
&
\sphinxAtStartPar
01
&
\sphinxAtStartPar
2
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsubsection{Arithmetic coding}
\label{\detokenize{Transmission/Entropy_coding:arithmetic-coding}}
\sphinxAtStartPar
To further improve on coding efficiency, we can combine vector coding
with Huffman coding in a method known as \sphinxhref{https://en.wikipedia.org/wiki/Arithmetic\_coding}{\sphinxstyleemphasis{arithmetic
coding}}. It uses the
probability of symbols to jointly encode a sequence symbols. For
example, consider the set of symbols 0….5 on the right with
corresponding occurrence probabilities \(P_{k}\). Further suppose
that we are supposed to encode the string “130”. The first step is to
assign every symbol to a unique segment  \( [s_k,\,s_{k+1}] \) of
the interval \( [0,\,1] \) such that the width of the segment
matches the probability of the symbol \( P_k = s_{k+1}-s_k \) .

\sphinxAtStartPar
The first symbol is “1” whereby we are assigned to the interval 0.40 …
0.67, which we will call the current interval. The central idea of
arithmetic coding is that next symbol is encoded inside the current
interval. That is, we shift and scale the \(s_{k}\)’s such that
they perfectly fit within the current interval 0.40 … 0.67. In
mathematical terms, if the current symbol is \(h\), then the current
interval is \(s_{h} ... s_{h+1}\), and the intervals of
the next symbol are shifted and scaled as
\begin{equation*}
\begin{split} s'_k = s_h + s_k(s_{h+1}-s_h) = s_h + s_kP_k. \end{split}
\end{equation*}
\sphinxAtStartPar
The second symbol was “3”, such that the current interval is 0.6133 …
0.6349. For the third symbol, the intervals are then shifted and scaled
as
\begin{equation*}
\begin{split} s''_k = s'_3 + s'_k(s_4-s_3) = 0.6133 + s'_k\times 0.08\times
0.27. \end{split}
\end{equation*}
\sphinxAtStartPar
The new intervals are listed on the right. The third symbol is “0” such
that the last current interval is 0.6133 … 0.6219, which we will
denote as \( s_{left} ... s_{right} \) .

\sphinxAtStartPar
The remaining step is to translate the last interval into a string of
bits. Let us divide the whole interval 0 … 1 into \(2^{B}\)
quantization levels on a uniform grid. such that the \(k\)th level is \(
k 2^{-B}. \) We then find the largest \(B\) such that there is a \(k\) with
which \(k2^{-B}\) is inside the last current segment \( s_{left}
... s_{right} \) that is, \( s_{left} \leq k2^{-B} \leq s_{right}
\) . Then \(k\) is the index to our quantization position, that is, it
uniquely describes the interval and thus uniquely describes the sequence
of symbols “130”. Specifically, with \(B=7\), we find that \(k=79\),
fulfills the criteria
\begin{equation*}
\begin{split} s_{left}=0.6133 \leq k2^{-B} = 0.6172 \leq s_{right} = 0.6219.
\end{split}
\end{equation*}
\sphinxAtStartPar
Decoding the sequence is then straightforward at the decoder.

\sphinxAtStartPar
A few additional points:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The average bit\sphinxhyphen{}rate is \( -\sum_k P_k \log_2 P_k \approx 2.2
\) bits per sample. In the example above we needed B=7 bits to
encode 3 samples, which gives 2.3 bits per sample. The actual number
of bits thus does not perfectly coincide with the average bit\sphinxhyphen{}rate.

\item {} 
\sphinxAtStartPar
In \DUrole{xref,myst}{a practical
implementation} of a decoder,
we need to either know the number of symbols or bits, transmit the
number of symbols or bits separately, or we need to use a special
symbol which signifies end\sphinxhyphen{}of\sphinxhyphen{}string.

\item {} 
\sphinxAtStartPar
Usually the last current segment does not exactly align with
\(k2^{-B}\) , which means that there are small unused spaces
in between the bitstring and the last current segment. This is an
inherent inefficiency of arithmetic coding. Heuristically it is easy
to understand that we must send an integer number of bits, but the
data might not be exactly an integer number of bits. The loss is
always less than 1 bit for the whole string, which is acceptable
when we send a large amount of symbols in a string.

\item {} 
\sphinxAtStartPar
Direct \DUrole{xref,myst}{implementation} of
the above description would be cumbersome since the intervals
rapidly become smaller than what can be expressed by discrete
arithmetic (fixed or floating point). Usually therefore algorithms
are designed to use an intermediate interval, from which we output
bits once they become known. For example, in the above example,
after the first symbol we already know that the interval is above
0.5, such that the first bit has to be 1, corresponding to the
interval 0.5 … 0.1.

\item {} 
\sphinxAtStartPar
The specific \DUrole{xref,myst}{implementation}
is rather involved and sensitive to errors.

\item {} 
\sphinxAtStartPar
Algorithmic complexity of an arithmetic coder is usually reasonable,
provided that the probabilities \(P_{k}\) are readily
available. If the probabilities need to be calculated online
(parametric probability model), then complexity increases
considerably.

\end{itemize}

\sphinxAtStartPar
Illustrative set of symbols and their corresponding probabilities.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
symbol \(k\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
probability \(P_k\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
interval \(s_k \dots s_{k+1}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
bits per symbol \(\log_2(P_k)\)
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0.40
&
\sphinxAtStartPar
0.00 … 0.40
&
\sphinxAtStartPar
1.32
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0.27
&
\sphinxAtStartPar
0.40 … 0.67
&
\sphinxAtStartPar
1.89
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
0.12
&
\sphinxAtStartPar
0.67 … 0.79
&
\sphinxAtStartPar
3.05
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
0.08
&
\sphinxAtStartPar
0.79 … 0.87
&
\sphinxAtStartPar
3.64
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
0.07
&
\sphinxAtStartPar
0.87 … 0.94
&
\sphinxAtStartPar
3.84
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
0.06
&
\sphinxAtStartPar
0.94 … 1.00
&
\sphinxAtStartPar
4.06
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The intervals of the second symbol.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
symbol \(k\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
interval \(s_k' \dots s_{k+1}'\)
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0.4000 … 0.5080
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0.5080 … 0.5809
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
0.5809 … 0.6133
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
0.6133 … 0.6349
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
0.6349 … 0.6538
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
0.6538 … 0.6700
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The intervals of the third symbol.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
symbol \(k\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
interval \(s_k'' \dots s_{k+1}''\)
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0.6133 … 0.6219
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0.6219 … 0.6278
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
0.6278 … 0.6304
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
0.6404 … 0.6321
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
0.6321 … 0.6336
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
0.6336 … 0.6439
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsubsection{Parametric coding}
\label{\detokenize{Transmission/Entropy_coding:parametric-coding}}
\sphinxAtStartPar
In the examples above, we used a table to list the probabilities of each
symbol. That leads to a rigid system, which cannot adapt to changes in
the signal. If we want to, for example, use a perceptual model to choose
the quantization accuracy of different samples, we need the ability to
adapt quantization bin widths and consequently, to adapt the
probabilities of each quantization bin. A simple approach is to model
the probability distribution of the signal and calculate probabilities
of each symbol on\sphinxhyphen{}line. We thus use a parametric model of the
probability distribution and correspondingly, we call a coding with
parametric models a \sphinxstyleemphasis{parametric coding}.

\sphinxAtStartPar
In speech coding, parametric coding is typically used in
frequency\sphinxhyphen{}domain coding to encode individual spectral components. We can
then assume that spectral components follow a Laplacian distribution and
derive the probabilities \(P_{k}\) using that distribution. More
refined alternatives include for example \DUrole{xref,myst}{Gaussian mixture models
(GMMs).}


\subsubsection{Algebraic coding}
\label{\detokenize{Transmission/Entropy_coding:algebraic-coding}}
\sphinxAtStartPar
Suppose we would like to encode a string like “00010000”, where “0”s
happen with a high likelihood and there is only a single “1”. We could
then use arithmetic and parametric coding to encode the probabilities of
0’s and 1’s to develop an output string. It quickly becomes awfully
complex, when it would be much simpler to just encode the position of
the single “1”. In this case, if the first position is 0, then the
single “1” is at position 3. There are a total of 8 positions, such that
we need 3 bits to encode the position. Very simple.

\sphinxAtStartPar
This approach can be readily extended to encompass for example both +1
and \sphinxhyphen{}1’s. Just encode the sign with a single extra bit. There exists
straightforward algorithms to include multiple non\sphinxhyphen{}zero elements as
well, as long as the number of non\sphinxhyphen{}zeros is small.

\sphinxAtStartPar
Such encoding algorithms are known as \sphinxstyleemphasis{algebraic coding}, where we use
an algebraic rule or explicit algorithms to encode strings. This is one
type of vector coding, since it encodes jointly a string of symbols.
Usually algebraic coding is fixed bitrate coding, since the number of
bits is decided in advance.

\sphinxAtStartPar
Algebraic coding works efficiently when there are a low number of
non\sphinxhyphen{}zeros and the number of quantization levels is very low.
Unfortunately these methods become increasingly complicated when higher
accuracy is required. Moreover, for higher accuracy quantization, it
becomes increasingly difficult to find the best quantization of a given
vector \(x\). Still, due to its simplicity and efficiency at low bitrates,
algebraic coding is so popular in speech coding that the most commonly
used codec type is known as \DUrole{xref,myst}{Algebraic code\sphinxhyphen{}excited linear prediction
(ACELP)}, since it uses algebraic
coding to encode the residual signal.

\sphinxstepscope


\subsection{Perceptual modelling in speech and audio coding}
\label{\detokenize{Transmission/Perceptual_modelling_in_speech_and_audio_coding:perceptual-modelling-in-speech-and-audio-coding}}\label{\detokenize{Transmission/Perceptual_modelling_in_speech_and_audio_coding::doc}}
\sphinxAtStartPar
Humans are usually the intended recipients of speech signals in
telecommunication, such that the quality of a transmission should be
measured in terms of how good a human listener would judge its quality.
\sphinxstyleemphasis{Perceptual models} refer to methods which try to approximate or predict
the judgement of auditory quality perceived by human listeners. In
coding applications we can thus define perceptual models as \sphinxstyleemphasis{evaluation
models}, with which we approximate the perceptual effect of distortions.

\sphinxAtStartPar
An another type of models which are frequently used in speech coding are
\sphinxstyleemphasis{source models}, which describe the inherent characteristics of the
source, which is the speech signal. You can think of a source model as
for example 1) physical models, which describe the physiological
processes which cause speech sounds or 2) the probability distribution
of speech signals. The important distinction is that source models do
not care about who is observing, but they only describe the objective
reality. In contrast, perceptual models are applied when \sphinxstyleemphasis{we
subjectively observe} the signal, to evaluate properties of the signal.

\sphinxAtStartPar
In speech and audio coding applications, practically all distortions
caused by the algorithms are due to quantization of the signal. The
objective of perceptual modelling is then to choose the quantization
accuracy such that the perceptually degrading effect of quantization is
minimized. Roughly speaking, this means that those signal components
which are more important to a human listener are quantized with a higher
accuracy than those which are less important.


\subsubsection{Frequency masking}
\label{\detokenize{Transmission/Perceptual_modelling_in_speech_and_audio_coding:frequency-masking}}
\sphinxAtStartPar
If we play two sinusoid with slightly different frequencies, then the
louder of the two can \sphinxstyleemphasis{mask} the second sinusoid such that it becomes
inaudible. This effect is known as \sphinxstyleemphasis{frequency masking}. In other words,
people are less sensitive to sounds which are near in frequency to other
sounds. In particular, when quantizing a signal, we can use a lower
quantization accuracy in frequency\sphinxhyphen{}regions which have more energy. The
effect is reduced the further away we are in frequency.

\sphinxAtStartPar
In practice, frequency masking models are similar to spectral (energy)
envelopes. That is, the shape of the frequency masking model is similar
to the spectral envelope, but a smoothed and less pronounced version
thereof. More accurate versions of the model can be generated based on
\sphinxhref{https://en.wikipedia.org/wiki/Psychoacoustics}{psychoacoustic} theory.

\sphinxAtStartPar
Frequency masking models are used in two ways:
\begin{itemize}
\item {} 
\sphinxAtStartPar
In frequency\sphinxhyphen{}domain codecs, where a frequency\sphinxhyphen{}domain representation
of the signal is quantized, we choose the quantization accuracy in
different regions of the spectrum based on a perceptual model.
Typically high\sphinxhyphen{}energy regions are quantized with less accuracy than
low\sphinxhyphen{}energy regions.

\item {} 
\sphinxAtStartPar
In time\sphinxhyphen{}domain codecs such as CELP, we typically use a
analysis\sphinxhyphen{}by\sphinxhyphen{}synthesis loop, where different quantized versions are
synthesized and the error between original and quantized signal is
determined with perceptual weighting. The weighting is here based on
a frequency masking model. Out of the different possible
quantizations, the one with the smallest perceptually weighted error
is chosen.

\end{itemize}


\subsubsection{Frequency scale}
\label{\detokenize{Transmission/Perceptual_modelling_in_speech_and_audio_coding:frequency-scale}}
\sphinxAtStartPar
The sensitivity of human hearing depends on the frequency range where
the sound is present. We are more sensitive in the “low” regions and
less sensitive at “high” frequencies. There is however some ambiguity in
how sensitivity is defined, and we have two prominent different
interpretations:
\begin{itemize}
\item {} 
\sphinxAtStartPar
In the cochlea of the human ear, sounds are processed in spectral
bands, which are independent such that sounds in separate bands do
not interfere with each other, but sounds within the same band \sphinxstyleemphasis{do
interfere} with the perception of each other. This is known as
auditory masking. The width of these bands is frequency dependent
and increases with increasing frequency. This aspect of perception
has been approximated with several models, including the
\sphinxhref{https://en.wikipedia.org/wiki/Bark\_scale}{Bark} and
\sphinxhref{https://en.wikipedia.org/wiki/Equivalent\_rectangular\_bandwidth}{ERB}
scales.

\item {} 
\sphinxAtStartPar
The distance between pitches are perceived differently depending on
their frequencies. In short, a perceptually small step in pitch
(measured in frequencies) is much larger at higher frequencies than
at low frequencies. This aspect of perception can be approximated
with the \sphinxhref{https://en.wikipedia.org/wiki/Mel\_scale}{Mel scale}.

\end{itemize}


\subsubsection{Temporal masking}
\label{\detokenize{Transmission/Perceptual_modelling_in_speech_and_audio_coding:temporal-masking}}
\sphinxAtStartPar
The variation in accuracy and sensitivity of perception is interesting
also across time. In particular, a loud sound can make imperceptible a
second, weaker sound which comes later in time. Say, if we have two
impulses, consecutive in time, and such that the second one is weaker,
and their distance in time is sufficiently short, then we cannot hear
the second impulse. Surprisingly, such temporal masking can occur also
the other way around, a \sphinxstyleemphasis{later} loud sound can mask a preceding weaker
sound.

\sphinxstepscope


\section{Code\sphinxhyphen{}excited linear prediction (CELP)}
\label{\detokenize{Transmission/Code-excited_linear_prediction_CELP:code-excited-linear-prediction-celp}}\label{\detokenize{Transmission/Code-excited_linear_prediction_CELP::doc}}
\sphinxAtStartPar
The most famous speech coding paradigm is code\sphinxhyphen{}excited linear prediction
(CELP). It was first invented in 1985 and is the basis of all
main\sphinxhyphen{}stream codecs dedicated to speech. Its most prominent variant is
the algebraic CELP, which uses an algebraic codebook to encode the noise
residual. Codecs such as
\sphinxhref{https://en.wikipedia.org/wiki/Adaptive\_Multi-Rate\_audio\_codec}{AMR},
\sphinxhref{https://en.wikipedia.org/wiki/Enhanced\_Voice\_Services}{EVS},
\sphinxhref{https://en.wikipedia.org/wiki/G.718}{G.718} and
\sphinxhref{https://en.wikipedia.org/wiki/Speex}{Speex} (superseded by
\sphinxhref{https://en.wikipedia.org/wiki/Opus\_(audio\_format)}{Opus}) are all based
on variants of CELP.

\sphinxAtStartPar
As an overview, CELP is based on a source\sphinxhyphen{}filter model of speech, where
\DUrole{xref,myst}{linear prediction} is used to model the filtering
effect of the vocal tract (and other effects) and this filter is excited
by the speech source, viz. the glottal source and turbulent noise.
Typically, the \DUrole{xref,myst}{pitch or fundamental frequency
model} is a long\sphinxhyphen{}term prediction (LTP)
filter, which is just a linear predictor with a long delay. To model
noise, CELP codecs usually use a \DUrole{xref,myst}{vector
codebook}. The codebook contribution is often
optimized with analysis\sphinxhyphen{}by\sphinxhyphen{}synthesis, where the output of different
quantizations are synthesised and the synthesised outputs are evaluated
to choose the best quantization. The evaluation uses {\hyperref[\detokenize{Transmission/Perceptual_modelling_in_speech_and_audio_coding::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{perceptual
weighting}}}} such that
the subjective, perceptual quality can be compared.

\sphinxAtStartPar
Since \DUrole{xref,myst}{linear prediction} and \DUrole{xref,myst}{fundamental frequency
modelling} are described in detail elsewhere,
below we will discuss only overall encoder/decoder structure, perceptual
evaluation, noise modelling and analysis\sphinxhyphen{}by\sphinxhyphen{}synthesis.

\sphinxAtStartPar
\sphinxincludegraphics{{175511854}.png}
The source\sphinxhyphen{}filter model of CELP codecs.


\subsection{Encoder/decoder structure}
\label{\detokenize{Transmission/Code-excited_linear_prediction_CELP:encoder-decoder-structure}}
\sphinxAtStartPar
The decoder (see image on the right) very closely implements the idea of
the source\sphinxhyphen{}filter model (see above). The only refinement are two
multiplications with scalar gains, where the noise codebook and pitch
contribution are scaled to the desired magnitude. Observe that here we
abbreviate linear predictive coding with LPC.

\sphinxAtStartPar
The encoder and decoder typically operate in frames of 20 ms length,
which are further subdivided into 5 ms subframes. Operations described
above are thus performed on vectors whose length correspond to 5 ms,
which at a sampling\sphinxhyphen{}rate of 12.8 kHz corresponds to 64 samples.

\sphinxAtStartPar
The encoder (see figure on the right) first estimate the linear
predictive (LPC) model, then removes its effect from the input. In other
words, since linear prediction is IIR\sphinxhyphen{}filtering, we can remove the
effect from the speech signal with the corresponding FIR\sphinxhyphen{}filter to
obtain the LPC\sphinxhyphen{}residual. We can then similarly estimate the fundamental
frequency (F0) from the residual and again remove its effect to obtain
the F0\sphinxhyphen{}residual.

\sphinxAtStartPar
The F0\sphinxhyphen{}residual closely resembles white noise (following the Laplacian
distribution). We can thus quantize it with a noise\sphinxhyphen{}quantizer (described
below) as well as the pitch and noise gains. To evaluate the output
quality of the signal, we then decode the quantized signal and calculate
a perceptually weighted error. Since LPC\sphinxhyphen{}filtering is autoregressive
(IIR), it however has a non\sphinxhyphen{}linear effect on the output such that
quantization has a non\sphinxhyphen{}linear effect on the output. We therefore cannot
know which quantization is the best one without trying out \sphinxstyleemphasis{all of
them}. To get best possible performance, in theory, we should try every
possible quantization! However, in practice, we choose a group of
potentially\sphinxhyphen{}good quantization and find the best out of them. This is
known as the \sphinxstyleemphasis{analysis\sphinxhyphen{}by\sphinxhyphen{}synthesis} method.

\sphinxAtStartPar
Analysis\sphinxhyphen{}by\sphinxhyphen{}synthesis is a celebrated method because it enables
optimization of CELP and achieves relatively high quality. Since CELP is
arguably more efficient than competing frequency\sphinxhyphen{}domain codecs, and
analysis\sphinxhyphen{}by\sphinxhyphen{}synthesis enables optimization of CELPs, it is important.
However, observe that this is a brute\sphinxhyphen{}force method, which has an
inherent penalty in computational complexity.

\sphinxAtStartPar
\sphinxincludegraphics{{175511877}.png}

\sphinxAtStartPar
CELP decoder structure

\sphinxAtStartPar
\sphinxincludegraphics{{175511879}.png}
CELP encoder structure


\subsection{Perceptual quality evaluation}
\label{\detokenize{Transmission/Code-excited_linear_prediction_CELP:perceptual-quality-evaluation}}
\sphinxAtStartPar
Perceptual quality in CELP codecs is evaluated with a weighted norm.
Suppose \(W\) is a convolution matrix corresponding to the perceptual
weighting filter, then the weighted norm between the true and quantized
residual signals \(x\) and \( \hat x \) , respectively, is
\begin{equation*}
\begin{split} d_W(x,\hat x):=\left\| W(x-\hat x)\right\|^2 = (x-\hat
x)^T W^T W (x-\hat x). \end{split}
\end{equation*}
\sphinxAtStartPar
Though this is a quadratic form, whose minimization is simple, notice
that we consider quantized vectors, such that the minimization is an
integer\sphinxhyphen{}valued minimization problem, which does not have an analytic
solution.

\sphinxAtStartPar
Further, the quantized signal is the sum of noise and pitch
contributions, both multiplied with scaling factors
\begin{equation*}
\begin{split} \hat x := \gamma_{F0} x_{F0} + \gamma_{noise} x_{noise}.
\end{split}
\end{equation*}
\sphinxAtStartPar
When estimating the F0, we can set the noise contribution to zero, such
that we minimize
\begin{equation*}
\begin{split} \arg\min_{x_{F0}}\, d_W(x,\gamma_{F0}x_{F0}):=
\arg\min_{x_{F0}}(x-\gamma_{F0}x_{F0})^T W^T W
(x-\gamma_{F0}x_{F0}). \end{split}
\end{equation*}
\sphinxAtStartPar
To compare different pitch contributions, we further need to exclude the
gain from the problem, which is achieved by setting the derivative with
respect to \( \gamma_{F0} \) to zero (left as an exercises), which
gives the optimal gain as
\begin{equation*}
\begin{split} \gamma_{F0}^* = \frac{x^TW^T W x_{F0}}{x_{F0}^TW^T W
x_{F0}}. \end{split}
\end{equation*}
\sphinxAtStartPar
Substituting back to the original problem, after removing constants,
yields
\begin{equation*}
\begin{split} x_{F0}^*:=\arg\min_{x_{F0}}\,
d_W(x,\gamma_{F0}^*x_{F0}):= \arg\max_{x_{F0}}
\frac{\left(x^TW^T W x_{F0}\right)^2}{x_{F0}^TW^T W x_{F0}}. \end{split}
\end{equation*}
\sphinxAtStartPar
Observe that this equation thus evaluates the weighted correlation
between the original signal \(x\) and the pitch contribution. In other
words, different F0’s can be evaluated with this function and the one
with the highest correlation is chosen as the F0.

\sphinxAtStartPar
Once the F0 has been chosen, we calculate the optimal gain and subtract
it from the original residual signal, \( x':=x - \gamma_{F0}^*
x_{F0}^*. \) This F0\sphinxhyphen{}residual is then approximately white noise and
can be modelled with the noise codebook. Similarly as above, we assume
that the noise\sphinxhyphen{}gain is optimal such that the noise codebook can be
optimized with
\begin{equation*}
\begin{split} \gamma_{noise}^* = \frac{x^TW^T W x_{noise}}{x_{noise}^TW^T W
x_{noise}} \end{split}
\end{equation*}
\sphinxAtStartPar
and
\begin{equation*}
\begin{split} x_{noise}^*:=\arg\min_{x_{noise}}\,
d_W(x',\gamma_{noise}^*x_{noise}):= \arg\max_{x_{noise}}
\frac{\left(x^TW^T W x_{noise}\right)^2}{x_{noise}^TW^T W
x_{noise}}. \end{split}
\end{equation*}
\sphinxAtStartPar
We have thus quantized the pitch and noise contributions, but for the
two gains we have optimal values, but not optimal \sphinxstyleemphasis{quantized} values.
Again, since quantized values are not continuous, we do not have an
analytic solution but must search for the best quantization among all
possible values. The optimization problem is
\begin{equation*}
\begin{split} \arg\min_{\gamma_{F0},\gamma_{noise}}\,
d_W(x,\gamma_{F0}x_{F0}^* + \gamma_{noise}x_{noise}^*) =
\arg\min_{\gamma_{F0},\gamma_{noise}}\,(x-\gamma_{F0}x_{F0}^* -
\gamma_{noise}x_{noise}^*)^T W^T W (x-\gamma_{F0}x_{F0}^* -
\gamma_{noise}x_{noise}^*). \end{split}
\end{equation*}
\sphinxAtStartPar
We note that the above equation is a polynomial of the two scalar gains
and all vector and matrix terms reduce to constants, such that
\begin{equation*}
\begin{split} \arg\min_{\gamma_{F0},\gamma_{noise}}\,
d_W(x,\gamma_{F0}x_{F0}^* + \gamma_{noise}x_{noise}^*) = c_0 +
\gamma_{F0}c_1 + \gamma_{F0}^2c_2 +\gamma_{noise}c_3 +
\gamma_{noise}\gamma_{F0}c_4 + \gamma_{noise}^2c_5. \end{split}
\end{equation*}
\sphinxAtStartPar
In difference to the optimization of the residual vectors, this
optimization is computationally relatively simple such that we can
exhaustively search for the best gains. The gains are usually quantized
with 8 to 10 bits, such that this involves only 256 to 1024 polynomial
evaluations.The final quantized residual is then
\begin{equation*}
\begin{split} \hat x^* = \gamma_{F0}^* x_{F0}^* + \gamma_{noise}^*
x_{noise}^*. \end{split}
\end{equation*}

\subsection{Noise modelling and algebraic coding}
\label{\detokenize{Transmission/Code-excited_linear_prediction_CELP:noise-modelling-and-algebraic-coding}}
\sphinxAtStartPar
As mentioned above, the residual after LPC filtering and F0 modelling is
approximately stationary white noise, that is, it is constant variance
and samples are uncorrelated. We would like to quantize this
effectively. White noise signals have however no structure left, except
their probability distribution. We can assume that the residual samples
\( \xi_k \) follow the Laplacian distribution with zero mean,
\begin{equation*}
\begin{split} f(\xi_k)= C \exp\left(-\frac{\|\xi_k\|}{s}\right). \end{split}
\end{equation*}
\sphinxAtStartPar
The joint log\sphinxhyphen{}likelihood is
\begin{equation*}
\begin{split} \log\prod_k f(\xi_k)= C'- \sum_k\frac{\|\xi_k\|}{s} = C' -
\frac1s \|x_{noise}\|_1, \end{split}
\end{equation*}
\sphinxAtStartPar
where \( x_{noise}:= [\xi_1,\dotsc,\,\xi_{K}]. \) and \(
\|x\|_1 \) is the 1\sphinxhyphen{}norm (absolute sum). In other words, if we
model constant\sphinxhyphen{}probability vectors \( x_{noise}, \)  then that is
equivalent with modelling vectors with a constant 1\sphinxhyphen{}norm, \(
\|x_{noise}\|_1=\text{constant}. \) We can thus build a codebook
which has constant 1\sphinxhyphen{}norm. For example, if we quantize to integer
values, then the absolute sum of the quantized signal is a fixed
integer.

\sphinxAtStartPar
In the simplest case, we can quantize \( x_{noise} \) to have one
signed pulse at location \(k\), and otherwise all samples are zero. The
location of the pulse can be encoded with \( \log_2 K \) bits, and
the sign with one bit, such that the overall bit\sphinxhyphen{}consumption is \(
1+\log_2 K. \) This encoding strategy can be readily extended by
adding more pulses. The bit\sphinxhyphen{}consumption of multi\sphinxhyphen{}pulse vectors however
becomes more complicated. The issue is that if apply a naive encoding
where we directly encode the position and sign of each pulse, then we
use more bits than necessary for two reasons. Firstly, if two pulses
overlap, then they must have the same sign otherwise they would cancel.
Secondly, pulses are indistinguishable, such that their ordering does
not matter, such that if we encode pulses one by one, changing their
order would give different bit\sphinxhyphen{}streams but the encoded signal would
remain the same. Both imply that we are using too many bits when using
such encodings for multiple pulses. Solutions exist for optimal encoding
of such multi\sphinxhyphen{}pulse vectors, but the algorithm becomes involved.

\sphinxAtStartPar
In any case, the outcome is that it is possible to generate
quantizations of residual signals with algorithmic methods. That is, we
have an algorithm or algebraic rule which defines all possible
quantizations and consequently, such quantization is known as algebraic
coding. The encoding can be chosen to have optimal bit\sphinxhyphen{}consumption for a
given number of pulses and it is thus (with loose assumptions) the best
possible quantization for the residual vector when using a fixed
bitrate. It is computationally efficient since the residual vectors are
mostly zeros, such that evaluation of the optimization function is
straightforward to calculate. It also efficient in the sense that
codebooks do not have to be stored, but can be generated on the fly by
an algorithm.

\sphinxAtStartPar
Algebraic coding is so central to CELP codecs that CELP codecs using
algebraic coding are known as algebraic CELP or ACELP. Most main stream
codecs, such as AMR, EVS and USAC use ACELP. Some codecs use also other
residual codebooks, but even then, algebraic codes are always the first
choice.

\sphinxstepscope


\section{Frequency\sphinxhyphen{}domain coding}
\label{\detokenize{Transmission/Frequency-domain_coding:frequency-domain-coding}}\label{\detokenize{Transmission/Frequency-domain_coding::doc}}
\sphinxAtStartPar
In audio coding, the classical approach is based on coding in the
frequency domain, which means that we are coding a \DUrole{xref,myst}{time\sphinxhyphen{}frequency
representation} of the signal. Such coding
methods are especially suitable for signals which have prolonged
stationary parts, such as many instrument\sphinxhyphen{}sounds, which are often
stationary for the duration of a note, more or less. Frequency\sphinxhyphen{}domain
codecs are based on {\hyperref[\detokenize{Transmission/Entropy_coding::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{entropy coding}}}} where the
quantization accuracy is chosen with a {\hyperref[\detokenize{Transmission/Perceptual_modelling_in_speech_and_audio_coding::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{perceptual
model}}}}.

\sphinxAtStartPar
Classical speech coding is however based on \DUrole{xref,myst}{code\sphinxhyphen{}excited linear
prediction (CELP)}, which is a
fundamentally different paradigm. In modern (mobile) applications, these
two modalities, speech and audio are often intertwined and we would like
to be able to encode all types of sounds. For example, a movie can have
music, speech, speech with music in the background, music with spoken
and sung parts, and there are frequent and seamless transitions between
such modalities. For a unified codec, which can encode both speech,
music, generic audio and their mixtures, we therefore needs codecs which
support also frequency\sphinxhyphen{}domain coding. Besides, stationary parts of
speech such as fricatives can sometimes be more efficiently encoded with
frequency\sphinxhyphen{}domain coding anyway.

\sphinxAtStartPar
Such codecs, which encode both speech, music and generic audio are often
collectively called \sphinxstyleemphasis{speech and audio codecs}. Most recent codecs such
as \sphinxhref{https://en.wikipedia.org/wiki/Unified\_Speech\_and\_Audio\_Coding}{MPEG
USAC},
\sphinxhref{https://en.wikipedia.org/wiki/Enhanced\_Voice\_Services}{3GPP EVS} and
\sphinxhref{https://en.wikipedia.org/wiki/Opus\_\%28audio\_format\%29}{Opus} are speech
and audio codecs. Internally, they contain elements of both CELP and
frequency\sphinxhyphen{}domain coding and they switch between these modes depending on
the content.

\sphinxAtStartPar
More specifically, frequency\sphinxhyphen{}domain codecs are based on a time\sphinxhyphen{}frequency
transform such as the \DUrole{xref,myst}{MDCT},
{\hyperref[\detokenize{Transmission/Perceptual_modelling_in_speech_and_audio_coding::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{perceptual modelling}}}}
to choose the quantization accuracy, and {\hyperref[\detokenize{Transmission/Entropy_coding::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{entropy
coding}}}} to transmit the quantized signal with the least
amount of bits. At the decoder, the process is reversed (as is obvious).
In many designs, the bitrate is variable, such that with the perceptual
model we choose the desired perceptual accuracy, and entropy coder
compresses that as much as possible. Then we always have approximately
the desired quality, but we however do not in advance know how many bits
we will use. In other designs, we strive for fixed bitrate, such that we
should reach the highest quality as long as the bit\sphinxhyphen{}consumption remains
under a chosen limit. With most entropy codecs this means that we have
to implement a \sphinxstyleemphasis{rate\sphinxhyphen{}loop}, where we iteratively search for the best
quantization accuracy which remains within the chosen limit on
bit\sphinxhyphen{}consumption.

\sphinxAtStartPar
\sphinxincludegraphics{{175513474}.png}

\sphinxstepscope


\chapter{Speech enhancement}
\label{\detokenize{Speech_enhancement:speech-enhancement}}\label{\detokenize{Speech_enhancement::doc}}
\sphinxAtStartPar
When using speech technology in real environments, we are often faced
with less than perfect signal quality. For example, if you make a phone
call at cafeteria, typically you have plenty of other people speaking in
the background, there could be music playing and the room itself can
have reverberation. Such effects distort the desired speech signal such
that the receiving end, the desired speech sounds less pleasant,
requires more effort to understand or at the worst case, it becomes less
intelligible. \sphinxstyleemphasis{Speech enhancement} refers to methods which try to reduce
such distortions, to make speech sounds more pleasant, reduce listening
effort and improve intelligibility.

\sphinxAtStartPar
The most prominent categories of speech enhancement are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Enhancement/Noise_attenuation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Noise attenuation}}}}, where we try to extract the
desired speech signalm when distorted by background noise(s).

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Enhancement/Echo_cancellation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Echo cancellation}}}} and feedback cancellation are
used when the sound played from a loudspeaker is picked up by a
microphone distorting the desired signal.

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Enhancement/Bandwidth_extension_BWE::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Bandwidth extensions}}}} refers to methods
which convert a signal at a lower sampling rate to a higher sampling
rate and fills the missing range of the spectrum with some plausible
content.

\item {} 
\sphinxAtStartPar
Dereverberation refers to methods which attenuate the effect of room
acoustics on the desired signal.

\item {} 
\sphinxAtStartPar
Source separation methods try to extract sounds of single sources
from a mixture, for example, in the classical cocktail\sphinxhyphen{}party
problem, we would like to isolate single speakers when multiple
people are talking at the same time.

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Enhancement/Multi-channel_speech_enhancement_and_beamforming::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Beamforming}}}}
refers to spatially selective methods, where the objective is
isolate sounds coming from a particular direction, by using the
information about the spatial separation of a set of microphones.

\end{enumerate}

\sphinxAtStartPar
The objective of speech enhancement however requires a bit more
consideration. In its most classical form, the objective is to extract a
clean speech signal from a distorted mixture, where the distortions can
be background and sensor noises, as well as room reverberation. Here the
clean reference signal is considered to be that signal which would be
rerecorded with a microphone close to the speaker, which does not
contain said noises or reverberation. It is then clear that it will be
challenging to obtain realistic data, since even a microphone close to
the speaker will usually contain background noises and effect of
reverberation. For development of methods, it is therefore often
difficult to obtain data which would accurately correspond to a
realistic situation. In any case, a typical objective would be to
improve the signal to noise ratio (with or without perceptual weighting)
as much as possible.

\sphinxAtStartPar
A more challenging scenario is when two or more persons are speaking in
the same acoustic environment. The second speaker can then be viewed as
a competing speaker (undesired source) or as a discussion partner
(desired source). Even if the two speakers are in an interaction with
each other, then often they will speak on top of each other, even if
stereotypically we think of a dialogue as a non\sphinxhyphen{}overlapping back and
forth exchange of non\sphinxhyphen{}overlapping arguments. If we want to separate
between the two speakers, then overlaps are difficult, because the
statistics of the both speech signals will be rather similar, whereas
noise signals with distinct statistics are easier to attenuate.

\sphinxAtStartPar
Sometimes we do not want to remove all distortions entirely, but just
attenuate their effect. Completely removing artefacts can sometimes make
the signal sound unnatural and besides removing distortions, processing
methods also almost always distorts the desired signal. Therefore, to
retain a natural\sphinxhyphen{}sounding signal and to minimize distortion of the
desired speech signal, we often limit the extent to which distortions
are removed.

\sphinxAtStartPar
A further aspect of enhancement is intelligibility and pleasantness; as
a starting point, observe that the speech of some people is by nature
difficult to understand or otherwise just annoying (unpleasant). It then
conceivable that we device some processing which improves the speech
signal to better than the original. What “sounds better” is however a
difficult concept, since we do not have unambiguous measures for “how
good it sounds” and opinions between listeners will certainly diverge.

\sphinxAtStartPar
Intelligibility with regard to human listeners is similarly complicated
as pleasantness, but luckily, we can use speech recognition engines to
obtain objective measures. That is, if we give noisy and improved speech
signals to a speech recognizer, we can determine the recognition
performance in both cases to estimate the benefit obtained with our
processing.

\sphinxstepscope

\sphinxAtStartPar
\sphinxincludegraphics{{175508067}.png}
Photo by Jezael Melgoza on Unsplash


\section{Noise attenuation}
\label{\detokenize{Enhancement/Noise_attenuation:noise-attenuation}}\label{\detokenize{Enhancement/Noise_attenuation::doc}}
\sphinxAtStartPar
When using speech technology in realistic environments, such as at home,
office or in a car, there will invariably be also other sounds present
and not only the speech sounds of desired speaker. There will be the
background hum of computers and air conditioning, cars honking, other
speakers, and so on. Such sounds reduces the quality of the desired
signal, making it more strenuous to listen, more difficult to understand
or at the worst case, it might render the speech signal unintelligible.
A common feature of these sounds is however that they are \sphinxstyleemphasis{independent}
of and \sphinxstyleemphasis{uncorrelated} with the desired signal. {[}\hyperlink{cite.Enhancement/Noise_attenuation:id41}{Benesty \sphinxstyleemphasis{et al.}, 2008}{]}

\sphinxAtStartPar
That is, we can usually assume that such noises are \sphinxstyleemphasis{additive}, such
that the observed signal \(y\) is the sum of the desired signal \(x\) and
interfering noises \(v\), that is, \(y=x+v\). To improve the quality of the
observed signal, we would like to make an estimate \( \hat x \) of
the desired signal \(x\). The estimate should approximate the desired
signal \( x\approx \hat x \) or conversely, we would like to
minimize the distance \( d\left(x,\hat x\right) \) with some
distance measure \(d(\cdot,\cdot)\).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Noise_attenuation_1_1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Noise gate}
\label{\detokenize{Enhancement/Noise_attenuation:noise-gate}}
\sphinxAtStartPar
Suppose you are talking in a reasonably quiet environement. For example, typically when you speak on a phone, you would go to a quiet room. Similarly, when attending an on\sphinxhyphen{}line lecture, you would most often want to be in a room without background noise.

\sphinxAtStartPar
What we perceive as quiet is however never entirely silent. When we play a sound recorded in a “quiet” room, then in the reproduction you then hear the \sphinxstyleemphasis{local} and the \sphinxstyleemphasis{recorded} background noises. Assuming the two noises have similar energies, then their sum has twice the energy, viz. 6dB higher than the original noises. In a teleconference with multiple participants, the background noises add up such that each contributes with an 6dB increase in the background noise level. You do not need many participants before the total noise level becomes so high that communication is impossible.

\sphinxAtStartPar
The \sphinxstylestrong{mute}\sphinxhyphen{}button in teleconferences is therefore essential. Participants can silence their microphones whenever they are not speaking, such that only the background noise of the \sphinxstyleemphasis{active} speaker(s) is transmitted to all listeners.

\sphinxAtStartPar
While the mute\sphinxhyphen{}button is a good user\sphinxhyphen{}interface in the sense that it gives control to the user, it is however an annoying user\sphinxhyphen{}interface in that users tend to forget to mute and unmute themselves. Would be better with an automatic mute.

\sphinxAtStartPar
\sphinxstylestrong{Noise gating} is a simple auto\sphinxhyphen{}mute in the sense that it thresholds signal energy and turns reproduction/transmission off if energy is too low. Typically it also features a hysterisis functionality such that reproduction/transmission is kept off for a while after the last speech segment. Moreover, to avoid discontinuities, there is a fade\sphinxhyphen{}in/fade\sphinxhyphen{}out functionality at the start and end.

\sphinxAtStartPar
Note that noise gating with a energy threshold is a simple implementation of a \sphinxstyleemphasis{voice activity detector (VAD)}. With more advanced features than mere energy we can refine voice activity detects quite a bit, to make them more robust especially in noisy and reverberant environments. In addition to enhancement of signal quality, such methods are often used also to preserve resources, such as transmission bandwidth (telecommunication) and computation costs (recognition applications such as speech recognition).

\sphinxAtStartPar
For the noise\sphinxhyphen{}gate, we first need to choose a threshold. Typically that the threshold is chosen relative to the mean (log) energy \(\sigma^2\) such that the threshold is \(x^2 < \sigma^2\gamma\), where \(\gamma\) is a tunable parameter. Moreover, we can implement the gate such that if we are below the threshold, we set a gain value to 0 and otherwise to 1. If we want fade\sphinxhyphen{}in/fade\sphinxhyphen{}out, we can ramp that gain value smoothly from 0 to 1 at the attack and from 1 to 0 at the release.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{frame\PYGZus{}energy} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{frame\PYGZus{}energy\PYGZus{}dB} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log10}\PYG{p}{(}\PYG{n}{frame\PYGZus{}energy}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}energy\PYGZus{}dB} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{frame\PYGZus{}energy\PYGZus{}dB}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} mean of energy in dB}

\PYG{n}{threshold\PYGZus{}dB} \PYG{o}{=} \PYG{n}{mean\PYGZus{}energy\PYGZus{}dB} \PYG{o}{+} \PYG{l+m+mf}{3.} \PYG{c+c1}{\PYGZsh{} threshold relative to mean}

\PYG{n}{speech\PYGZus{}active} \PYG{o}{=} \PYG{n}{frame\PYGZus{}energy\PYGZus{}dB} \PYG{o}{\PYGZgt{}} \PYG{n}{threshold\PYGZus{}dB}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Noise_attenuation_4_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
This is quite awful isn’t it? Though we did loose many stationary segments of noise, we also distorted the speech signal significantly. In particular, typically we loose plosives at the beginning of words. Overall the sound also sounds odd when it turns on an off again.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{hysteresis\PYGZus{}time\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{300}
\PYG{n}{hysteresis\PYGZus{}time} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{hysteresis\PYGZus{}time\PYGZus{}ms}\PYG{o}{/}\PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{p}{)}

\PYG{n}{speech\PYGZus{}active\PYGZus{}hysteresis} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{window\PYGZus{}count}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{window\PYGZus{}ix} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{window\PYGZus{}count}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{range\PYGZus{}start} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{window\PYGZus{}ix}\PYG{o}{\PYGZhy{}}\PYG{n}{hysteresis\PYGZus{}time}\PYG{p}{)}
    \PYG{n}{speech\PYGZus{}active\PYGZus{}hysteresis}\PYG{p}{[}\PYG{n}{window\PYGZus{}ix}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{speech\PYGZus{}active}\PYG{p}{[}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{range\PYGZus{}start}\PYG{p}{,}\PYG{n}{window\PYGZus{}ix}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Noise_attenuation_7_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
This sounds quite a bit better already. There are only some sudden muted areas (depending on your sound sample), but overall the sound is clearly better.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Fade\PYGZhy{}in and fade\PYGZhy{}out}
\PYG{n}{fade\PYGZus{}in\PYGZus{}time\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{50}
\PYG{n}{fade\PYGZus{}out\PYGZus{}time\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{300}
\PYG{n}{fade\PYGZus{}in\PYGZus{}time} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{fade\PYGZus{}in\PYGZus{}time\PYGZus{}ms}\PYG{o}{/}\PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{p}{)}
\PYG{n}{fade\PYGZus{}out\PYGZus{}time} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{fade\PYGZus{}out\PYGZus{}time\PYGZus{}ms}\PYG{o}{/}\PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{p}{)}

\PYG{n}{speech\PYGZus{}active\PYGZus{}sloped} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n}{window\PYGZus{}count}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{frame\PYGZus{}ix} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{window\PYGZus{}count}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{speech\PYGZus{}active\PYGZus{}hysteresis}\PYG{p}{[}\PYG{n}{frame\PYGZus{}ix}\PYG{p}{]}\PYG{p}{:}
        \PYG{n}{range\PYGZus{}start} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{frame\PYGZus{}ix}\PYG{o}{\PYGZhy{}}\PYG{n}{fade\PYGZus{}in\PYGZus{}time}\PYG{p}{)}
        \PYG{n}{speech\PYGZus{}active\PYGZus{}sloped}\PYG{p}{[}\PYG{n}{frame\PYGZus{}ix}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{speech\PYGZus{}active\PYGZus{}hysteresis}\PYG{p}{[}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{range\PYGZus{}start}\PYG{p}{,}\PYG{n}{frame\PYGZus{}ix}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{range\PYGZus{}start} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{frame\PYGZus{}ix}\PYG{o}{\PYGZhy{}}\PYG{n}{fade\PYGZus{}out\PYGZus{}time}\PYG{p}{)}
        \PYG{n}{speech\PYGZus{}active\PYGZus{}sloped}\PYG{p}{[}\PYG{n}{frame\PYGZus{}ix}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{speech\PYGZus{}active\PYGZus{}hysteresis}\PYG{p}{[}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{range\PYGZus{}start}\PYG{p}{,}\PYG{n}{frame\PYGZus{}ix}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Noise_attenuation_10_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
This doesn’t sound all too bad! The sudden on\sphinxhyphen{} and off\sphinxhyphen{}sets are gone and the transitions to muted areas sound reasonably natural.

\sphinxAtStartPar
Now we have implemented gating for the full\sphinxhyphen{}band signal. Gating can be easily improved by band\sphinxhyphen{}wise \sphinxhyphen{}processing. Depending on the amount of processing you can afford, you could go all the way and applying gating on individual frequency\sphinxhyphen{}bins in the STFT.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{hysteresis\PYGZus{}time\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{hysteresis\PYGZus{}time} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{hysteresis\PYGZus{}time\PYGZus{}ms}\PYG{o}{/}\PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{p}{)}

\PYG{n}{fade\PYGZus{}in\PYGZus{}time\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{30}
\PYG{n}{fade\PYGZus{}out\PYGZus{}time\PYGZus{}ms} \PYG{o}{=} \PYG{l+m+mi}{60}
\PYG{n}{fade\PYGZus{}in\PYGZus{}time} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{fade\PYGZus{}in\PYGZus{}time\PYGZus{}ms}\PYG{o}{/}\PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{p}{)}
\PYG{n}{fade\PYGZus{}out\PYGZus{}time} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{fade\PYGZus{}out\PYGZus{}time\PYGZus{}ms}\PYG{o}{/}\PYG{n}{window\PYGZus{}step\PYGZus{}ms}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} NB: This is a pedagogic, but very slow implementation since it involves multiple for\PYGZhy{}loops.}
\PYG{n}{spectrogram\PYGZus{}binwise} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{,}\PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{complex}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{bin\PYGZus{}ix} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{fft\PYGZus{}length}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{bin\PYGZus{}energy\PYGZus{}dB} \PYG{o}{=} \PYG{l+m+mf}{10.}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log10}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{bin\PYGZus{}ix}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{mean\PYGZus{}energy\PYGZus{}dB} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{bin\PYGZus{}energy\PYGZus{}dB}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} mean of energy in dB}
    \PYG{n}{threshold\PYGZus{}dB} \PYG{o}{=} \PYG{n}{mean\PYGZus{}energy\PYGZus{}dB} \PYG{o}{+} \PYG{l+m+mf}{16.} \PYG{c+c1}{\PYGZsh{} threshold relative to mean}
    \PYG{n}{speech\PYGZus{}active} \PYG{o}{=} \PYG{n}{bin\PYGZus{}energy\PYGZus{}dB} \PYG{o}{\PYGZgt{}} \PYG{n}{threshold\PYGZus{}dB}
    
    \PYG{n}{speech\PYGZus{}active\PYGZus{}hysteresis} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n}{speech\PYGZus{}active}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{window\PYGZus{}ix} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{window\PYGZus{}count}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{range\PYGZus{}start} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{window\PYGZus{}ix}\PYG{o}{\PYGZhy{}}\PYG{n}{hysteresis\PYGZus{}time}\PYG{p}{)}
        \PYG{n}{speech\PYGZus{}active\PYGZus{}hysteresis}\PYG{p}{[}\PYG{n}{window\PYGZus{}ix}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{speech\PYGZus{}active}\PYG{p}{[}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{range\PYGZus{}start}\PYG{p}{,}\PYG{n}{window\PYGZus{}ix}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
        
    \PYG{c+c1}{\PYGZsh{}speech\PYGZus{}active\PYGZus{}sloped = np.zeros\PYGZus{}like(spe}
    \PYG{k}{for} \PYG{n}{frame\PYGZus{}ix} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{window\PYGZus{}count}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{speech\PYGZus{}active\PYGZus{}hysteresis}\PYG{p}{[}\PYG{n}{frame\PYGZus{}ix}\PYG{p}{]}\PYG{p}{:}
            \PYG{n}{range\PYGZus{}start} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{frame\PYGZus{}ix}\PYG{o}{\PYGZhy{}}\PYG{n}{fade\PYGZus{}in\PYGZus{}time}\PYG{p}{)}
            \PYG{n}{speech\PYGZus{}active\PYGZus{}sloped}\PYG{p}{[}\PYG{n}{frame\PYGZus{}ix}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{speech\PYGZus{}active\PYGZus{}hysteresis}\PYG{p}{[}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{range\PYGZus{}start}\PYG{p}{,}\PYG{n}{frame\PYGZus{}ix}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{range\PYGZus{}start} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{frame\PYGZus{}ix}\PYG{o}{\PYGZhy{}}\PYG{n}{fade\PYGZus{}out\PYGZus{}time}\PYG{p}{)}
            \PYG{n}{speech\PYGZus{}active\PYGZus{}sloped}\PYG{p}{[}\PYG{n}{frame\PYGZus{}ix}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{speech\PYGZus{}active\PYGZus{}hysteresis}\PYG{p}{[}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{range\PYGZus{}start}\PYG{p}{,}\PYG{n}{frame\PYGZus{}ix}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
            
    \PYG{n}{spectrogram\PYGZus{}binwise}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{bin\PYGZus{}ix}\PYG{p}{]} \PYG{o}{=} \PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{bin\PYGZus{}ix}\PYG{p}{]}\PYG{o}{*}\PYG{n}{speech\PYGZus{}active\PYGZus{}sloped}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Noise_attenuation_13_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
This is clearly again a step better, but you should note two things:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The parameters are now quite a bit different; hysteresis and ramps shorter as well as higher threshold. This was required to get acceptable quality.

\item {} 
\sphinxAtStartPar
Some musical noise left in the low and high frequencies, where isolated areas are turned on for a while.

\end{itemize}

\sphinxAtStartPar
If possible, try listening to this sound with headphones and from loudspeakers. Is there a difference? Which one sounds better?

\sphinxAtStartPar
A possible impression is that, with good\sphinxhyphen{}quality headphones, the result is too clean. It sounds unnatural. With loudspeakers, however, it may sound quite ok. When listening to headphones, you better perceive the absence of background noise, whereas on the loudspeaker, there is more background noise present locally, such that the absence of reproduced background noise is not noticeable. With loudspeakers, any reproduced background noise would also interact with the local room, generating a double room reverberation (assuming that the reproduced loudspeaker sound already had reverberation).

\sphinxAtStartPar
In any case, it seems therefore clear that muting the background noise entirely is not always desirable (at least when listening with headphones to a single speaker with background noise). We should therefore apply some more intelligent gain factor (see more in the spectral subtraction section below).


\subsection{Statistical model}
\label{\detokenize{Enhancement/Noise_attenuation:statistical-model}}
\sphinxAtStartPar
The {\hyperref[\detokenize{Representations/Spectrogram_and_the_STFT::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{STFT spectrum}}}} of a signal is a good
domain for noise attenuation because we can reasonably safely assume
that spectral components are uncorrelated with each other, such that we
treat each component separately. In other words, in the spectrum, we can
apply noise attenuation on every frequency bin with scalar operations,
whereas if the components would be correlated, we would have to use
vector and matrix operations. The benefit of scalar operations is that
they are computationally simple, \(O(N)\), whereas matrix operations are
typically at least \(O(N^2)\).

\sphinxAtStartPar
The sources are, according to our assumption, uncorrelated, which
means that the expected correlation is zero,
\begin{equation*}
\begin{split}E[xv] = 0.\end{split}
\end{equation*}
\sphinxAtStartPar
A consequence of this assumption is that, for the mixture \(y = x + v\), we have the energy expectation
\begin{equation*}
\begin{split}E\left[y^2\right] = E\left[x+v^2\right] = E\left[x^2\right] + E\left[v^2\right] + 2E\left[xv\right]
= E\left[x^2\right] + E\left[v^2\right].\end{split}
\end{equation*}
\sphinxAtStartPar
In other words, in terms of expectations, the energy is the sum of component energies, which will become a very practical property.

\sphinxAtStartPar
To find the energy of the speech signal, we then just need to estimate \(E\left[v^2\right]\).


\subsubsection{Noise estimation and modelling}
\label{\detokenize{Enhancement/Noise_attenuation:noise-estimation-and-modelling}}

\paragraph{Mean\sphinxhyphen{}energy with voice activity detection}
\label{\detokenize{Enhancement/Noise_attenuation:mean-energy-with-voice-activity-detection}}
\sphinxAtStartPar
To be able to remove noise, we first need to estimate noise characteristics or statistics. We thus need to find sections of the signal which have noisy only (non\sphinxhyphen{}speech). One approach would be to use {\hyperref[\detokenize{Recognition/Voice_activity_detection::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{voice activity detection}}}} (VAD) to find non\sphinxhyphen{}speech segments. Assuming we have a good VAD this can be effective. It works if we assume that noise is stationary, such that the noise on non\sphinxhyphen{}speech parts are similar to the noise in speech\sphinxhyphen{}parts. In general, VADs are accurate only at low noise levels.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}
\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} VAD through energy thresholding}
\PYG{n}{frame\PYGZus{}energy} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{frame\PYGZus{}energy\PYGZus{}dB} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log10}\PYG{p}{(}\PYG{n}{frame\PYGZus{}energy}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}energy\PYGZus{}dB} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{frame\PYGZus{}energy\PYGZus{}dB}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} mean of energy in dB}

\PYG{n}{noise\PYGZus{}threshold\PYGZus{}dB} \PYG{o}{=} \PYG{n}{mean\PYGZus{}energy\PYGZus{}dB} \PYG{o}{\PYGZhy{}} \PYG{l+m+mf}{3.} \PYG{c+c1}{\PYGZsh{} threshold relative to mean}

\PYG{n}{noise\PYGZus{}active} \PYG{o}{=} \PYG{n}{frame\PYGZus{}energy\PYGZus{}dB} \PYG{o}{\PYGZlt{}} \PYG{n}{noise\PYGZus{}threshold\PYGZus{}dB}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Noise_attenuation_19_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Estimate noise in noise frames}
\PYG{n}{noise\PYGZus{}frames} \PYG{o}{=} \PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{noise\PYGZus{}active}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}
\PYG{n}{noise\PYGZus{}estimate} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{noise\PYGZus{}frames}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Noise_attenuation_21_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
In typical office environments and many other real\sphinxhyphen{}world scenarios, the background noise is dominated by low\sphinxhyphen{}frequencies, that is, the low frequencies have high energy. Often low quality hardware also leaks analog distortion into the microphone, such that there can be visible peaks in the higher frequencies.


\paragraph{Minimum statistics}
\label{\detokenize{Enhancement/Noise_attenuation:minimum-statistics}}
\sphinxAtStartPar
An alternative estimate would be the \sphinxstyleemphasis{minimum\sphinxhyphen{}statistics} estimate, where we take the minimum energy of each component over time. Since speech+noise has more energy than just noise alone, the minimum most likely represents only noise. {[}\hyperlink{cite.Enhancement/Noise_attenuation:id40}{Martin, 2001}{]}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{noise\PYGZus{}estimate\PYGZus{}minimum} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Noise_attenuation_24_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We see that the minimum statistics estimate of noise is much lower than the mean. Duh, it is by definition lower. However, the shape of both spectra is the same. Thus, the minimum statistics estimate is biased, but preserves the shape. The bias is something we can correct by adding a fixed constant. The benefit of minimum statistics is that it does not depend on a threshold. We thus replace the threshold parameter by a bias parameter, which is much less prone to errors.

\sphinxAtStartPar
In the following, we will use the mean as our noise model for simplicity.


\subsection{Spectral subtraction}
\label{\detokenize{Enhancement/Noise_attenuation:spectral-subtraction}}
\sphinxAtStartPar
The basic idea of spectral subtraction is that we assume that we have
access to an estimate of the noise energy \( E[\|v\|^2] = \sigma_v^2\) , and we subtract that from the energy of the observation, such that
we define the energy of our estimate as {[}\hyperlink{cite.Enhancement/Noise_attenuation:id42}{Boll, 1979}{]}
\begin{equation*}
\begin{split} \|\hat x\|^2 := \|y\|^2 - \sigma_v^2. \end{split}
\end{equation*}
\sphinxAtStartPar
Unfortunately, since our estimate of noise energy is not perfect and
because we have hiddenly made an inaccurate assumption that \(x\) and \(v\)
are uncorrelated, the above formula can give negative values for the
energy estimate. Negative energy is not realizable and nobody likes
pessimists, so we have to modify the formula to threshold at zero
\begin{equation*}
\begin{split} \|\hat x\|^2 := \begin{cases} \|y\|^2 - \sigma_v^2 & \text{if }
\|y\|^2 \geq \sigma_v^2 \\ 0 & \text{if } \|y\|^2 < \sigma_v^2
\end{cases}. \end{split}
\end{equation*}
\sphinxAtStartPar
Since STFT spectra are complex\sphinxhyphen{}valued, we then still have to find the
complex angle of the signal estimate. If the noise energy is small \(
\|v\|^2 \ll \|x\|^2 \) , then the complex angle of \(x\) is
approximately equal to the angle of \(y\), \( \angle x \approx \angle
y \) , such that our final estimate is (when \( \|y\|^2\geq
\sigma_v^2 \) )
\begin{equation*}
\begin{split} \hat x := \angle y \cdot \|\hat x\| = \frac{y}{\|y\|} \sqrt{
\|y\|^2 - \sigma_v^2} = y \sqrt{\frac{\|y\|^2 -
\sigma_v^2}{\|y\|^2}}. \end{split}
\end{equation*}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{energy\PYGZus{}enhanced} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{subtract}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{noise\PYGZus{}estimate}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{energy\PYGZus{}enhanced} \PYG{o}{*}\PYG{o}{=} \PYG{p}{(}\PYG{n}{energy\PYGZus{}enhanced} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} threshold at zero}
\PYG{n}{enhancement\PYGZus{}gain} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{energy\PYGZus{}enhanced}\PYG{o}{/}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{spectrogram\PYGZus{}enhanced} \PYG{o}{=} \PYG{n}{spectrogram\PYGZus{}matrix}\PYG{o}{*}\PYG{n}{enhancement\PYGZus{}gain}\PYG{p}{;}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Noise_attenuation_28_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Typically spectral subtraction does improve the quality of the signal, but it also leaves room for improvement. In particular, usually we hear musical noise in the higher frequencies, which are components which alternate between on and off. Because these components appear independetly from the speech signal, they are perceived as independent sounds and because they are essentially isolated sinusoids, they are perceived as tones; thus we hear musical noisy “tones”.


\subsection{Minimum\sphinxhyphen{}mean Square Estimate (MMSE) aka. Wiener filtering}
\label{\detokenize{Enhancement/Noise_attenuation:minimum-mean-square-estimate-mmse-aka-wiener-filtering}}
\sphinxAtStartPar
Observe that the form of the relationship above is \( \hat x = y\cdot
g, \) where \(g\) is a scalar scaling coefficient. Instead of the above
heuristic, we could then derive a formula which gives the smallest
error, for example in the minimum error energy expectation sense or
minimum mean square error (MMSE). Specifically, the error energy
expectation is
\begin{equation*}
\begin{split} E\left[\|e\|^2\right] = E\left[\|x-\hat x\|^2\right] =
E\left[\|x-gy\|^2\right] = E\left[\|x\|^2\right] + g^2
E\left[\|y\|^2\right] - 2g E\left[xy\right]. \end{split}
\end{equation*}
\sphinxAtStartPar
If we assume that target speech and noise are uncorrelated, \(
E\left[xv\right]=0 \) ,

\sphinxAtStartPar
then \(
E\left[xy\right]=E\left[x(x+v)\right]=E\left[\|x\|^2\right]
\) and
\begin{equation*}
\begin{split} E\left[\|e\|^2\right] = E\left[\|x\|^2\right] + g^2
E\left[\|y\|^2\right] - 2g E\left[\|x\|^2\right] =
(1-2g)E\left[\|x\|^2\right] + g^2 E\left[\|y\|^2\right]. \end{split}
\end{equation*}
\sphinxAtStartPar
The minimum is found by setting the derivative to zero
\begin{equation*}
\begin{split} 0 = \frac{\partial}{\partial g}E\left[\|e\|^2\right] =
-2E\left[\|x\|^2\right] + 2 g E\left[\|y\|^2\right], \end{split}
\end{equation*}
\sphinxAtStartPar
such that the final solution is
\begin{equation*}
\begin{split} g = \frac{E\left[\|x\|^2\right]}{E\left[\|y\|^2\right]} =
\frac{E\left[\|y\|^2\right]-\sigma_v^2}{E\left[\|y\|^2\right]}.
\end{split}
\end{equation*}
\sphinxAtStartPar
and the \sphinxhref{https://en.wikipedia.org/wiki/Wiener\_filter}{Wiener estimate} becomes
\begin{equation*}
\begin{split} \hat x := y \left(\frac{\|y\|^2 - \sigma_v^2}{\|y\|^2}\right).
\end{split}
\end{equation*}
\sphinxAtStartPar
Observe that this estimate is almost equal to the above, but the square
root is omitted. With different optimization criteria, we can easily
derive further such estimates. Such estimates have different weaknesses
and strengths and it is then a matter of application specific tuning to
choose the best estimate.

\sphinxAtStartPar
Overall, it is however somewhat unsatisfactory that \sphinxstyleemphasis{additive} noise is
attenuated with a \sphinxstyleemphasis{multiplicative} method. However, without a better
model of source and noise characteristics, this is probably the best we
can do. Still, spectral subtraction is a powerful method when taking
into account how simple it is. With minimal assumptions we obtain a
signal estimate which can give a clear improvement in quality.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mmse\PYGZus{}gain} \PYG{o}{=} \PYG{n}{energy\PYGZus{}enhanced}\PYG{o}{/}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{spectrogram\PYGZus{}enhanced\PYGZus{}mmse} \PYG{o}{=} \PYG{n}{spectrogram\PYGZus{}matrix}\PYG{o}{*}\PYG{n}{mmse\PYGZus{}gain}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Noise_attenuation_32_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The quality of the MMSE estimate is not drastically different from the spectral subtraction algorithm. This is not surprising as the algorithms are very similar.

\sphinxAtStartPar
In comparison to noise gating, we also notice that it may not be entirely clear which one is better. To a large extent, this will happen with good quality signals, where the SNR is high. With worse SNR, energy thresholding becomes more difficult and noise gating will surely fail. MMSE is more robust and therefore typically has better quality at low SNR. These methods can also be combined to give the best of both worlds.


\subsubsection{Treating musical noise}
\label{\detokenize{Enhancement/Noise_attenuation:treating-musical-noise}}
\sphinxAtStartPar
For spectral subtraction \sphinxhyphen{}type methods (which includes MMSE), we often encounter musical noise (described above). Similar problems are common in coding applications which use frequency\sphinxhyphen{}domain quantization. The problem is related to discontinuity in spectral components over time. Musical noise is perceived when spectral components are randomly turned on and off.

\sphinxAtStartPar
The solution to musical noise must thus be to avoid components going on and off.
Possible approaches include:


\paragraph{Noise filling}
\label{\detokenize{Enhancement/Noise_attenuation:noise-filling}}
\sphinxAtStartPar
Spectral holes (zeros) can be avoided by a noise floor, where we add random noise any time spectral components are below a threshold.


\paragraph{Mapping}
\label{\detokenize{Enhancement/Noise_attenuation:mapping}}
\sphinxAtStartPar
In spectral subtraction \sphinxhyphen{}type methods, the gain value was thresholded at zero before multiplication by the spectrum. It is this thresholding which causes the problem. Therefore, we could replace the hard threshold with a soft threshold. One such soft threshold is the soft\sphinxhyphen{}plus, defined for an input \(x\) as \(y=\ln\left(e^x+1\right)\).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Noise_attenuation_35_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Noise fill with min(eps,x)}
\PYG{n}{noisefill\PYGZus{}threshold\PYGZus{}dB} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{60}   \PYG{c+c1}{\PYGZsh{} dBs below average noise}
\PYG{n}{noisefill\PYGZus{}level} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{10}\PYG{o}{*}\PYG{o}{*}\PYG{p}{(}\PYG{n}{noisefill\PYGZus{}threshold\PYGZus{}dB}\PYG{o}{/}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}\PYG{o}{*}\PYG{n}{noise\PYGZus{}estimate}  
\PYG{c+c1}{\PYGZsh{}noisefill\PYGZus{}level = 0.2}

\PYG{n}{energy\PYGZus{}enhanced} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{noise\PYGZus{}estimate}

\PYG{c+c1}{\PYGZsh{}energy\PYGZus{}noisefill = noisefill\PYGZus{}level + 0.5*(energy\PYGZus{}enhanced \PYGZhy{} noisefill\PYGZus{}level + np.abs(energy\PYGZus{}enhanced \PYGZhy{} noisefill\PYGZus{}level))}
\PYG{n}{energy\PYGZus{}noisefill} \PYG{o}{=} \PYG{n}{noisefill\PYGZus{}level} \PYG{o}{+} \PYG{p}{(}\PYG{p}{(}\PYG{n}{energy\PYGZus{}enhanced} \PYG{o}{\PYGZhy{}} \PYG{n}{noisefill\PYGZus{}level}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{energy\PYGZus{}enhanced} \PYG{o}{\PYGZhy{}} \PYG{n}{noisefill\PYGZus{}level}\PYG{p}{)}
\PYG{n}{mmse\PYGZus{}noisefill\PYGZus{}gain} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{energy\PYGZus{}noisefill}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{spectrogram\PYGZus{}matrix}\PYG{p}{)}
\PYG{n}{spectrogram\PYGZus{}enhanced\PYGZus{}noisefill} \PYG{o}{=} \PYG{n}{spectrogram\PYGZus{}matrix}\PYG{o}{*}\PYG{n}{mmse\PYGZus{}noisefill\PYGZus{}gain}\PYG{p}{;}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_hide-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Noise_attenuation_37_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.lib.display.Audio object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We find that noisefilling \sphinxhyphen{}type methods offer a compromise between amount of noise removed and the amount of musical noise. Usually we try to tune the parameter such that we are just above the level where musical noise is perceived, since it is a non\sphinxhyphen{}natural distortion and therefore more annoying than an incomplete noise attenuation. However, closer tuning is dependent on application and preference.


\subsubsection{Wiener filtering for vectors}
\label{\detokenize{Enhancement/Noise_attenuation:wiener-filtering-for-vectors}}
\sphinxAtStartPar
Above we considered the scalar case, or conversely, the case where we
can treat components of a vector to be independent such that they can be
equivalently treated as a collection of scalars. In some cases, however,
we might be unable to find an uncorrelated representation of the signal
or the corresponding whitening process could be unfeasibly complex or it
can incur too much algorithmic delay. We then have to take into account
the correlation between components.

\sphinxAtStartPar
Consider for example a desired signal \( x\in{\mathbb R}^{N \times1}
\) , a noise signal \( v\in{\mathbb R}^{N \times1} \) and their
additive sum, the observation \( y = x+v, \) from which we want to
estimate the desired signal with a linear filter \( \hat x := a^H y.
\) Following the MMSE derivation above, we set the derivative of the
error energy expectation to zero
\begin{equation*}
\begin{split} \begin{split} 0&=\frac{\partial}{\partial
a}E\left[\|e\|^2\right] =\frac{\partial}{\partial
a}E\left[\|x-\hat x\|^2\right] =\frac{\partial}{\partial
a}E\left[\|x-a^H y\|^2\right] \\& =\frac{\partial}{\partial
a}E\left[\|x-a^H (x+v)\|^2\right] =2E\left[(x+v)\left(x-a^H
(x+v)\right)^H\right] \\& =2\left[ E[xx^H] - \left(E[xx^H] +
E[vv^H]\right)a\right] =2\left[ R_{xx} - \left(R_{xx} +
R_{vv}\right)a\right] \end{split} \end{split}
\end{equation*}
\sphinxAtStartPar
Where the covariance matrices are \( R_{xx} = E[xx^H] \) and \(
R_{vv} = E[vv^H] \) , and we used the fact that \(x\) and \(v\) are
uncorrelated \( E[xv^H]=0 \) . The solution is then clearly
\begin{equation*}
\begin{split} a=\left(R_{xx}+R_{vv}\right)^{-1} R_{xx} = R_{yy}^{-1}
\left(R_{yy}-R_{vv}\right), \end{split}
\end{equation*}
\sphinxAtStartPar
where we for now assume that the inverse exists. This solution is
clearly similar to the MMSE solution for the scalar case.

\sphinxAtStartPar
A central weakness of this approach is that it involves a matrix
inversion, which is computationally complex operation, such that on\sphinxhyphen{}line
application is challenging. It furthermore requires that the covariance
matrix \(R_{yy}\) is invertible (positive definite), which places
constraints on the methods for estimating such covariances.

\sphinxAtStartPar
In any case, Wiener filtering is a convenient method, because it
provides an analytical expression for an optimal solution in noise
attenuation. It consequently has very well documented properties and
performance guarantees.


\subsection{Masks, power spectra and temporal characteristics}
\label{\detokenize{Enhancement/Noise_attenuation:masks-power-spectra-and-temporal-characteristics}}
\sphinxAtStartPar
As seen above, we can attenuate noise if we have a good estimate of the
noise energy. However, actually, both the spectral subtraction and
Wiener filtering methods use models of the speech and noise energies.
The models used above were rather heuristic; noise energy was assumed to
be “known” and speech energy was defined as energy of observation minus
noise energy. It is however not very difficult to make better models
than that. Before going to improved models, note that we did not use
speech and noise energies independently, but only their ratio. Now
clearly the ratio of speech and noise is the signal\sphinxhyphen{}to\sphinxhyphen{}noise ratio (SNR)
of that particular component. We thus obtain an estimate of the SNR of
the whole spectrum. Conversely, we would need only the SNR of the
spectrum to attenuate noise with the above methods. The SNR as a
function of frequency and time is often referred to as a \sphinxstyleemphasis{mask} and in
the following we will discuss some methods for generating such masks. It
is however important to understand that mask\sphinxhyphen{}based methods are operating
on the power (or magnitude) spectrum and thus do not include any models
of the complex phase. Indeed, efforts have in general focused mostly on
the power spectrum and much less on the phase. On one hand, the
motivation is that characteristics of the power spectrum are much more
important to perception than the phase (though the phase is also
important), but on the other hand, the power spectrum is also much
easier to work with than the phase. Therefore there has been both much
more demand and supply of methods which treat the power spectrum.

\sphinxAtStartPar
To model speech signals, we can begin by looking at spectral envelopes,
the high\sphinxhyphen{}level structure of the power spectrum. It is well\sphinxhyphen{}known that
the phonetic information of speech lies primarily in the shape of the
spectral envelope, and the lowest\sphinxhyphen{}frequency peaks of the envelope
identify the vowels uniquely. In other words, the distribution of
spectral energy varies smoothly across the frequencies. This is
something we can model and use to estimate the spectral mask. Similarly,
we know that phonemes vary slowly over time, which means that the
envelope varies slowly over time. Thus, again, we can model envelope
behaviour over time to generate spectral masks.

\sphinxAtStartPar
A variation of such masks is known as \sphinxstyleemphasis{binary} masks, where we can set,
for example, that the mask is 1 if speech energy is larger than noise
energy, and 0 otherwise. Clearly this is equivalent with thresholding
the SNR at unity (which is equivalent to 0 dB), such that an SNR\sphinxhyphen{}mask
can always be converted to a binary mask, but the reverse is not
possible. The benefit of binary masks is that it simplifies some
computations.

\sphinxAtStartPar
If we then want to attenuate noise in a particular frame of speech it is
then useful to use as much of the surrounding data (context) as
possible. For best quality, we can model, say, a second of the speech
signal both before and after the target frame. Though this can improve
quality of the estimate, this has two clear negative consequences. First
of all, including more data increases computational complexity. The
level of acceptable complexity is though highly dependent on the
application. Secondly, if we look into \sphinxstyleemphasis{future} frames to process the
current frame, then we have to have access to such data. In a on\sphinxhyphen{}line
system, this means that we have to wait for the whole look\sphinxhyphen{}ahead data to
arrive before processing, such that the overall system has a delay
corresponding to the length of the look\sphinxhyphen{}ahead. We can extrapolate the
current frame from past frames, but interpolating between past and
future frames does give much better quality. The amount of acceptable
delay is also an application dependent question.


\subsection{Machine learning methods}
\label{\detokenize{Enhancement/Noise_attenuation:machine-learning-methods}}
\sphinxAtStartPar
The first choice in designing machine learning methods for noise
attenuation and other speech enhancement tasks is the overall systems
architecture. The application is usually simply a neural network which
takes noisy speech as input and outputs an estimate of the clean speech.
A natural choice would then be to train the network with a large
database of noisy speech samples and minimize the distance of the output
to the clean speech signal. Since we assume that noise is additive, we
can create synthetic samples by adding noise to speech. By varying the
intensity (volume) of the noise samples, we can further choose the
signal to noise ratio of the noise samples. With reasonable size
databases of speech and noise, we thus get a practically infinite number
of unique noisy samples such that we can make even a large neural
network to converge.

\sphinxAtStartPar
A weakness of this model however is that even if the database is thus
large, it has only a limited number of unique noises and unique
speakers. There is no easy way of getting assurance that unseen noises
and speakers would be enhanced effectively. What if we receive a noisy
sample where a 3 year\sphinxhyphen{}old child talks with annoying
\sphinxhref{https://en.wikipedia.org/wiki/Vuvuzela}{vuvuzelas} playing in the
background. If our database contained only adult speakers and did not
contain vuvuzela\sphinxhyphen{}sounds, then we cannot know whether our enhancement is
effective on the noisy sample.

\sphinxAtStartPar
Machine learning configuration for speech enhancement with noisy and
target clean speech signal.

\sphinxAtStartPar
\sphinxincludegraphics{{175508243}.png}

\sphinxAtStartPar
To overcome the problem of inadequate noise databases, we can take an
\sphinxstyleemphasis{adversarial} approach, where we have a \sphinxstyleemphasis{generative} network which
generates noises and an enhancement network which attenuates noises
which corrupt speech. This approach is known as a \sphinxstyleemphasis{generative
adversarial network (GAN)}. We then have two optimization tasks;
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
To optimize the enhancement network (minimize estimation error) to
remove the noise generated by the generative network and

\item {} 
\sphinxAtStartPar
to optimize the generative network (maximize estimation error) to
generate noises which the enhancement network is unable to remove.

\end{enumerate}

\sphinxAtStartPar
These two tasks are adversial in the sense that they work against each
other. In practical application we would use only the enhancement
network, so the generative network is used only in training.

\sphinxAtStartPar
Application and training with a generative adversarial network (GAN)
structure for speech enhancement.

\sphinxAtStartPar
\sphinxincludegraphics{{175508259}.png}


\subsection{References}
\label{\detokenize{Enhancement/Noise_attenuation:references}}
\sphinxstepscope


\section{Echo cancellation}
\label{\detokenize{Enhancement/Echo_cancellation:echo-cancellation}}\label{\detokenize{Enhancement/Echo_cancellation::doc}}
\sphinxAtStartPar
A frequently occurring distortion in speech telecommunication scenarios
is echoes, which have either an electric or acoustic cause. Electric
echoes appear in analogue networks at points of impedance mismatch.
Since a majority of telecommunication networks today are digital, such
electric echoes are mostly of historical interest and not discussed here
further. Acoustic echoes however is a problem which has become more 
important, especially with the increasing use of teleconferencing
services. Such acoustic echoes appear when the speech of a person A is
played through the loudspeakers for a person B, such that the
loudspeaker sound is picked up by the microphone of person B and
transmitted back to person A. If the delay would be mere milliseconds,
then person A would perceive the feedback signal as reverberation, which
is \sphinxstyleemphasis{not} too disturbing (in some circumstances such an effect, known as
\sphinxstyleemphasis{side\sphinxhyphen{}talk}, could actually be a useful feature indicating that the
microphone is recording). However, typically the transmission delay is
above 100ms, such that the feedback is perceived as an echo, which is
usually highly disconcerting and disturbing. Even worse, sometimes the
acoustic echo signal completes the feedback loop and goes in a circle,
again and again. If the feedback signal is attenuated on each loop, then
the feedback slowly diminishes, but sometimes the feedback loop can
amplify the signal such that the signal quickly escalates up to the
physical limit of hardware or until the loudspeakers blow up. It is thus
clear that echoes must be avoided in real\sphinxhyphen{}life systems.

\sphinxAtStartPar
Fortunately, \sphinxstyleemphasis{echo cancellation} is well\sphinxhyphen{}understood problem with
“standard” solutions available. In short, these methods are based on
estimating the acoustic room\sphinxhyphen{}impulse\sphinxhyphen{}response (RIR), filtering the
loudspeaker signal with the RIR to obtain an estimate of the acoustic
echo, and subtracting the estimated echo from the microphone signal.
Though it is a classic problem with off\sphinxhyphen{}the\sphinxhyphen{}shelf solutions available,
it remains a difficult problem. Central difficulties include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The RIR is not stationary and must therefore be estimated on\sphinxhyphen{}line as
an \sphinxstyleemphasis{adaptive filter}. The RIR changes whenever a door or window is
opened or closed, furniture are moved or people move within the
room, and even when the temperature of the room changes. Changes are
even more rapid when using mobile, handheld or wearable devices,
when the location of the device can change rapidly. In the worst
case, the microphone and loudspeaker can be on different devices
such that their acoustic distance can change rapidly.

\item {} 
\sphinxAtStartPar
When the RIR has a long tail, that is, when the room has a long
reverberation, then the corresponding filters must be long, which
increases \sphinxstyleemphasis{computational complexity}. Fast adaptation to changing
RIRs also increases computational requirements.

\end{itemize}

\sphinxAtStartPar
Observe that echo \sphinxstyleemphasis{cancellation} methods refer to \sphinxstyleemphasis{subtracting} the
estimated echo from the microphone signal. In contrast, {\hyperref[\detokenize{Enhancement/Noise_attenuation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{noise
attenuation}}}} methods \sphinxstyleemphasis{multiply} the microphone
signal with a positive scalar, such that the output signal approximates
the echo\sphinxhyphen{}free signal. The essential difference is that multiplicative
methods generally estimate only the energy/magnitude of the signal,
while subtractive methods try to match both magnitude and phase. The
benefit of subtractive methods is that the output quality is generally
better since the removal\sphinxhyphen{}method matches the physical process which
creates the signal. Multiplicative methods however are much more robust
than subtractive methods; in particular, if the phase is incorrectly
estimated, then a subtractive method can \sphinxstyleemphasis{increase} the amount of noise
in a signal. In the worst case, an echo canceller poorly matched to the
room response can generate a catastrophic feedback loop, whereas
multiplicative methods can be designed to never increase the amount of
noise.

\sphinxAtStartPar
\sphinxstyleemphasis{Echo suppression} methods use this insight to remove acoustic echo with
a multiplicative method similar to that of spectral subtraction or
Wiener filtering used in noise attenuation.


\subsection{Echo cancellation solutions}
\label{\detokenize{Enhancement/Echo_cancellation:echo-cancellation-solutions}}
\sphinxAtStartPar
As it is mentioned above, the problem that echo cancellation needs to
solve is the effect of the room in the path from a loudspeaker to the
microphone used in the communication. This means that the signal played
by the loudspeaker enters the system with a certain delay and multiple
echo paths (Room impulse response). From now on, we will refer to the
received signal played by the loudspeaker as far\sphinxhyphen{}end (\sphinxstyleemphasis{x(n)}), and the
useful speech signal from the user will be called near\sphinxhyphen{}end signal
(\sphinxstyleemphasis{s(n)}). The mixture recorded by the microphone that would be sent to
the other end of the call can be represented as:
\begin{equation*}
\begin{split} d(n) = s(n) + h(n)*x(n) + v(n) \end{split}
\end{equation*}
\sphinxAtStartPar
where \sphinxstyleemphasis{v(n)} represents additive noise in the scene and, for simplicity,
will be considered part of \sphinxstyleemphasis{s(n)}. The effect of the room on the far\sphinxhyphen{}end
signal can be modelled as an FIR filter \sphinxstyleemphasis{h(n)}, added to the far\sphinxhyphen{}end
signal using a convolution. The idea of echo cancellation is to find an
estimation of this FIR filter, and applying it to the received far\sphinxhyphen{}end
signal, we can then subtract it from \sphinxstyleemphasis{d(n)} to extract the useful
near\sphinxhyphen{}end signal.

\sphinxAtStartPar
The estimation of the echo path model is done by \sphinxstyleemphasis{minimizing} the \sphinxstyleemphasis{mean
square error} (MMSE) between the recorded signal and the estimated
filtered far\sphinxhyphen{}end, assuming that s(n) is not present. However, only one
estimation is not possible due to multiple factors.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The echo path is unknown at the beginning of the communication,
additionally, any changes in the echo path can be catastrophic in
the estimation.

\item {} 
\sphinxAtStartPar
The non\sphinxhyphen{}stationariety of the speech signal makes the estimation of
the echo path a complex task.

\item {} 
\sphinxAtStartPar
It is very likely that \sphinxstyleemphasis{s(n)} is present in the recorded signal,
causing the estimation to be flawed (Double\sphinxhyphen{}talk).

\end{enumerate}

\sphinxAtStartPar
These three issues require a continuous monitoring of the quality of the
estimation and the filter must be periodically updated in order to
accurately represent the echo path. For that reason the MMSE method must
be calculated iteratively as we receive more information from the
far\sphinxhyphen{}end and recorded signals. The most popular algorithm to calculate
this adaptive filter is Least Mean Squares (LMS) and its multiple
variants. The expression to minimise is:
\begin{equation*}
\begin{split} MMSE = min \|d(n) - \hat{h}(n)*x(n)\|^2 \end{split}
\end{equation*}
\sphinxAtStartPar
The objective is to find the coefficients of the estimated filter that
minimise the previous equation, and a simple depiction of the iterative
LMS algorithm used to update the filter can be divided in three steps:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Estimate the recorded echo signal:

\end{itemize}
\begin{equation*}
\begin{split} \hat{y}(n) = \sum_{i} \hat{h}(n-i)*x(n-i) \end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
Estimate the error:

\end{itemize}
\begin{equation*}
\begin{split} e(n) = d(n) - \hat{y}(n) \end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
Update the filter weights:

\end{itemize}
\begin{equation*}
\begin{split} \hat{h}_{t+1}(n) = \hat{h}_{t}(n) + \mu \cdot e^H(n) \cdot
x(n) \end{split}
\end{equation*}
\sphinxAtStartPar
The term \sphinxstyleemphasis{μ} represents the learning rate of the algorithm. Higher
values will make the algorithm converge faster, but might converge to a
bigger error value, while smaller learning rate will converge slower and
it might not be able to follow the changes in the echo path. It has been
proven that an adaptive learning rate provides the best results and most
modifications on this method, focus on handling an adaptive learning
rate according to the required specifications. As we can see in the
second equation, the estimation error of the adaptation algorithm is
also the signal that will be used as output and that will contain the
corresponding \sphinxstyleemphasis{s(n)} in an optimal case.

\sphinxAtStartPar
This first approach of the LMS adaptive filter is proposed to be applied
on the time domain of the audio signal. If the filter updates on every
sample of the received audio signal, accurately modelling a room impulse
response will require a filter of a few thousand samples. Updating the
filter on every new sample becomes computationally expensive, and for
these reasons other methods are proposed based on this approach:
\begin{itemize}
\item {} 
\sphinxAtStartPar
BlockLMS: Reduce the rate of updates in the algorithm, such that the
update is only applied once every certain number of samples can help
considerably reduce the complexity of the algorithm. However, the
block processing can affect the convergence of the algorithm and
reduce its responsiveness.

\item {} 
\sphinxAtStartPar
Frequency Domain Adaptive Filters (FDAF): The simplest case of FDAF
consists in converting the audio signal to the frequency domain
using an STFT, and then apply an independent LMS filter on each of
the frequency components of the signal. This alllows to represent
the echo path with a much smaller amount of samples. Considering the
algorithm with quadratic complexity, it is preferable to have many
short filters than having just a long one.

\end{itemize}

\sphinxAtStartPar
Finally, as we mention above, the main problem that adaptive filters
face in real\sphinxhyphen{}life communication applicatoins is double talk. In a common
interaction between two people, it is very likely that both speakers are
active at the same time. In the echo cancellation framework, that means
that both far\sphinxhyphen{}end signal \sphinxstyleemphasis{x(n)} and near\sphinxhyphen{}end \sphinxstyleemphasis{s(n)} will be present in
the mixture. As the adaptive filter tries to minimize the error between
the far\sphinxhyphen{}end signal and the recorded one, in the presence of double\sphinxhyphen{}talk
the filter will likely diverge and remove or distort the near\sphinxhyphen{}end signal
instead of reducing the echo feedback.

\sphinxAtStartPar
To reduce the effect of double\sphinxhyphen{}talk in the adaptation process, it is
important to detect when double\sphinxhyphen{}talk starts and stop the adaptation.
Assuming that the filter had converged before the double\sphinxhyphen{}talk segment,
the the far\sphinxhyphen{}end signal should still be removed, while the near\sphinxhyphen{}end
speaker would pass through. Many methods have been presented to control
the learning rate of the adaptative filter depending on the detected
double\sphinxhyphen{}talk, but most of them are based on three main ideas:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Energy based detection (Geigel detector):
\begin{itemize}
\item {} 
\sphinxAtStartPar
This detector assumes that, if the energy ratio between the
far\sphinxhyphen{}end signal and the recorded one will remain almnost constant
until an additional voice is added from the near\sphinxhyphen{}end signal.
When the ratio between far\sphinxhyphen{}end and recorded signals changes, we
can assume that there is double\sphinxhyphen{}talk.

\item {} 
\sphinxAtStartPar
It is a very simple method that barely adds any computational
complexity to the system.

\item {} 
\sphinxAtStartPar
The detector assumes that the echo level is clearly different
than the near\sphinxhyphen{}end speech. Therefore, this method is highley
influenced by noise and signal misalignment.

\item {} 
\sphinxAtStartPar
The method also requires to be tuned for each specific
configuration and changes in the echo path during communication
might trigger the double\sphinxhyphen{}talk detection.

\end{itemize}

\item {} 
\sphinxAtStartPar
Normalized Cross\sphinxhyphen{}Correlation (NCC):
\begin{itemize}
\item {} 
\sphinxAtStartPar
This method measures the similarity between the input and
processed signals.

\item {} 
\sphinxAtStartPar
It is more robust to noise than the energy levels.

\item {} 
\sphinxAtStartPar
The value of NCC will be close to 1 when double\sphinxhyphen{}talk is present,
and 0 when it is not. Therefore, the NCC can be used as a
scaling factor for the learning rate.

\end{itemize}

\end{itemize}
\begin{equation*}
\begin{split} NCC = \frac{\sigma^{2}_{ed}(n)}{\sigma_{e}(n)\sigma_{d}(n)}
\end{split}
\end{equation*}\begin{equation*}
\begin{split} \sigma^{2}_{x} = E[xx^H] \end{split}
\end{equation*}\begin{equation*}
\begin{split} \sigma^{2}_{x}(n) =
\lambda\sigma^{2}_{x}(n - 1) + (1 - \lambda)\sigma^{2}_{x} \end{split}
\end{equation*}\begin{equation*}
\begin{split} \mu_{NCC} = (1 - NCC)\mu_{max} \end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
Two\sphinxhyphen{}path Echo Cancellation:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Two filters, background and foreground, process the echo signal
simultaneously. The background filter is continuously adapting
on every step, while the foreground one remains fixed. A control
module then decides if there is double\sphinxhyphen{}talk and the better
solution is to update the coefficients or keep them unmodified
based on the output of the backround and foreground filters.

\item {} 
\sphinxAtStartPar
It is the most robust of the presented methods.

\item {} 
\sphinxAtStartPar
Requires additional computational complexity, as the far\sphinxhyphen{}end
signal needs to be filtered twice before deciding if ther is
double\sphinxhyphen{}talk.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
The acoustic feedback loop in telecommunication applications.

\sphinxAtStartPar
\sphinxincludegraphics{{175509295}.png}

\sphinxAtStartPar
Model of the echo path and estimated filter

\sphinxAtStartPar
\sphinxincludegraphics{{203126383}.png}

\sphinxAtStartPar
Feedback loop for echo cancellation

\sphinxAtStartPar
\sphinxincludegraphics{{203126386}.png}

\sphinxAtStartPar
Two\sphinxhyphen{}path echo cancellation model

\sphinxAtStartPar
\sphinxincludegraphics{{203126388}.png}

\sphinxstepscope


\section{Bandwidth extension (BWE)}
\label{\detokenize{Enhancement/Bandwidth_extension_BWE:bandwidth-extension-bwe}}\label{\detokenize{Enhancement/Bandwidth_extension_BWE::doc}}
\sphinxstepscope


\section{Multi\sphinxhyphen{}channel speech enhancement and beamforming}
\label{\detokenize{Enhancement/Multi-channel_speech_enhancement_and_beamforming:multi-channel-speech-enhancement-and-beamforming}}\label{\detokenize{Enhancement/Multi-channel_speech_enhancement_and_beamforming::doc}}
\sphinxAtStartPar
Simple, single\sphinxhyphen{}channel {\hyperref[\detokenize{Enhancement/Noise_attenuation::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{noise attenuation}}}} and
dereverberation is often not sufficient for acceptable quality,
especially in very noisy environments, such as in a car or on a busy
sidewalk. To reach better quality, we can then add more microphones. The
benefit of added microphones includes at least:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Time\sphinxhyphen{}of\sphinxhyphen{}arrival differences between sources enables the use of
beamforming filters, which use such time (phase) differences to
separate between sources.

\item {} 
\sphinxAtStartPar
Intensity level differences between sources; In for example a mobile
phone, we can have forward and backward facing microphones such that
the backward facing microphone is used primarily for estimating
background noise, while the forward facing microphone records the
desired speech signal. By using the background\sphinxhyphen{}noise estimate from
the backward facing microphone, we can then use noise attenuation on
the forward facing microphone to gain better quality.

\item {} 
\sphinxAtStartPar
Each microphone will feature some level of sensor\sphinxhyphen{}noise, that is,
the hardware itself causes some inaccuracies to the signal.
Sensor\sphinxhyphen{}noises are for most parts independent across microphones such
that with each additional microphone we can better separate desired
sources from noise.

\end{itemize}

\sphinxAtStartPar
The most\sphinxhyphen{}frequently discussed approach is to use microphone arrays,
typically in either a linear configuration, where microphones are
equi\sphinxhyphen{}spaced on a straight line, or in a circular configuration, where
microphones are equi\sphinxhyphen{}spaced on a circle. The benefits include that a
linear configuration makes analytical analysis easier, whereas a
circular array can have an almost uniform response in all directions.


\subsection{Delay\sphinxhyphen{}and\sphinxhyphen{}sum beamforming}
\label{\detokenize{Enhancement/Multi-channel_speech_enhancement_and_beamforming:delay-and-sum-beamforming}}
\sphinxAtStartPar
As an introduction to beamforming consider a linear array of \(K\)
microphones with input signals \(x_{k}(t)\), where \(k\) and \(t\) are
the microphone and time indices. We assume that the desired source is
sufficiently far away that we can approximate it with a plane wave. Then
the signal will arrive at the microphones at different times  \(
\Delta t_{x_k} \) and we can calculate time time difference of
arrival (TDOA) of each microphone \( t_{x_k} = \Delta
t_{x_k}-\Delta t_{x_1} \) where we used microphone \(k=1\) as a
reference point. The delayed signals thus have \( x_k(t) = x(t-\Delta
t_{x_k}) = x\left(t-\Delta t_{x_k}+\Delta t_{x_1} - \Delta
t_{x_1}\right) = x_1(t-t_{x_k}). \) Similarly, for the noise sources
we have \( y_k(t) = t_1(t-t_{y_k}). \)

\sphinxAtStartPar
If the desired and noise sources appear at different angles, then their
corresponding delays will be different. Moreover, if we add a
signal \(z(t)\) with itself at a random offset δ, then the summation is
destructive, that is, smaller than the original \(
\frac12\left\|z(t)+z(t+\delta)\right\| \leq \left\|z(t)\right\|
\) . Addition without an offset is obviously constructive, such that we
can form the \sphinxstyleemphasis{delay and sum estimate} as
\begin{equation*}
\begin{split} \hat x(t) = \frac 1K \sum_{k=1}^K x_k(t-t_{x_k}). \end{split}
\end{equation*}
\sphinxAtStartPar
In this summation, all signals approaching from the same direction as
the desired source will be additive (constructive) and other directions
will be (more or less) destructive.

\sphinxAtStartPar
Trivial as it is, the delay\sphinxhyphen{}and\sphinxhyphen{}sum should however be treated as a
pedagogical example only. It does not ideally amplify the desired source
nor attenuate the noise source, and it is sensitive to errors in the
TDOAs.

\sphinxAtStartPar
\sphinxincludegraphics{{175509409}.png}

\sphinxstepscope


\chapter{Computational models of human language processing}
\label{\detokenize{Computational_models_of_human_language_processing:computational-models-of-human-language-processing}}\label{\detokenize{Computational_models_of_human_language_processing::doc}}
\sphinxAtStartPar
One area of research making use of speech technology is the study of
human language learning and processing. Language is a highly complex
phenomenon with physical, biological, psychological, social and cultural
dimensions. Therefore it is also studied across several disciplines,
such as linguistics, neuroscience, psychology, and anthropology. While
many of these fields primarily focus on empirical and theoretical work
on language, computational models and simulations provide another
important aspect to the research: capability to test theoretical models
in practice. Implementation of models capable of processing real speech
data requires techniques from speech processing and machine learning.
For instance, techniques for speech
{\hyperref[\detokenize{Representations/Representations::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{signal representation}}}}
and
{\hyperref[\detokenize{Pre-processing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{pre\sphinxhyphen{}processing}}}} are needed to interface the models with
acoustic speech recordings. Different types of {\hyperref[\detokenize{Modelling_tools_in_speech_processing::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{classifiers and machine
learning algorithms}}}} are needed to
implement learning mechanisms in the models or to analyze behavior of
the developed models. In addition, model training data may be generated
with {\hyperref[\detokenize{Speech_Synthesis::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{speech synthesizers}}}} (e.g., {[}\hyperlink{cite.References:id33}{Havard \sphinxstyleemphasis{et al.}, 2017}{]}), whereas linguistic reference data for model evaluation may
be extracted from speech recordings using {\hyperref[\detokenize{Recognition/Speech_Recognition::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{automatic speech
recognition}}}}.

\sphinxAtStartPar
The basic idea of computational modeling is to understand how humans
learn and process language by implementing human\sphinxhyphen{}like learning and
speech processing capabilities as computational algorithms. The models
are then exposed to inputs similar to what humans observe, and the model
behavior is then recorded and compared to human data (Fig. 1).
Computational models can focus on questions such as how adult speech
perception operates (e.g., the highly\sphinxhyphen{}influential TRACE model of speech
perception; McClelland and Elman {[}\hyperlink{cite.References:id26}{1986}{]}), how language learning takes place
in young children (native language aka. L1 learners; e.g., {[}\hyperlink{cite.References:id34}{Dupoux, 2018}, \hyperlink{cite.References:id20}{Räsänen, 2012}{]}) or in second\sphinxhyphen{}language (L2) learners, or they may
study the emergence and evolution of language through communicative
coordination between multiple agents (see, e.g.,  Kirby {[}\hyperlink{cite.References:id29}{2002}{]}, Steels {[}\hyperlink{cite.References:id14}{1997}{]}, for overviews).

\sphinxAtStartPar
\sphinxincludegraphics{{180302220}.png}
\sphinxstylestrong{Figure 1:} A high\sphinxhyphen{}level schematic view of a typical computational model development and evaluation process.


\section{Human cognition as a sensorimotor information processing system}
\label{\detokenize{Computational_models_of_human_language_processing:human-cognition-as-a-sensorimotor-information-processing-system}}
\sphinxAtStartPar
Computational modeling research is based on the metaphor of human brain
as a computational information processing system. From an external
observer viewpoint, this system perceives the environment using a number
of input channels (senses), processes the information using some type of
processing steps (the nervous system), and creates outputs (motor
actions) based on the available sensory information and other internal
states of the system. This input/output\sphinxhyphen{}relationship is affected by
developmental factors and learning from earlier sensorimotor experience,
realized as changes in the connectivity and structure of the central
nervous system. Computational research attempts to understand the
components of this perception\sphinxhyphen{}action loop by replacing the human
physiology and neurophysiology with computational algorithms for sensory
(or sensorimotor) information processing. Typically the aim is not to
replicate information processing of the brain at the level of individual
neurons, but to focus on the \sphinxstyleemphasis{computational and algorithmic principles}
of the process, i.e., the \sphinxstyleemphasis{information representation}, \sphinxstyleemphasis{flow} and
\sphinxstyleemphasis{transformation} within the system (see \sphinxhref{https://en.wikipedia.org/wiki/David\_Marr\_(neuroscientist)\#Levels\_of\_analysis}{Marr’s levels of
analysis};
Marr {[}\hyperlink{cite.References:id28}{1982}{]}). These processing steps could then be implemented in
infinitely many ways using different hardware (biological neurons,
silicon chips architectures, CPU instruction sets, quantum computing
etc.) or translations from computational description to
implementation\sphinxhyphen{}specific instructions (consider, e.g., different
programming languages with the same CPU instruction set). Despite the
implementation differences, the observed behavior of the system in terms
of inputs and the resulting outputs can still be similar.

\sphinxAtStartPar
To give an example, a model of adult spoken word recognition could focus
on explaining the acoustic, phonetic and/or other linguistic factors
that affect the process of word recognition. Such a model could focus on
the details of how word recognition process evolves over time when a
spoken word is heard, describing how alternative word candidates are
being considered or rejected during this process (see, e.g., Magnuson \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.References:id27}{2020}{]}, Weber and Scharenborg {[}\hyperlink{cite.References:id12}{2012}{]}, for examples). Even if the
model would not focus on modeling neurons of the human brain, it could
still explain how our minds decode linguistic information from speech
input. This explanation could include how the process is affected by
factors such as noisy environments, misprounciations, distributional
characteristics of the input, or non\sphinxhyphen{}native language background of the
listener—all useful information to understand both theoretical
underpinnings and practical aspects of speech communication.

\sphinxAtStartPar
Another central aspect of the modeling is the relationship between human
learning and computational methods trying to characterize the process.
According to the present understanding, \sphinxstyleemphasis{human language learning is
largely driven by} the interaction of \sphinxstyleemphasis{statistical regularities in the
sensory input} available to the learner (e.g., Maye \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.References:id24}{2002}{]}, Saffran \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.References:id16}{1996}{]}, Saffran and Kirkham {[}\hyperlink{cite.References:id15}{2018}{]}, Werker and Tees {[}\hyperlink{cite.References:id11}{1984}{]}),
\sphinxstyleemphasis{innate mechanisms, constraints, and biases for perception and learning}
from such input, and \sphinxstyleemphasis{other mechanisms responsible for social,
communicative and exploratory needs} \sphinxstyleemphasis{o}f the learner. By extracting the
statistical regularities from their sensorimotor linguistic enviroment,
children are capable of learning any of the world’s languages while
fundamentally sharing the same basic cognitive mechanisms. A central
topic in computational modeling of language acqusition is therefore to
understand how much of language structure can be learned from the input
data, and how much language\sphinxhyphen{}related prior knowledge needs to be built\sphinxhyphen{}in
to the hard\sphinxhyphen{}coded mechanisms of these models. Note that human
statistical learning is closely related to machine learning in
computers, as both aim to extract statistical regularities from data
using some sort of pre\sphinxhyphen{}specified learning principles. However, unlike
standard speech technology systems such as {\hyperref[\detokenize{Recognition/Speech_Recognition::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{automatic speech
recognition}}}}, humans learners do not have access to
data labels or consistent reward signals. For instance, a computational
model of early infant word learning is essentially trying to find a
solution to unsupervised pattern discovery problem: how to learn words
from acoustic or multimodal input when there is no data labeling
available. By applying a combination of speech processing and machine
learning techniques to data representative of infant language
experiences, explanation proposals for such a process can be created.


\subsection{Computational modeling versus cognitive computationalism}
\label{\detokenize{Computational_models_of_human_language_processing:computational-modeling-versus-cognitive-computationalism}}
\sphinxAtStartPar
Note that computational modeling and \sphinxstyleemphasis{representations} often studied in
the models should not be confused with classical
\sphinxhref{https://en.wikipedia.org/wiki/Computational\_theory\_of\_mind}{computationalism}.
The latter is loaded with certain assumptions regarding the nature of
the entities processed by the computational system (e.g., \sphinxstyleemphasis{content of
the representations, symbols}) and what are the basic computational
operations (e.g., \sphinxstyleemphasis{symbol manipulation} using Turing machines). In
contrast, computational models are simply descriptions of the studied
process in terms of the described assumptions, inputs, outputs, and
processing mechanisms without prescribing further \sphinxstyleemphasis{meaning} to the
components (unless otherwise specified). For instance, \sphinxstyleemphasis{representations}
of typical DSP and machine\sphinxhyphen{}learning \sphinxhyphen{}based models can simply be treated
as quantifiable states, such as artificial neuron/layer activations,
posterior distributions, neural layer weights, distribution parameters.
In other words, the representations are scalars, vectors, or matrices
that are somehow causally related to the inputs of the system. Behavior
of these representations can then be correlated and compared with
theoretical concepts regarding the phenomenon of interest (e.g.,
comparing selectivity of neural layer activations towards known phoneme
categories in the acoustic input to the model; see, e.g., Nagamine \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.References:id23}{2015}{]}) or comparing the overall model behavior to human behavior
with similar input (e.g., Räsänen and Rasilo {[}\hyperlink{cite.References:id19}{2015}{]}). As long as the models
are able to explain the data or phenomena of interest, the \sphinxstyleemphasis{models are a
computational and hypothetical explanation} to the phenomenon without
loading the components with additional theoretical or philosophical
assumptions. Additional theoretical loading comes from the \sphinxstyleemphasis{data} and
\sphinxstyleemphasis{evaluation protocols} chosen to investigate the models and in terms of
how the modeling findings are interpreted.


\section{Role of computational models in scientific research}
\label{\detokenize{Computational_models_of_human_language_processing:role-of-computational-models-in-scientific-research}}
\sphinxAtStartPar
Computational modeling has a role in scientific theory development and
hypothesis testing by providing the means to test high\sphinxhyphen{}level theories of
language processing with practical simulations (Fig. 2). This supports
the more traditional approaches to language research that include
collection of empirical data on human language processing, conducting
brain research, or running controlled behavioral experiments in the
laboratory or as real\sphinxhyphen{}world intervention studies. By implementing
high\sphinxhyphen{}level conceptual models of language processing using real
algorithms operating on real\sphinxhyphen{}world language data, one can test whether
the models scale up to complexity of real\sphinxhyphen{}world sensory data accessible
to human listeners. In addition to explaining already collected data on
human language processing, computational models can also lead to new
insights and hypotheses about the human processing to be tested in
behavioral experiments.

\sphinxAtStartPar
\sphinxincludegraphics{{180300259}.png}
\sphinxstylestrong{Figure 2:}
Different aspects of human language processing research and how they
interact.Computational modeling uses data from empirical research to test and
inform high\sphinxhyphen{}level theories related to the given topic.

\sphinxAtStartPar
One potential advantage of computational modeling is its capability to
address multiple processing mechanisms and language phenomena
simultaneously. This is since \sphinxstyleemphasis{computational models can, and must,
explicitly address all aspects of the information processing chain} from
input data to the resulting behaviour. By first formulating theories of
language processing in terms of computational goals and operations, then
implementing them as functional signal processing and machine learning
algorithms, and finally exposing them to realistic sensory data
comparable to what real humans experience, ecological plausibility and
validity of the underlying theories can be explicitly tested (cf. Marr {[}\hyperlink{cite.References:id28}{1982}{]}). In contrast, behavioral experiments with real humans—although
necessary for the advancement of our scientific understanding and for
general data collection—can usually focus on only one phenomenon at a
time due to the need for highly\sphinxhyphen{}controlled experimental setups. The
fragmentation of focus also makes it difficult to combine knowledge from
individual studies into holistic theoretical frameworks (e.g.,
understanding how phonemic, lexical, and syntactic learning are
dependent on each other in early language development).


\section{Examples of computational modeling research}
\label{\detokenize{Computational_models_of_human_language_processing:examples-of-computational-modeling-research}}
\sphinxAtStartPar
\sphinxstyleemphasis{\sphinxstylestrong{Computational models of child language development}}: Computational
models of language learning aim at understanding how human children
learn to perceive and produce their native language. The basic idea is
to simulate the learning of a human child, either starting from “birth”
or from a specific stage of language development. Individual models
typically aim to answer questions such as: how phonemic categories are
learned, how word segmentation is achieved, how spoken words are
associated with their referential meanings, or how syntax can be
acquired? The grand challenge is to understand how the adult\sphinxhyphen{}like
understanding of language as a discrete, symbolic, and compositional
system can emerge from the exposure to noisy and inherently continuous
sensorimotor environment. Typical computational modeling research
questions include: 1) \sphinxstyleemphasis{to what extent are languages learnable from the
statistics of sensory experiences}, 2) \sphinxstyleemphasis{what type of learning mechanisms
or constraints are needed for the process}, and 3) \sphinxstyleemphasis{what kind of and how
much data} (“experiences”) are required in the process (quality and
quantity of speech, uni\sphinxhyphen{} vs. multimodal input etc.). A broader view
takes into account the fact that the children are not just passive
receivers of sensory information but can interact with their caregivers
and their environment as active explorers and learners. Therefore it is
also of interest 4) \sphinxstyleemphasis{what type of additional interaction\sphinxhyphen{}related
mechanisms and dynamically created experiences are critica}l. The big
and yet unaswered question is what are the critical ingredients for
successful language learning, as all normally developing children with
very different language experiences, environments, and also somewhat
differing cognitive skills still manage to converge to a shared
communicative system of their native language.      As the short\sphinxhyphen{}term outcomes, models of language learning can test
and propose different hypotheses for different aspects of language
learning. They also produce functional algorithms for processing
acoustic or multimodal language data in low\sphinxhyphen{}resource settings, where
access to data labels is limited (e.g., Kakouros and Räsänen {[}\hyperlink{cite.References:id31}{2016}{]}, Kamper \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.References:id30}{2017}{]}, Räsänen \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.References:id17}{2018}{]}). Long\sphinxhyphen{}term outcomes from language
acquisition modeling contribute to both basic science and practice. In
terms of basic science, the research tries to answer the question of how
one of the most advanced aspects of human cognition, i.e., language,
operates. Long\sphinxhyphen{}term practical goals include understanding the impact of
external factors in language development and how to ensure equally
supportive environments for children in different social settings,
understanding different types of language\sphinxhyphen{}related disorders and how to
best respond to them, but also how to develop autonomous AI systems
capable of human\sphinxhyphen{}like language learning and understanding without
supervised training, i.e.., development of systems ultimately capable of
\sphinxstyleemphasis{understanding the intentions and meaning in communication}.      Computational modeling of early language acquisition is closely
related to zero\sphinxhyphen{}resource speech processing (see
\sphinxurl{http://www.zerospeech.com/}) that aims at algorithms capable of
unsupervised language learning from speech data.

\sphinxAtStartPar
\sphinxstyleemphasis{\sphinxstylestrong{Models of spoken word recognition}:} Another widely studied topic is
speech perception in adults. Computational models developed for this
purpose attempt to explain how the brain processes incoming speech in
order to recognize words in the input. Models in this area may focus on
explaining the interaction between sub\sphinxhyphen{}word and word\sphinxhyphen{}level units in
perception, on how words compete with each other during the recognition
process, or, e.g., on how the speech perception is affected by noise in
native and non\sphinxhyphen{}native listeners. Since word recognition is essentially a
temporal process, particular attention is typically paid to the
evolution of the recognition process as a function of time (or
proportion of input word or utterance perceived).    For an overview, see Weber and Scharenborg {[}\hyperlink{cite.References:id12}{2012}{]}. For some examples
of models, see Magnuson \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.References:id27}{2020}{]}, McClelland and Elman {[}\hyperlink{cite.References:id26}{1986}{]}, Norris {[}\hyperlink{cite.References:id22}{1994}{]}.

\sphinxAtStartPar
\sphinxstyleemphasis{\sphinxstylestrong{Models of speech production}}: This line of research attempts to
explain how human speech production works in terms of articulators and
their motor control. Some studies also focus on the acquisition of
speech production skills. Typical speech production models involve an
articulatory {\hyperref[\detokenize{Speech_Synthesis::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{speech synthesizer}}}}—an algorithm capable
of producing audible speech signals by modeling the physical
characteristics of the vocal apparatus—and some type of motor control
algorithms that are responsible for phonation and articulator movements.
Sometimes hearing system is simulated as well. These models have various
uses from general understanding of the articulatory basis of speech to
understanding speech pathologies, articulatory learning in childhood or
adulthood, or special types of sound production such as singing.    For classical and more recent examples of articulatory models of
speech production, see, e.g., Birkholz {[}\hyperlink{cite.References:id36}{2005}{]}, Birkholz \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.References:id35}{2015}{]}, Maeda {[}\hyperlink{cite.References:id25}{1988}{]}. For models of infant learning of speech
production, see, e.g.,Howard and Messum {[}\hyperlink{cite.References:id32}{2014}{]}, Rasilo and Räsänen {[}\hyperlink{cite.References:id18}{2017}{]}, Tourville and Guenther {[}\hyperlink{cite.References:id13}{2011}{]}.

\sphinxAtStartPar
\sphinxstyleemphasis{\sphinxstylestrong{Multi\sphinxhyphen{}agent models of language learning, evolution and
communication:}} Languages are essentially cultural conventions based
on social activity, enabled by genetically coded cognitive and
physiological mechanisms, and learned through interactions between
people. One branch of computational modeling focuses on understanding
how languages emerge, evolve, and are learned through multi\sphinxhyphen{}agent
communication and interaction. These simulations, sometimes referred to
as \sphinxstyleemphasis{language games} or \sphinxstyleemphasis{iterated learning} (see Kirby {[}\hyperlink{cite.References:id29}{2002}{]}), focus on
non\sphinxhyphen{}linear dynamical systems that result from the interaction of
multiple communicative computational agents. These agents can be purely
based on simulation, or they can be based on physical robots interacting
in a shared  physical environment. By providing the agents with
different types of innate goals, mechanisms, learning skills and
environmental conditions, one can study the extent that language\sphinxhyphen{}like
signaling systems (as a social system) or language skills (as subjective
capabilities) can emerge from such conditions.    For overviews, see Kirby {[}\hyperlink{cite.References:id29}{2002}{]}, Steels {[}\hyperlink{cite.References:id14}{1997}{]}.


\section{References and further reading}
\label{\detokenize{Computational_models_of_human_language_processing:references-and-further-reading}}
\sphinxAtStartPar


\sphinxAtStartPar
Birkholz, P.: VocalTractLab: \sphinxurl{http://www.vocaltractlab.de/} {[}for work
on articulatory synthesis{]}

\sphinxAtStartPar
Dupoux, E. et al.: Zero Resource Speech Challenge:
\sphinxurl{http://www.zerospeech.com/} {[}a challenge on unsupervised speech
pattern learning{]}

\sphinxstepscope

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}
\end{sphinxuseclass}
\end{sphinxuseclass}

\chapter{Security and privacy in speech technology}
\label{\detokenize{Security_and_privacy:security-and-privacy-in-speech-technology}}\label{\detokenize{Security_and_privacy::doc}}
\sphinxAtStartPar
\sphinxstyleemphasis{\sphinxstylestrong{DISCLAIMER:}} \sphinxstyleemphasis{This document is meant to be an introduction to
questions in security and privacy in speech technology for engineering
students, such that they would understand the main problematic. In
particular, this is not a legal document. In real\sphinxhyphen{}life application of
technology and data collection, you must consult legal experts to
determine whether you follow the law. You are responsible.}

\sphinxAtStartPar
The \sphinxhref{https://en.wikipedia.org/wiki/Right\_to\_privacy}{right to privacy}
is a widely accepted concept though its definition varies. It is however
clear that people tend to think that some things are “theirs”, that they
have ownership of \sphinxstyleemphasis{things}, including information about themselves. A
possible definition of privacy would then be “the absence of attention
from others” and correspondingly security could be defined as the
protection of that which one owns, including material and immaterial
things. It however must be emphasised that there are no widely shared
and accepted definitions and in particular, the legal community has a
wide range of definitions depending on the context and field of
application.

\sphinxAtStartPar
From the perspective of speech technology, security and privacy has two
principal aspects;
\begin{itemize}
\item {} 
\sphinxAtStartPar
Security and privacy of data related to speech signals and

\item {} 
\sphinxAtStartPar
Protection against attacks which use speech signals as a tool.

\end{itemize}

\sphinxAtStartPar
The latter aspect is mainly related to speaker identity; fraudsters can
for example synthesise speech which mimics (spoofs) a target person to
gain access to restricted systems, such as access to the bank account of
the target person. Such use cases fall mainly under the discussions
under {\hyperref[\detokenize{Recognition/Speaker_Recognition_and_Verification::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{speaker recognition and
verification}}}}, and not discussed
further here.

\sphinxAtStartPar
Observe that in the isolated category of telephony (classical telephone
connections) privacy and security already have well\sphinxhyphen{}established ethical
standards as well as legislation. In typical jurisdictions, telephone
calls are private in the sense that only the “intended” participants can
listen to them and sometimes even recording them is restricted. Covert
listening is usually allowed only for the police and even for them only
in specially regulated situations, such as with a permission granted by
a court or judge.

\sphinxAtStartPar
{[}\hyperlink{cite.Security_and_privacy:id43}{Lareo, 2019}, \hyperlink{cite.Security_and_privacy:id51}{Nautsch \sphinxstyleemphasis{et al.}, 2019}{]}


\section{System models}
\label{\detokenize{Security_and_privacy:system-models}}

\subsection{All\sphinxhyphen{}human interaction}
\label{\detokenize{Security_and_privacy:all-human-interaction}}
\sphinxAtStartPar
Speech is a tool for communication such that it is generally sensible to always discuss interactions between two agents, say, Alice and Bob. The interaction between them is the desired function such that the information exchanged there is explicitly permitted. By choosing to talk with each other, they both reveal information to the extent speech contains such information.


\subsubsection{Primary interaction}
\label{\detokenize{Security_and_privacy:primary-interaction}}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Though Alice and Bob knowingly and intentionally interact, they might reveal private things. This is the classic “\sphinxstyleemphasis{slip of the tongue}”.


\subsubsection{Secondary interactions}
\label{\detokenize{Security_and_privacy:secondary-interactions}}
\sphinxAtStartPar
A second\sphinxhyphen{}order question are third parties, who are not part of the main speech interaction. The pertinent question is the degree to which the third party is allowed to partake in an interaction. As a practical example, suppose Alice and Bob have a romantic dinner at a restaurant. To which extent is the waitress Eve allowed to interact with the discussion of Alice and Bob? Clearly Eve has some necessary tasks such that interaction is unavoidable. Will Alice and Bob, for example, pause their discussion when Eve approaches?

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Observe that we have here labelled Eve as a “\sphinxstyleemphasis{restricted}” and not as an “\sphinxstyleemphasis{unauthorized}” interactor. If access is unauthorized, then it is clear that Eve should not have any access to the speech interaction, which is generally straightforward to handle. The word restricted, on the other hand, implies that unimpeded access should not be granted, but that some access can be allowed. It is thus not question of “\sphinxstyleemphasis{if}” access should be granted but “\sphinxstyleemphasis{how much?}”.


\subsubsection{Ownership and personal privacy}
\label{\detokenize{Security_and_privacy:ownership-and-personal-privacy}}
\sphinxAtStartPar
Privacy is closely connected to ownership of immaterial property, that is, information. Such ownership can also be translated to the question of \sphinxstyleemphasis{who has the control} over some information? In terms of \sphinxstyleemphasis{personal privacy}, it is clear that it relates to information to which a single person can claim ownership.

\sphinxAtStartPar
Speech is more complicated. Speech is a form of communication and thus relates to an interaction between two parties. Dialogues can also commonly lead to co\sphinxhyphen{}creation of meaning, where new information is generated through the dialogue in a form which none of the involved parties could have alone produced {[}\hyperlink{cite.Security_and_privacy:id44}{Gasiorek, 2018}{]}. None of the users can thus claim sole ownership of the information, but the ownership is shared. Currently we do not have the tools for handling such shared ownership.


\subsection{Interactions which involve devices or services}
\label{\detokenize{Security_and_privacy:interactions-which-involve-devices-or-services}}

\subsubsection{Telecommunication}
\label{\detokenize{Security_and_privacy:telecommunication}}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Talking over the phone and video conference involve transmission of speech through a telecommunication service. Here we consider scenarios where the telecommunication device does not include any advanced functionalities or artificial intelligence. Most countries have clearly definied rules that specify the situations when such communication can be eavesdropped. In most jurisdictions, only the police is allowed to intercept such traffic and only in specific situations.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Such interception of private communication is interesting primarily from ethical and legal perspectives, but does not contain technological challenges related to the communication itself. The main technological challenges are related to forensics;
\begin{itemize}
\item {} 
\sphinxAtStartPar
What information can the police extract (e.g. speaker identity, emotions and health)?

\item {} 
\sphinxAtStartPar
How can speakers (and service providers) protect themselves from unauthorized interception by, for example, stripping away information such as speaker identity, from the transmitted signal?

\end{itemize}


\subsubsection{Discussion in the presence of speech interfaces}
\label{\detokenize{Security_and_privacy:discussion-in-the-presence-of-speech-interfaces}}
\sphinxAtStartPar
A commonly occuring scenario is one where two or more users engage in a discussion such that there is one or more speech operated devices nearby. For example, a user could have their mobile phone or there could be a smart speaker nearby.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Often, one of the users will be the primary user of said devices (e.g. it is \sphinxstyleemphasis{their} phone), so the question is how the devices should relate to the other users. For example, suppose Alice has a smart speaker at home and Bob comes to visit. What would be the appropriate approach then for both Alice and the agent? Should Alice or the agent notify Bob of the presence of an agent? Or should the agent automatically detect the presence of Bob and change its behaviour (e.g. go to sleep)?

\sphinxAtStartPar
We seem to lack both the cultural habits which dictate how to handle such situations, the legal tools which regulates such situations as well as the technical tools to manage multi\sphinxhyphen{}user access.


\subsubsection{Interaction with a speech interface}
\label{\detokenize{Security_and_privacy:interaction-with-a-speech-interface}}
\sphinxAtStartPar
An interaction with a speech interface or agent is surprisingly free of problems as long as the agent is not connected to any outside entity. We can think of the agent as a local device. If nobody else has access to that device, then all the information remains in the user’s direct control. Even if the agent exist in a remote cloud\sphinxhyphen{}service, if information remains strictly within the desired service, there are very little problems to consider.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
An exception is analysis, which the agent can perform, which is not related to the desired service, which can take abusive forms. For example, suppose the agent analyses the user’s voice for health problems and identifies that the user has \sphinxhref{https://en.wikipedia.org/wiki/Alzheimer\%27s\_disease}{Alzheimer’s disease}. What should the agent then do with that information? Not doing anthing seems unethical \sphinxhyphen{} getting early access to medical services could greatly improve life quality. Informing the user, on the other hand, involves risks. How will the user react to the information? Is the user sufficiently psychologically stable to handle it? What if the analysis is incorrect and the agent thus causes suffering? It is also easy to think of further problematic scenarios.


\subsubsection{Multi\sphinxhyphen{}user interaction with a speech interface}
\label{\detokenize{Security_and_privacy:multi-user-interaction-with-a-speech-interface}}
\sphinxAtStartPar
An agent can be involved in an interaction with multiple users at the same time.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
This scenario differs from the single\sphinxhyphen{}user case in particular in the way the device can store information. To which extent should different combinations of users be permitted to have access to information from prior interactions? Quite obviously Alice should not have default, unrestricted access to Bob’s prior interactions without Bob’s permission. Where Alice and Bob have engaged in a joint discussion, the question of access becomes more complicated. It would seem natural that both can have access to information about their prior discussions. However, if Bob is in a discussion with Eve, then access to prior discussion between Bob and Alice should again be restricted. The rules governing access will thus be complicated, often non\sphinxhyphen{}obvious and they will have many exceptions.


\subsubsection{Interaction with a speech interface in the presence of others}
\label{\detokenize{Security_and_privacy:interaction-with-a-speech-interface-in-the-presence-of-others}}
\sphinxAtStartPar
In the early days of mobile phones, a common \sphinxstyleemphasis{faux pas} was to speak loudly on the phone in public places such as on a bus or subway. It seems that it causes an uncomfortable feeling to people when they overhear private discussions. It can also be hard to ignore speech when you hear it.
Obviously, the reverse is also true, participants of a private discussion often feel uncomfortable if they fear that outsiders can hear their discussion.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The same applies to speech operated devices. Such interactions can be private, but even when they are not, the fact that they can be overheard is often uncomfortable to all parties.

\sphinxAtStartPar
This is a problem when designing user interfaces to services. Speech interaction is often a natural way to user a service or device, but it is not practical in locations where other people can overhear private information and where it the sound is annoying to other people present.


\subsubsection{Interaction with a speech interface connected to other services}
\label{\detokenize{Security_and_privacy:interaction-with-a-speech-interface-connected-to-other-services}}
\sphinxAtStartPar
When interacting with a speech interface, users typically do it with a specific objective in mind. For example, suppose Alice wants to turn off the lights in the bedroom and says “\sphinxstyleemphasis{Computer, lights off}”. To which extent is it permissible that that information is relayed to a cloud\sphinxhyphen{}service? If the local device is unable to or uncapable of deciphering the command, it can transmit it to the cloud. The cloud\sphinxhyphen{}service then obtains information from a very private part of Alice’s life.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Information obtained this way can be very useful for example, to advertisers. By analyzing the habits of users, they can serve more meaningful advertisements. Arguably, by better targeting of users, advertisement can more effective, which could \sphinxstyleemphasis{potentially} reduce the need for advertisement. It is however questionable whether advertisers ever would have incentives to \sphinxstyleemphasis{reduce} the amount of advertisement.
Still, some people are creeped out by “\sphinxstyleemphasis{overly fitting}” advertisement.

\sphinxAtStartPar
There are however plenty of other scenarios which are more potent sources of danger. What if insurance agencies analyze users life patterns and increase payments for at\sphinxhyphen{}risk users such as substance abusers? Some smart devices already today can call the emergency services if they recognize cries of help or other obvious signs of distress. What are the moral dilemmas of that?


\subsection{Multi\sphinxhyphen{}user and multi\sphinxhyphen{}device scenarios}
\label{\detokenize{Security_and_privacy:multi-user-and-multi-device-scenarios}}
\sphinxAtStartPar
Things get even more complicated when multipled users and/or users co\sphinxhyphen{}exist in the same space. Consider, for example, an open office with two users simulatenously engaged in independent video conferences.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\section{Information contained in speech signals}
\label{\detokenize{Security_and_privacy:information-contained-in-speech-signals}}
\sphinxAtStartPar
Speech signals contain a wide variety of information which are often
potentially private or even sensitive and it is difficult or impossible
to list all categories of potential information. However, information
which speech at least contains includes for example;
\begin{itemize}
\item {} 
\sphinxAtStartPar
The linguistic (text) content of a speech signal can contain almost
anything
\begin{itemize}
\item {} 
\sphinxAtStartPar
If you reveal private information about yourself in a
conversation, then clearly that information is contained in a
transcription of the conversation.

\item {} 
\sphinxAtStartPar
Word\sphinxhyphen{}choices and manners of speaking \sphinxhref{https://www.theguardian.com/lifeandstyle/2009/mar/08/language-voice}{can reveal things about the
speaker}, without the speaker realizing it himself or herself.

\end{itemize}

\item {} 
\sphinxAtStartPar
Para\sphinxhyphen{}linguistic (i.e. non\sphinxhyphen{}linguistic viz. non\sphinxhyphen{}text) content has a
wide variety of information:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Speaker identity

\item {} 
\sphinxAtStartPar
Physical traits, including gender, body size and age

\item {} 
\sphinxAtStartPar
Emotional state (happy, sad, angry, excited etc.)

\item {} 
\sphinxAtStartPar
Speaking style (public, intimate, theatrical etc.)

\item {} 
\sphinxAtStartPar
State of health, such as flu, mental health and diseases like
\sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4876915/}{Alzheimer’s},
but also including tiredness and intoxication.

\item {} 
\sphinxAtStartPar
Association and affiliation with reference groups, including
groups of gender\sphinxhyphen{}identity, ethnicity, culture background,
geographic background, nationality, political and religious
affiliations etc.

\end{itemize}

\item {} 
\sphinxAtStartPar
In interaction with other speakers:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Proof that you have met with the other speaker

\item {} 
\sphinxAtStartPar
Level of familiarity, intimacy and
\sphinxhref{https://arxiv.org/abs/2007.15711}{trust}.

\item {} 
\sphinxAtStartPar
Power structures (leader/follower/partner)

\item {} 
\sphinxAtStartPar
Family, romantic and other relationships

\end{itemize}

\end{itemize}

\sphinxAtStartPar
In other words, speech contains or can contain just about all types of
private and sensitive information you could imagine. As speech is a tool
for communication, this is not surprising; anything we can communicate
about, can be spoken. Conversely, if we find that (and we do find that)
privacy is important, then speech is among the most important signals to
protect.


\section{Types of privacy}
\label{\detokenize{Security_and_privacy:types-of-privacy}}
\sphinxAtStartPar
In a more general scope than just speech, privacy can be categorized
into seven types: {[}\hyperlink{cite.Security_and_privacy:id5}{Finn \sphinxstyleemphasis{et al.}, 2013}{]}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Privacy of the person}, which refers to the privacy with respect
to our physical body as well as any information about our body such
as fingerprints, voice characteristics and medical history.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Privacy of behaviour and action}, refers to privacy with respect
to what we do; for example, nobody needs to know what I do within
the confine of my home.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Privacy of communication}, is particularly important in speech
communication and refers to privacy of the content and meta\sphinxhyphen{}content
of communication. That is, it is not only about the literal content
of a communication, but also about the style of communication and
also about the fact that communication has happened.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Privacy of data, including images and sound}, which extends the
privacy of possessions to immaterial things. In particular, this
addresses privacy with regard to sharing information about a third
person.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Privacy of thoughts and} \sphinxstylestrong{feelings}, is paraphrasing, the
privacy of thinking. We have the right to share our thoughts and
emotions with whom we like, or to choose not to share our feelings
with anyone. This does not mean that people would have to listen to
you, but that your are allowed to offer them your thoughts.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Privacy of location and space}, has become increasingly
important, since so many of our mobile devices has the ability to
track your location. However, this type of privacy covers both your
location and location history (as in location tracking).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Privacy of association}, refers to the privacy of whether you
belong or to which extent you otherwise associate yourself to a
particular group (religious, political, ethnic, gender identity,
professional, interest groups etc.).

\end{itemize}

\sphinxAtStartPar
Note that this list does not make any claims with respect to \sphinxstyleemphasis{rights} to
these types of privacy, but that privacy\sphinxhyphen{}issues can be often be split
into these sub\sphinxhyphen{}topics. Whether someone has a right to privacy is a
society\sphinxhyphen{}level decision and political choice, where psychological and
cultural aspects play a big role.


\section{Threat and attack scenarios}
\label{\detokenize{Security_and_privacy:threat-and-attack-scenarios}}
\sphinxAtStartPar
Threats to privacy in speech communication can almost always be defined
as covert extraction of information as well as storage, processing and
usage of that information in ways of which the speaker is not aware,
and/or with which the speaker does not agree. Variability in scenarios
is then almost entirely due to the type of information involved as well
as the stakeholders. In particular,
\begin{itemize}
\item {} 
\sphinxAtStartPar
Companies can extract information from private users for unethical
advantage
\begin{itemize}
\item {} 
\sphinxAtStartPar
Insurance policies and mortgages can be denied on based on
covertly extracted information

\item {} 
\sphinxAtStartPar
The price of services can be increased for vulnerable groups

\item {} 
\sphinxAtStartPar
Access to information (search results) can be restricted and
information can be targeted (advertisement) to covertly
influence users for unethical advantage. Such behaviour is often
advertised under the pretence of customizing services to the
users preferences, but without giving the user any tools for
choosing how services are customized.

\end{itemize}

\item {} 
\sphinxAtStartPar
State operators can use surveillance and access restrictions on
their own citizens
\begin{itemize}
\item {} 
\sphinxAtStartPar
Authoritative regimes can track and eavesdrop the political
opposition, dissent and other groups such as religious, ethnic
and sexual orientations.

\item {} 
\sphinxAtStartPar
Also legitimate uses by police for surveillance in criminal
investigations

\end{itemize}

\item {} 
\sphinxAtStartPar
State operators can use surveillance on foreign citizens
\begin{itemize}
\item {} 
\sphinxAtStartPar
Spies can use speech technology for (remote) eavesdropping and
information extraction

\end{itemize}

\item {} 
\sphinxAtStartPar
Criminals can steal information and use it for their advantage
\begin{itemize}
\item {} 
\sphinxAtStartPar
Identity theft

\item {} 
\sphinxAtStartPar
Paparazzi’s can steal private information of famous people

\item {} 
\sphinxAtStartPar
Private information can potentially be used for extortion

\item {} 
\sphinxAtStartPar
Explicit content could be sold as entertainment

\end{itemize}

\item {} 
\sphinxAtStartPar
Private persons can covertly use speech technology for eavesdropping
and extracting information of other persons
\begin{itemize}
\item {} 
\sphinxAtStartPar
For example, the command history of smart speakers can give
access to past commands of all users, also when speech commands
have been made in private.

\end{itemize}

\end{itemize}


\section{Privacy and security scandals}
\label{\detokenize{Security_and_privacy:privacy-and-security-scandals}}
\sphinxAtStartPar
Most of the threats and attack scenarios are not familiar to the common
public and some of them might be too abstract to be relevant to average
users. Typically, we can hypothesize that scenarios which do not touch
directly on the life of an individual, probably do not get much
attention in the media. Topics in privacy and security which however
have received attention in the public media include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio}{Amazon workers are listening to what you tell
Alexa}
(Bloomberg, 2019). Later it was revealed that Google, Apple and
others are doing the same.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://threatpost.com/amazon-1700-alexa-voice-recordings/140201/}{Amazon Sends 1,700 Alexa Voice Recordings to a Random
Person}
(Threatpost, 2018).

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.theguardian.com/technology/2018/may/24/amazon-alexa-recorded-conversation}{Amazon’s Alexa recorded private conversation and sent it to random
contact}
(The Guardian, 2018).

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402}{Fraudsters Used AI to Mimic CEO’s Voice in Unusual Cybercrime
Case}
(The Wall Street Journal, 2019).

\end{itemize}

\sphinxAtStartPar
Note that the fact that many of the above examples are related to
Amazon/Alexa is probably more coincidence than an indication that Alexa
would treat privacy differently than its competitors.


\section{Approaches for safeguarding privacy in and improving usability of speech technology}
\label{\detokenize{Security_and_privacy:approaches-for-safeguarding-privacy-in-and-improving-usability-of-speech-technology}}

\subsection{Basic design concepts}
\label{\detokenize{Security_and_privacy:basic-design-concepts}}
\sphinxAtStartPar
At least from the European perspective, the following design concepts
are seen as basis of good design for privacy. In fact, they are mandated
by the General Data Protection Regulations of the European Union.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Privacy by default}; systems should always default to the least
invasive configuration. Additional services, which are more
invasive, can be chosen (opt\sphinxhyphen{}in) if the user so chooses.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Privacy by design}; privacy should not be an after\sphinxhyphen{}thought but
systems should be designed for privacy. The overall systems
structure should be chosen such that it supports privacy.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Data minimization}; data can be extracted from users only the
extent explicitly required by the system. For example, if you order
a pizza through a web\sphinxhyphen{}portal, you need to tell which pizza you want
and where it should be delivered. Otherwise you cannot receive the
service. In other words, all services require some level of
information transfer, but the services cannot ask about information
which is irrelevant to the service. For example, the pizza service
cannot ask you about your gender\sphinxhyphen{}identity. Data minimization cab
also be interpreted as \sphinxstyleemphasis{data ecology}, which would underline the
fact that “more data” usually means also a higher consumption of
energy and other resources. In fact, data could (or should) be
treated as a natural resource itself. This line of argumentation
connects privacy with the \sphinxhref{https://www.un.org/development/desa/disabilities/envision2030.html}{UN sustainability
goals}.An important aspect of data minimization is that information should
stored only as long as it is necessary for the service.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Informed and meaningful consent}; when the user chooses a service,
he should receive information about its implications to privacy in
an understandable and accessible way, and the service provider
should receive the users consent without any form of coercion.

\end{itemize}

\sphinxAtStartPar
Typically it is reasonable that service providers have access to
aggregated data such as ensemble averages, but not to information about
individuals. For example, in a hypothetical case, a smart speaker
operator could receive the information that 55\% of users are male, but
would not get the gender of any individual user.


\subsection{Definitions}
\label{\detokenize{Security_and_privacy:definitions}}
\sphinxAtStartPar
A central problematic in privacy with speech signals is the concept of
“uniquely identifiable”. Legal frameworks such as the GDPR state that
private information is such data where individual users are “uniquely
identifiable”, but there is no accurate definition of what it really
means. If your partner recognizes your “Hello” on the phone, it means
that for her, your “Hello” is uniquely identifiable. However, if you
give 10.000 speech samples to your partner, one of which is your
“Hello”, then there’s a significant likelihood that your partner would
not find your “Hello” from the pile. An unanswered question is thus,
“What is the size of the group where a user should be uniquely
identifiable?”.

\sphinxAtStartPar
A more detailed aspect is that of significance. The speaker recognition
approach is to find the most likely speaker, out of the reference group
of size \sphinxstyleemphasis{N}, whereas speaker verification tries to determine whether we,
within some confidence intervals, can be sure that you are who you claim
you are. In engineering terms, this means that we want to find the
speaker with the highest likelihood, but with a sufficient margin to all
other speakers. In the opposite direction, we can also use a lower
threshold; we could say that statistically significant correlation
already exposes the users privacy. For example, if we find that the
speaker is either you or your father/mother, then we have a significant
statistical correlation, but your are not uniquely identified.

\sphinxAtStartPar
A further consideration is that of adjoining data; Suppose there is a
recording of a speaker A, and that you happen to know a speaker A very
well. Then it will be easy for you to recognize the voice of A in that
recording. That is, you have a lot of experience (stored data) about how
A sounds, therefore it is easy for you to identify A. Does that mean
that A is uniquely identifiable in that recording? After all, A would
not be identifiable if you did not know A (= if you would not have
prior, stored data about A).A slight variation of the above case is a recording of a speaker B,
where B is relatively famous public person, such that there are readily
available sound samples of his voice on\sphinxhyphen{}line. Does that make the
recording of B uniquely identifiable? Or if there is a recording of a
currently non\sphinxhyphen{}famous person C, who later becomes famous. Does that
change the status of the recording of C to uniquely identifiable?

\sphinxAtStartPar
Today, this question remains unanswered and we have no commonly agreed
interpretation of what “uniquely identifiable” really means. What level
of statistical confidence is assumed? What level of adjoining data is
assumed (in terms of GDPR probably: any and all data which exists)? Can
it change over time if new information becomes public (probably: yes)?
Can it change over time if new technologies are developed (probably:
yes)?


\subsection{Local/edge processing}
\label{\detokenize{Security_and_privacy:local-edge-processing}}
\sphinxAtStartPar
Privacy is an issue only if some other party has access to data about
you. Data which resides on a device which is in your control is
therefore relatively safe, assuming that no outsider has access to that
device. If data is sent to a cloud server then there are more entities
which could potentially have access to your data. Therefore all storage
and processing which can be done on your local device is usually by
design more private than any cloud server.

\sphinxAtStartPar
Observe that this does not protect you from other local users. For
example, if multiple persons are using one smart speaker at home, then
the other users could have access to information about you through that
device and any connected other devices.

\sphinxAtStartPar
Central limitations of edge processing are
\begin{itemize}
\item {} 
\sphinxAtStartPar
Many services require outside access; say if you ask your phone
“What’s the weather tomorrow?”, the phone cannot know that by
itself, but has to retrieve the information from a cloud server. The
essential content of your speech is therefore relayed to the cloud
and you don’t have much benefit from edge processing.

\item {} 
\sphinxAtStartPar
Improving voice operated services requires a lot of data. Moreover,
data which reflects features of actual users is much better than any
simulations. Service providers thus argue that they need to collect
data from users to provide high\sphinxhyphen{}quality services, and local
processing could prevent the services providers from getting that
data.

\item {} 
\sphinxAtStartPar
Edge devices would use their full capacity only when they pick up
speech in their microphone, which means that most of the time, edge
devices would lie dormant, waiting for speech commands. This is a
wasteful use of resources; a cloud server can better balance the
load because with a large number of users, the resource requirements
would likely be more stable.

\end{itemize}


\subsection{Differential privacy}
\label{\detokenize{Security_and_privacy:differential-privacy}}
\sphinxAtStartPar
Even when operating with aggregate data, like the mean user age, it is
still possible to extract private information in some scenarios. For
example, if we know the mean user age and the number of users at a time
\(t\), and we also know that the age of user \(X\) was added to the mean at
time \(t+1\), as well as the mean user age at \(t+1\), then we can deduce
the age of user \(X\) with basic algebra. As a safeguard against such
differential attacks, to provide \sphinxhref{https://en.wikipedia.org/wiki/Differential\_privacy}{differential
privacy}, it is
possible to add noise to any data transfers. Individual data points are
then obfuscated and cannot be exactly recovered. However, the ensemble
average can still be deduced if the distribution of the added noise is
known.

\sphinxAtStartPar
The required compromise here is that the level of privacy corresponds to
amount of noise, which is inversely proportional to the accuracy of the
ensemble mean. That is, if the amount of noise is large, then we need a
huge number of users to determine an accurate ensemble average. On the
other hand, if the amount of noise is small, then we can get a fair
guess of an individual data point, but also the ensemble average is
accurate.


\subsection{Federated learning}
\label{\detokenize{Security_and_privacy:federated-learning}}
\sphinxAtStartPar
To enable machine learning in the cloud without the need to provide
access to private data, we can use \sphinxhref{https://en.wikipedia.org/wiki/Federated\_learning}{federated
learning}, where
private data remains on the local device, but only model updates are
sent to the cloud. Clearly this is approach has better privacy than one
where all private data is sent to the cloud. However, currently we do
not yet have clear understanding of the extent of privacy with this type
of methods; some data is sent to the cloud, but can some private data
still be traced back to the user?


\subsection{Homomorphic encryption}
\label{\detokenize{Security_and_privacy:homomorphic-encryption}}
\sphinxAtStartPar
Suppose a service provider has a proprietary model, say an analysis
method for Alzheimer’s disease from the voice, and your doctor would
like to analyse your voice with that method. Naturally your voice is
also private, so you do not want to send your voice to the third\sphinxhyphen{}party
service provider, but also the service provider does not want to send
the model to you. \sphinxhref{https://en.wikipedia.org/wiki/Homomorphic\_encryption}{Homomorphic
encryption}
provides a method for applying the secret model on \sphinxstyleemphasis{encrypted} data,
such that you have to only send your data in an encrypted form to the
service provider. Your doctor would then receive only the final
diagnosis, but not the model nor your speech data. The concept is in
principle beautiful, it solves the problem of mutual distrust very
nicely. However, the compromise is that currently available homomorphic
encryption methods require that all processing functions can be written
as polynomial functions. In theory, we can transform any function to a
corresponding polynomial, but the increase in complexity is often
dramatic. Consequently, privacy\sphinxhyphen{}preserving methods based on homomorphic
encryption typically have a prohibitively high computational complexity.


\subsection{myData}
\label{\detokenize{Security_and_privacy:mydata}}
\sphinxAtStartPar
In addition to privacy\sphinxhyphen{}preserving algorithms, we can also design
privacy\sphinxhyphen{}preserving architectures. The \sphinxhref{https://mydata.org/}{myData} paradigm is based on a
three\sphinxhyphen{}tier design, where the user can choose where all his/her data is
stored and where the user can give access for service providers to
his/her data when required. The idea is to separate service providers
from data storage, such that users have better control over his/her
data. To transform existing services to adhere with the myData concept
requires that new storage services for private data are created and that
APIs between storage and processing services are specified.

\sphinxAtStartPar
Note that, if a user chooses to store private data on a cloud\sphinxhyphen{}server,
then it is still susceptible for abuse by the storage\sphinxhyphen{}service\sphinxhyphen{}provider,
unless appropriate encryption methods are used. However, the user could
in principle choose to store private data on a edge device, such that
the storage\sphinxhyphen{}service\sphinxhyphen{}provider is cut out of the loop.

\sphinxAtStartPar
A further risk is that in the myData concept, we usually assume that
data is stored at a single central location, which becomes a central
point of weakness. Should someone gain illegitimate access to the
storage, then all your data would be compromised. Distributing data to
several different storage locations might therefore be reasonable.


\section{Design goals, human computer interfaces and user experience}
\label{\detokenize{Security_and_privacy:design-goals-human-computer-interfaces-and-user-experience}}
\sphinxAtStartPar
A common prejudice is that privacy and security requirements cause
problems for developers and make systems more difficult for users to
use. Such prejudice are unfortunate and patently misguided. The problem
is that many privacy problems are not visible to the casual observer and
their effects become apparent only when it already is too late. Another
argument is “\sphinxstyleemphasis{privacy is} \sphinxstyleemphasis{not my concern} \sphinxstyleemphasis{because I haven’t seen any
privacy problems}”, which is like saying that “\sphinxstyleemphasis{rape is not my concern
because I haven’t seen any rapes”.} This is thus an absurd argument.
Privacy safeguards are meant to protect users and developers from \sphinxhref{https://www.reuters.com/article/us-apple-cyber-idUSKCN1VR29K}{very bad
consequences}.
These problems are real. \sphinxstyleemphasis{You} cannot ignore them.

\sphinxAtStartPar
However, designing for privacy is also \sphinxstyleemphasis{not only} about protection of
users. It is also very much about designing technology which is \sphinxstyleemphasis{easy to
use} and where the user experience feels intuitive and natural. For
example, speaker recognition can be used to grant access to voice
technology such that the user does not have to be bothered with
passwords, PIN\sphinxhyphen{}codes or other cumbersome authentication methods. Overall
speech technology promises to give access to services without the need
scroll through menus on your washing machine to find that one mode which
is optimized for white curtains made out of cotton.

\sphinxAtStartPar
The overall design goal could be that people should be able to \sphinxstyleemphasis{trust}
the system. In particular, a trustworthy system will be
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{consistent}; It does what it says, and it says what it does.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{competent}; It is able to do what it should do and what it says it
does.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{benevolent}; It cares for the user and it shows that in both \sphinxstyleemphasis{what}
is says and \sphinxstyleemphasis{how} it says it.

\end{itemize}

\sphinxAtStartPar
These goals are best illustrated by examples;
\begin{itemize}
\item {} 
\sphinxAtStartPar
We should avoid cognitive dissonance; if the computer has the
intellectual capacity corresponding to a 3\sphinxhyphen{}year\sphinxhyphen{}old child, but if it
then speaks with the authoritative voice of a middle\sphinxhyphen{}aged person, it
is giving mixed signals which can confuse users. Similarly, if a
computer leaks out all your information to the world, while speaking
with an intimate voice, it is also giving the wrong impression to
the user. Conversely, the intuitive impression which a device gives
should be consistent with its true nature.

\item {} 
\sphinxAtStartPar
Consistency is extremely important and one mistake can be very
costly; If your friend Greg once tells about your intimate
health\sphinxhyphen{}incident to your friends, it casts a shadow of doubt over all
future and all of the past 10 years. Did he already earlier tell my
secrets to them? Who else has he told my secrets to? It takes a very
long time to patch such breaches of trust.

\end{itemize}


\section{Ethical dilemmas}
\label{\detokenize{Security_and_privacy:ethical-dilemmas}}
\sphinxAtStartPar
The following is a list of hypothetical questions which can (and do)
arise in the design of speech operated systems:
\begin{itemize}
\item {} 
\sphinxAtStartPar
If a device hears cries of help, should it call the police
automatically, even when it breaches privacy and the trust of the
user?

\item {} 
\sphinxAtStartPar
If a device hears indications of domestic abuse, but has no direct
evidence, should it call the police? What level of evidence is
required?

\item {} 
\sphinxAtStartPar
If a device or service recognizes that you have Alzheimer’s or some
other serious illness, should it tell it to you? Even if your doctor
would not yet have noticed it? Even if the diagnosis might be
incorrect? Even if you might choose not to want to hear it? Even if
you’d have a history of depression and such bad news might trigger
suicidal thoughts?

\item {} 
\sphinxAtStartPar
Suppose you have been dreaming of buying a new bicycle, but haven’t
told anyone. Your smart TV, though, knows about it because you have
searched for information about that bicycle. Suppose that your
spouse is simultaneously trying to come up with a nice surprise
birthday present. Should your smart TV suggest to your spouse that
she buys the bicycle?

\item {} 
\sphinxAtStartPar
Suppose your local cafeteria has automatic speech operated ordering
of drinks. On this day, last year, you bought a birthday\sphinxhyphen{}surprise
drink from this cafeteria. Should the computer remember that and
congratulate you today about your birthday?

\item {} 
\sphinxAtStartPar
The better your services know you, the better they could serve you.
That’s undoubtedly a fact. (The fact that current services are not
optimal is not a contradiction.) However, do you want to have
privacy from devices in the same way you have privacy from your
friends. For example, your friends do not follow you to the toilet
or the bedroom; is it ok if your device does that?

\end{itemize}


\section{Security and privacy in speech research}
\label{\detokenize{Security_and_privacy:security-and-privacy-in-speech-research}}
\sphinxAtStartPar
Scientific research is based on arguments supported by evidence, where
evidence, in the speech sciences, is recordings of speech. Access to
speech data is therefore a mandatory part of research in the speech
sciences. To obtain trustworthy results, independent researchers have to
be able to verify each others results, which means that they have to
have access to the same or practically identical data sources. Shared
data is the gold standard for \sphinxhref{https://en.wikipedia.org/wiki/Reproducibility}{reproducible
research}. However, the
sharing of speech data can be problematic with respect to speaker
privacy.

\sphinxAtStartPar
The concept of “uniquely identifiable” is here the key. If an individual
is \sphinxstyleemphasis{not} uniquely identifiable in a data set, then you are allowed to
share that data. Conversely, if you remove all identifying data, then
you can share data relatively freely. However, in perspective of the
discussions above, it should be clear that it is not clear what
constitutes identifying data nor is it clear what makes that data
“uniquely” identifying.

\sphinxAtStartPar
A second important consideration is \sphinxstyleemphasis{consent}. The persons whose voices
are recorded must be allowed to choose freely whether they want to
participate and that choice has to be explicit; you need to ask them
clearly whether they want to participate in a recording. The research
needs to be able to prove that consent has been given, and therefore
that consent must be documented carefully. Consent must also be given
freely such that there are no explicit or hidden penalties of rejecting
consent. Furthermore, if any uniquely identifying data of a participant
is stored, then the participant must be allowed to withdraw consent
afterwards. There are however some important exemptions to this rule;
the right to withdraw consent can be rejected, for example, if that
would corrupt the integrity of the data set, such as
\begin{itemize}
\item {} 
\sphinxAtStartPar
If withdrawal of a participant could bias the results, then that
\sphinxstyleemphasis{could} be grounds for denying withdrawal. For example, if a dataset
is constructed in a way that it represents a balanced subset of the
population (the amount of say, males and females, different age,
cultural and education backgrounds are chosen to match the general
population), then we cannot remove any participants without
corrupting the distribution. Moreover, people more educated in
questions of privacy could hypothetically be more prone to
withdrawing their consent, such that the population becomes biased.

\item {} 
\sphinxAtStartPar
If withdrawal of a participant could jeopardize the reproducibility
of results, then that \sphinxstyleemphasis{could} be grounds for denying withdrawal. For
example, if a machine learning algorithm is used on a dataset, then
we can recreate that algorithm only if we have \sphinxstyleemphasis{exactly} the same
data available. This is especially problematic if the dataset is
relatively small, where small changes in the dataset can have big
consequences on the output.

\end{itemize}

\sphinxAtStartPar
To allow plausible grounds for denying the right to withdraw consent,
datasets can then be designed to be either balanced or relatively small.
Collecting balanced datasets is good practice in any case, such that
this is not a limitation but can actually improve quality. Conversely,
good data is balanced and that should be our goal; A consequence is that
we \sphinxstyleemphasis{might be forced} to deny the right to withdraw consent. Avoiding the
collection of excessively large data sets is also good from the
perspective of \sphinxstyleemphasis{data minimization} and data ecology.

\sphinxAtStartPar
In a request for consent, the data collector should state the purpose of
the dataset (i.e. \sphinxstyleemphasis{purpose binding}). For instance, a dataset could be
collected for development of wake\sphinxhyphen{}word detection methods and consent is
received for that purpose. Then \sphinxstyleemphasis{it is not permissible} to use the same
data set for speaker detection experiments or medical analysis of the
voice. Period. It is therefore good practice to ask for consent in a
sufficiently wide way, such that researchers have some flexibility in
using the data. Blanket consent to all research purposes is however not
good practice. In particular, it is recommend that processing of
sensitive information such as health, ethnic, political information is
excluded if it is not the express purpose of the dataset (cf. data
minimization).

\sphinxAtStartPar
If a dataset by nature does include uniquely identifiable data, then the
researchers need to apply stronger layers of safeguards. In particular,
typically researchers have to keep track of who has access to the data,
to ensure purpose binding and to allow withdrawal of consent. This could
also require that any researcher who downloads the data signs a contract
with the data provider, where the terms of usage are defined. Such a
contract can be required in any case, not only with uniquely
identifiable data.

\sphinxAtStartPar
Data such as medical information, data about children or other exposed
groups, political, religious and gender\sphinxhyphen{}identity affiliations etc. are
particularly \sphinxstyleemphasis{sensitive}. If your dataset contains \sphinxstyleemphasis{any} such
information, then you have to apply stronger safeguards. To begin with,
access to such data has to be, in practice, always limited to only
persons who are included in a legally binding contract specifying access
rights and allowable uses, processing and storage.

\sphinxAtStartPar
As an overall principle, note that the principal investigator (research
group leader) is legally responsible for the use of the data that is
collected, stored and processed. In particular, if a third party
downloads the data and misuses it, for example by analysing health
information even if no consent has been acquired for that purpose, then
it is the principal investigator who is responsible. However, the
principal investigator is only required to apply \sphinxstyleemphasis{reasonable}
\sphinxstyleemphasis{safeguards} to ensure that data is not misused. What level of
safeguards are sufficient has however not yet been agreed. It is likely
that there will never be rules which specify exactly a sufficient level
of safeguards.

\sphinxAtStartPar
In the above discussion it has become clear that the nature of \sphinxstyleemphasis{unique
identifiability} can change over time, when new information is published
and new technologies emerge. This means that datasets which previously
were adequately protected, over time become exposed to privacy problems.
It is therefore important that researchers monitor their published
datasets over time such that if new threats emerge, they can take
appropriate action. For example, they could withdraw an dataset
entirely. Reasonable ways for implementing this could be:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Expiry date}; All datasets should have a clearly stated shelf\sphinxhyphen{}life
and use of the dataset after the expiry date should be prohibited.
The manager of the dataset could update the expiry date if no new
threats are discovered. Academic publications based on expired
datasets should not be accepted.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Controlled access to datasets}; To enforce purpose binding and to
enable withdrawal of datasets, the data manager can require that all
users are registered and sign a formal contract which specifies
accepted uses.

\end{itemize}

\sphinxAtStartPar
As a last resort, when data is so sensitive and private that it cannot
be publicly released, it is possible to require on\sphinxhyphen{}site processing of
data. For example, you can design a computing architecture, where data
resides on a secure server, to which researcher have access through a
secure
\sphinxhref{https://en.wikipedia.org/wiki/Application\_programming\_interface}{API}.
Data never leaves the server such that privacy is always preserved. For
an even higher level of security, data can be stored on an
\sphinxhref{https://en.wikipedia.org/wiki/Air\_gap\_\%28networking\%29}{air\sphinxhyphen{}gapped}
computer system, which means that access to the data requires that
researchers physically come to the computer (no network access). This
level of security is usually the domain of military\sphinxhyphen{}grade systems.


\section{References}
\label{\detokenize{Security_and_privacy:references}}\begin{itemize}
\item {} 
\sphinxAtStartPar
ISCA Special Interest Group “Security and Privacy in Speech
Communication”, \sphinxurl{https://www.spsc-sig.org/}

\end{itemize}

\sphinxstepscope


\chapter{References}
\label{\detokenize{References:references}}\label{\detokenize{References::doc}}
\begin{sphinxthebibliography}{Backstro}
\bibitem[Nol75]{Representations/Waveform:id44}
\sphinxAtStartPar
Peter Noll. A comparative study of various quantization schemes for speech encoding. \sphinxstyleemphasis{Bell System Technical Journal}, 54(9):1597–1614, 1975. URL: \sphinxurl{https://doi.org/10.1002/j.1538-7305.1975.tb02053.x}.
\bibitem[BackstromLF+17]{Preprocessing/Pre-emphasis:id43}
\sphinxAtStartPar
Tom Bäckström, Jérémie Lecomte, Guillaume Fuchs, Sascha Disch, and Christian Uhle. \sphinxstyleemphasis{Speech coding: with code\sphinxhyphen{}excited linear prediction}. Springer, 2017. URL: \sphinxurl{https://doi.org/10.1007/978-3-319-50204-5}.
\bibitem[ZSLC17]{Recognition/Wake-word_and_keyword_spotting:id45}
\sphinxAtStartPar
Yundong Zhang, Naveen Suda, Liangzhen Lai, and Vikas Chandra. Hello edge: keyword spotting on microcontrollers. \sphinxstyleemphasis{arXiv preprint arXiv:1711.07128}, 2017. URL: \sphinxurl{https://doi.org/10.48550/arXiv.1711.07128}.
\bibitem[Zha19]{Recognition/Speech_Recognition:id46}
\sphinxAtStartPar
Xiaohui Zhang. \sphinxstyleemphasis{Strategies for Handling Out\sphinxhyphen{}of\sphinxhyphen{}Vocabulary Words in Automatic Speech Recognition}. PhD thesis, Johns Hopkins University, 2019. URL: \sphinxurl{http://jhir.library.jhu.edu/handle/1774.2/62275}.
\bibitem[BZMA20]{Recognition/Paralinguistic_speech_processing:id12}
\sphinxAtStartPar
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: a framework for self\sphinxhyphen{}supervised learning of speech representations. \sphinxstyleemphasis{Advances in Neural Information Processing Systems}, 33:12449–12460, 2020. URL: \sphinxurl{https://doi.org/10.48550/arXiv.2006.11477}.
\bibitem[BGV92]{Recognition/Paralinguistic_speech_processing:id10}
\sphinxAtStartPar
Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal margin classifiers. In \sphinxstyleemphasis{Proceedings of the fifth annual workshop on Computational learning theory}, 144–152. 1992. URL: \sphinxurl{https://doi.org/10.1145/130385.130401}.
\bibitem[CHTG19]{Recognition/Paralinguistic_speech_processing:id11}
\sphinxAtStartPar
Yu\sphinxhyphen{}An Chung, Wei\sphinxhyphen{}Ning Hsu, Hao Tang, and James Glass. An unsupervised autoregressive model for speech representation learning. \sphinxstyleemphasis{arXiv preprint arXiv:1904.03240}, 2019. URL: \sphinxurl{https://doi.org/10.48550/arXiv.1904.03240}.
\bibitem[EWollmerS10]{Recognition/Paralinguistic_speech_processing:id15}
\sphinxAtStartPar
Florian Eyben, Martin Wöllmer, and Björn Schuller. Opensmile: the munich versatile and fast open\sphinxhyphen{}source audio feature extractor. In \sphinxstyleemphasis{Proceedings of the 18th ACM international conference on Multimedia}, 1459–1462. 2010. URL: \sphinxurl{https://doi.org/10.1145/1873951.1874246}.
\bibitem[PRasanenK15]{Recognition/Paralinguistic_speech_processing:id14}
\sphinxAtStartPar
Jouni Pohjalainen, Okko Räsänen, and Serdar Kadioglu. Feature selection methods and their combinations in high\sphinxhyphen{}dimensional classification of speaker likability, intelligibility and personality traits. \sphinxstyleemphasis{Computer Speech \& Language}, 29(1):145–171, 2015. URL: \sphinxurl{https://doi.org/10.1016/j.csl.2013.11.004}.
\bibitem[SB13]{Recognition/Paralinguistic_speech_processing:id13}
\sphinxAtStartPar
Björn Schuller and Anton Batliner. \sphinxstyleemphasis{Computational paralinguistics: emotion, affect and personality in speech and language processing}. John Wiley \& Sons, 2013. URL: \sphinxurl{https://www.wiley.com/en-us/9781118706626}.
\bibitem[WVBWK15]{Recognition/Paralinguistic_speech_processing:id8}
\sphinxAtStartPar
Zhizheng Wu, Cassia Valentini\sphinxhyphen{}Botinhao, Oliver Watts, and Simon King. Deep neural networks employing multi\sphinxhyphen{}task learning and stacked bottleneck features for speech synthesis. In \sphinxstyleemphasis{2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)}, 4460–4464. IEEE, 2015. URL: \sphinxurl{https://doi.org/10.1109/ICASSP.2015.7178814}.
\bibitem[HB96]{Synthesis/Concatenative_speech_synthesis:id54}
\sphinxAtStartPar
Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In \sphinxstyleemphasis{1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings}, volume 1, 373–376. IEEE, 1996. URL: \sphinxurl{https://doi.org/10.1109/ICASSP.1996.541110}.
\bibitem[RS07]{Synthesis/Concatenative_speech_synthesis:id55}
\sphinxAtStartPar
Lawrence R Rabiner and Ronald W Schafer. Introduction to digital speech processing. \sphinxstyleemphasis{Foundations and Trends in Signal Processing}, 1(1):1–194, 2007. URL: \sphinxurl{https://doi.org/10.1561/2000000001}.
\bibitem[BackstromLF+17]{Transmission/Design_goals:id43}
\sphinxAtStartPar
Tom Bäckström, Jérémie Lecomte, Guillaume Fuchs, Sascha Disch, and Christian Uhle. \sphinxstyleemphasis{Speech coding: with code\sphinxhyphen{}excited linear prediction}. Springer, 2017. URL: \sphinxurl{https://doi.org/10.1007/978-3-319-50204-5}.
\bibitem[BackstromLF+17]{Transmission/Modified_discrete_cosine_transform_MDCT:id43}
\sphinxAtStartPar
Tom Bäckström, Jérémie Lecomte, Guillaume Fuchs, Sascha Disch, and Christian Uhle. \sphinxstyleemphasis{Speech coding: with code\sphinxhyphen{}excited linear prediction}. Springer, 2017. URL: \sphinxurl{https://doi.org/10.1007/978-3-319-50204-5}.
\bibitem[BSH+08]{Enhancement/Noise_attenuation:id41}
\sphinxAtStartPar
Jacob Benesty, M Mohan Sondhi, Yiteng Huang, and others. \sphinxstyleemphasis{Springer handbook of speech processing}. Volume 1. Springer, 2008. URL: \sphinxurl{https://doi.org/10.1007/978-3-540-49127-9}.
\bibitem[Bol79]{Enhancement/Noise_attenuation:id42}
\sphinxAtStartPar
Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. \sphinxstyleemphasis{IEEE Transactions on acoustics, speech, and signal processing}, 27(2):113–120, 1979. URL: \sphinxurl{https://doi.org/10.1109/TASSP.1979.1163209}.
\bibitem[Mar01]{Enhancement/Noise_attenuation:id40}
\sphinxAtStartPar
Rainer Martin. Noise power spectral density estimation based on optimal smoothing and minimum statistics. \sphinxstyleemphasis{IEEE Transactions on speech and audio processing}, 9(5):504–512, 2001. URL: \sphinxurl{https://doi.org/10.1109/89.928915}.
\bibitem[Bir05]{Computational_models_of_human_language_processing:id54}
\sphinxAtStartPar
Peter Birkholz. \sphinxstyleemphasis{3D\sphinxhyphen{}Artikulatorische Sprachsynthese}. PhD thesis, der Universität Rostock, 2005. URL: \sphinxurl{https://www.vocaltractlab.de/publications/birkholz-2005-dissertation.pdf}.
\bibitem[BMW+15]{Computational_models_of_human_language_processing:id53}
\sphinxAtStartPar
Peter Birkholz, Lucia Martin, Klaus Willmes, Bernd J Kröger, and Christiane Neuschaefer\sphinxhyphen{}Rube. The contribution of phonation type to the perception of vocal emotions in german: an articulatory synthesis study. \sphinxstyleemphasis{The Journal of the Acoustical Society of America}, 137(3):1503–1512, 2015. URL: \sphinxurl{https://doi.org/10.1121/1.4906836}.
\bibitem[Dup18]{Computational_models_of_human_language_processing:id52}
\sphinxAtStartPar
Emmanuel Dupoux. Cognitive science in the era of artificial intelligence: a roadmap for reverse\sphinxhyphen{}engineering the infant language\sphinxhyphen{}learner. \sphinxstyleemphasis{Cognition}, 173:43–59, 2018. URL: \sphinxurl{https://doi.org/10.1016/j.cognition.2017.11.008}.
\bibitem[HBR17]{Computational_models_of_human_language_processing:id51}
\sphinxAtStartPar
William Havard, Laurent Besacier, and Olivier Rosec. Speech\sphinxhyphen{}coco: 600k visually grounded spoken captions aligned to mscoco data set. \sphinxstyleemphasis{arXiv preprint arXiv:1707.08435}, 2017. URL: \sphinxurl{https://doi.org/10.21437/GLU.2017-9}.
\bibitem[HM14]{Computational_models_of_human_language_processing:id50}
\sphinxAtStartPar
Ian S Howard and Piers Messum. Learning to pronounce first words in three languages: an investigation of caregiver and infant behavior using a computational model of an infant. \sphinxstyleemphasis{PLoS One}, 9(10):e110334, 2014. URL: \sphinxurl{https://doi.org/10.1371/journal.pone.0110334}.
\bibitem[KRasanen16]{Computational_models_of_human_language_processing:id49}
\sphinxAtStartPar
Sofoklis Kakouros and Okko Räsänen. 3pro–an unsupervised method for the automatic detection of sentence prominence in speech. \sphinxstyleemphasis{Speech Communication}, 82:67–84, 2016. URL: \sphinxurl{https://doi.org/10.1016/j.specom.2016.06.004}.
\bibitem[KJG17]{Computational_models_of_human_language_processing:id48}
\sphinxAtStartPar
Herman Kamper, Aren Jansen, and Sharon Goldwater. A segmental framework for fully\sphinxhyphen{}unsupervised large\sphinxhyphen{}vocabulary speech recognition. \sphinxstyleemphasis{Computer Speech \& Language}, 46:154–174, 2017. URL: \sphinxurl{https://doi.org/10.1016/j.csl.2017.04.008}.
\bibitem[Kir02]{Computational_models_of_human_language_processing:id47}
\sphinxAtStartPar
Simon Kirby. Natural language from artificial life. \sphinxstyleemphasis{Artificial life}, 8(2):185–215, 2002. URL: \sphinxurl{https://doi.org/10.1162/106454602320184248}.
\bibitem[Mae88]{Computational_models_of_human_language_processing:id43}
\sphinxAtStartPar
Shinji Maeda. Improved articulatory models. \sphinxstyleemphasis{The Journal of the Acoustical Society of America}, 84(S1):S146–S146, 1988. URL: \sphinxurl{https://doi.org/10.1121/1.2025845}.
\bibitem[MYL+20]{Computational_models_of_human_language_processing:id45}
\sphinxAtStartPar
James S Magnuson, Heejo You, Sahil Luthra, Monica Li, Hosung Nam, Monty Escabi, Kevin Brown, Paul D Allopenna, Rachel M Theodore, Nicholas Monto, and others. Earshot: a minimal neural network model of incremental human speech recognition. \sphinxstyleemphasis{Cognitive science}, 44(4):e12823, 2020. URL: \sphinxurl{https://doi.org/10.1111/cogs.12823}.
\bibitem[Mar82]{Computational_models_of_human_language_processing:id46}
\sphinxAtStartPar
David Marr. \sphinxstyleemphasis{Vision: A computational investigation into the human representation and processing of visual information}. W.H. Freeman and Company, 1982.
\bibitem[MWG02]{Computational_models_of_human_language_processing:id42}
\sphinxAtStartPar
Jessica Maye, Janet F Werker, and LouAnn Gerken. Infant sensitivity to distributional information can affect phonetic discrimination. \sphinxstyleemphasis{Cognition}, 82(3):B101–B111, 2002. URL: \sphinxurl{https://doi.org/10.1016/S0010-0277(01)00157-3}.
\bibitem[ME86]{Computational_models_of_human_language_processing:id44}
\sphinxAtStartPar
James L McClelland and Jeffrey L Elman. The trace model of speech perception. \sphinxstyleemphasis{Cognitive psychology}, 18(1):1–86, 1986. URL: \sphinxurl{https://doi.org/10.1016/0010-0285(86)90015-0}.
\bibitem[NSM15]{Computational_models_of_human_language_processing:id41}
\sphinxAtStartPar
Tasha Nagamine, Michael L Seltzer, and Nima Mesgarani. Exploring how deep neural networks form phonemic categories. In \sphinxstyleemphasis{Sixteenth Annual Conference of the International Speech Communication Association}. 2015. URL: \sphinxurl{https://www.isca-speech.org/archive\_v0/interspeech\_2015/papers/i15\_1912.pdf}.
\bibitem[Nor94]{Computational_models_of_human_language_processing:id40}
\sphinxAtStartPar
Dennis Norris. Shortlist: a connectionist model of continuous speech recognition. \sphinxstyleemphasis{Cognition}, 52(3):189–234, 1994. URL: \sphinxurl{https://doi.org/10.1016/0010-0277(94)90043-4}.
\bibitem[OKS19]{Computational_models_of_human_language_processing:id39}
\sphinxAtStartPar
Pierre\sphinxhyphen{}Yves Oudeyer, George Kachergis, and William Schueller. Computational and robotic models of early language development: a review. In J.S. Horst and J. von Koss Torkildsen, editors, \sphinxstyleemphasis{International handbook of language acquisition}. Routledge/Taylor \& Francis Group, 2019. URL: \sphinxurl{https://psycnet.apa.org/doi/10.4324/9781315110622-5}.
\bibitem[RRasanen17]{Computational_models_of_human_language_processing:id36}
\sphinxAtStartPar
Heikki Rasilo and Okko Räsänen. An online model for vowel imitation learning. \sphinxstyleemphasis{Speech Communication}, 86:1–23, 2017. URL: \sphinxurl{https://doi.org/10.1016/j.specom.2016.10.010}.
\bibitem[Rasanen12]{Computational_models_of_human_language_processing:id38}
\sphinxAtStartPar
Okko Räsänen. Computational modeling of phonetic and lexical learning in early language acquisition: existing models and future directions. \sphinxstyleemphasis{Speech Communication}, 54(9):975–997, 2012. URL: \sphinxurl{https://doi.org/10.1016/j.specom.2012.05.001}.
\bibitem[RasanenDF18]{Computational_models_of_human_language_processing:id35}
\sphinxAtStartPar
Okko Räsänen, Gabriel Doyle, and Michael C Frank. Pre\sphinxhyphen{}linguistic segmentation of speech into syllable\sphinxhyphen{}like units. \sphinxstyleemphasis{Cognition}, 171:130–150, 2018. URL: \sphinxurl{https://doi.org/10.1016/j.cognition.2017.11.003}.
\bibitem[RasanenR15]{Computational_models_of_human_language_processing:id37}
\sphinxAtStartPar
Okko Räsänen and Heikki Rasilo. A joint model of word segmentation and meaning acquisition through cross\sphinxhyphen{}situational learning. \sphinxstyleemphasis{Psychological review}, 122(4):792, 2015. URL: \sphinxurl{https://psycnet.apa.org/doi/10.1037/a0039702}.
\bibitem[SAN96]{Computational_models_of_human_language_processing:id34}
\sphinxAtStartPar
Jenny R Saffran, Richard N Aslin, and Elissa L Newport. Statistical learning by 8\sphinxhyphen{}month\sphinxhyphen{}old infants. \sphinxstyleemphasis{Science}, 274(5294):1926–1928, 1996. URL: \sphinxurl{https://doi.org/10.1126/science.274.5294.1926}.
\bibitem[SK18]{Computational_models_of_human_language_processing:id33}
\sphinxAtStartPar
Jenny R Saffran and Natasha Z Kirkham. Infant statistical learning. \sphinxstyleemphasis{Annual review of psychology}, 69:181–203, 2018. URL: \sphinxurl{https://doi.org/10.1146/annurev-psych-122216-011805}.
\bibitem[Ste97]{Computational_models_of_human_language_processing:id32}
\sphinxAtStartPar
Luc Steels. The synthetic modeling of language origins. \sphinxstyleemphasis{Evolution of communication}, 1(1):1–34, 1997. URL: \sphinxurl{https://doi.org/10.1075/eoc.1.1.02ste}.
\bibitem[TG11]{Computational_models_of_human_language_processing:id31}
\sphinxAtStartPar
Jason A Tourville and Frank H Guenther. The diva model: a neural theory of speech acquisition and production. \sphinxstyleemphasis{Language and cognitive processes}, 26(7):952–981, 2011. URL: \sphinxurl{https://doi.org/10.1080/01690960903498424}.
\bibitem[WS12]{Computational_models_of_human_language_processing:id30}
\sphinxAtStartPar
Andrea Weber and Odette Scharenborg. Models of spoken\sphinxhyphen{}word recognition. \sphinxstyleemphasis{Wiley Interdisciplinary Reviews: Cognitive Science}, 3(3):387–401, 2012. \sphinxhref{https://doi.org/10.1002/wcs.1178}{doi:10.1002/wcs.1178}.
\bibitem[WT84]{Computational_models_of_human_language_processing:id29}
\sphinxAtStartPar
Janet F Werker and Richard C Tees. Cross\sphinxhyphen{}language speech perception: evidence for perceptual reorganization during the first year of life. \sphinxstyleemphasis{Infant behavior and development}, 7(1):49–63, 1984. URL: \sphinxurl{https://doi.org/10.1016/S0163-6383(84)80022-3}.
\bibitem[FWF13]{Security_and_privacy:id5}
\sphinxAtStartPar
Rachel L Finn, David Wright, and Michael Friedewald. Seven types of privacy. In \sphinxstyleemphasis{European data protection: coming of age}, pages 3–32. Springer, 2013. URL: \sphinxurl{https://doi.org/10.1007/978-94-007-5170-5\_1}.
\bibitem[Gas18]{Security_and_privacy:id44}
\sphinxAtStartPar
Jessica Gasiorek. \sphinxstyleemphasis{Message processing: The science of creating understanding}. UH Mānoa Outreach College, 2018. URL: \sphinxurl{http://pressbooks-dev.oer.hawaii.edu/messageprocessing/}.
\bibitem[Lar19]{Security_and_privacy:id43}
\sphinxAtStartPar
Xabier Lareo. Smart speakers and virtual assistants. \sphinxstyleemphasis{TechDispatch \#1:}, 2019. URL: \sphinxurl{https://data.europa.eu/doi/10.2804/004275}.
\bibitem[NJK+19]{Security_and_privacy:id51}
\sphinxAtStartPar
Andreas Nautsch, Catherine Jasserand, Els Kindt, Massimiliano Todisco, Isabel Trancoso, and Nicholas Evans. The gdpr \& speech data: reflections of legal and technology communities, first steps towards a common understanding. \sphinxstyleemphasis{arXiv preprint arXiv:1907.03458}, 2019. URL: \sphinxurl{https://doi.org/10.21437/Interspeech.2019-2647}.
\bibitem[1]{References:id43}
\sphinxAtStartPar
Peter Noll. A comparative study of various quantization schemes for speech encoding. \sphinxstyleemphasis{Bell System Technical Journal}, 54(9):1597–1614, 1975. URL: \sphinxurl{https://doi.org/10.1002/j.1538-7305.1975.tb02053.x}.
\bibitem[2]{References:id42}
\sphinxAtStartPar
Tom Bäckström, Jérémie Lecomte, Guillaume Fuchs, Sascha Disch, and Christian Uhle. \sphinxstyleemphasis{Speech coding: with code\sphinxhyphen{}excited linear prediction}. Springer, 2017. URL: \sphinxurl{https://doi.org/10.1007/978-3-319-50204-5}.
\bibitem[3]{References:id44}
\sphinxAtStartPar
Yundong Zhang, Naveen Suda, Liangzhen Lai, and Vikas Chandra. Hello edge: keyword spotting on microcontrollers. \sphinxstyleemphasis{arXiv preprint arXiv:1711.07128}, 2017. URL: \sphinxurl{https://doi.org/10.48550/arXiv.1711.07128}.
\bibitem[4]{References:id45}
\sphinxAtStartPar
Xiaohui Zhang. \sphinxstyleemphasis{Strategies for Handling Out\sphinxhyphen{}of\sphinxhyphen{}Vocabulary Words in Automatic Speech Recognition}. PhD thesis, Johns Hopkins University, 2019. URL: \sphinxurl{http://jhir.library.jhu.edu/handle/1774.2/62275}.
\bibitem[5]{References:id8}
\sphinxAtStartPar
Björn Schuller and Anton Batliner. \sphinxstyleemphasis{Computational paralinguistics: emotion, affect and personality in speech and language processing}. John Wiley \& Sons, 2013. URL: \sphinxurl{https://www.wiley.com/en-us/9781118706626}.
\bibitem[6]{References:id9}
\sphinxAtStartPar
Jouni Pohjalainen, Okko Räsänen, and Serdar Kadioglu. Feature selection methods and their combinations in high\sphinxhyphen{}dimensional classification of speaker likability, intelligibility and personality traits. \sphinxstyleemphasis{Computer Speech \& Language}, 29(1):145–171, 2015. URL: \sphinxurl{https://doi.org/10.1016/j.csl.2013.11.004}.
\bibitem[7]{References:id10}
\sphinxAtStartPar
Florian Eyben, Martin Wöllmer, and Björn Schuller. Opensmile: the munich versatile and fast open\sphinxhyphen{}source audio feature extractor. In \sphinxstyleemphasis{Proceedings of the 18th ACM international conference on Multimedia}, 1459–1462. 2010. URL: \sphinxurl{https://doi.org/10.1145/1873951.1874246}.
\bibitem[8]{References:id7}
\sphinxAtStartPar
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: a framework for self\sphinxhyphen{}supervised learning of speech representations. \sphinxstyleemphasis{Advances in Neural Information Processing Systems}, 33:12449–12460, 2020. URL: \sphinxurl{https://doi.org/10.48550/arXiv.2006.11477}.
\bibitem[9]{References:id6}
\sphinxAtStartPar
Yu\sphinxhyphen{}An Chung, Wei\sphinxhyphen{}Ning Hsu, Hao Tang, and James Glass. An unsupervised autoregressive model for speech representation learning. \sphinxstyleemphasis{arXiv preprint arXiv:1904.03240}, 2019. URL: \sphinxurl{https://doi.org/10.48550/arXiv.1904.03240}.
\bibitem[10]{References:id5}
\sphinxAtStartPar
Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal margin classifiers. In \sphinxstyleemphasis{Proceedings of the fifth annual workshop on Computational learning theory}, 144–152. 1992. URL: \sphinxurl{https://doi.org/10.1145/130385.130401}.
\bibitem[11]{References:id3}
\sphinxAtStartPar
Zhizheng Wu, Cassia Valentini\sphinxhyphen{}Botinhao, Oliver Watts, and Simon King. Deep neural networks employing multi\sphinxhyphen{}task learning and stacked bottleneck features for speech synthesis. In \sphinxstyleemphasis{2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)}, 4460–4464. IEEE, 2015. URL: \sphinxurl{https://doi.org/10.1109/ICASSP.2015.7178814}.
\bibitem[12]{References:id47}
\sphinxAtStartPar
Lawrence R Rabiner and Ronald W Schafer. Introduction to digital speech processing. \sphinxstyleemphasis{Foundations and Trends in Signal Processing}, 1(1):1–194, 2007. URL: \sphinxurl{https://doi.org/10.1561/2000000001}.
\bibitem[13]{References:id46}
\sphinxAtStartPar
Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis system using a large speech database. In \sphinxstyleemphasis{1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings}, volume 1, 373–376. IEEE, 1996. URL: \sphinxurl{https://doi.org/10.1109/ICASSP.1996.541110}.
\bibitem[14]{References:id38}
\sphinxAtStartPar
Jacob Benesty, M Mohan Sondhi, Yiteng Huang, and others. \sphinxstyleemphasis{Springer handbook of speech processing}. Volume 1. Springer, 2008. URL: \sphinxurl{https://doi.org/10.1007/978-3-540-49127-9}.
\bibitem[15]{References:id37}
\sphinxAtStartPar
Rainer Martin. Noise power spectral density estimation based on optimal smoothing and minimum statistics. \sphinxstyleemphasis{IEEE Transactions on speech and audio processing}, 9(5):504–512, 2001. URL: \sphinxurl{https://doi.org/10.1109/89.928915}.
\bibitem[16]{References:id39}
\sphinxAtStartPar
Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. \sphinxstyleemphasis{IEEE Transactions on acoustics, speech, and signal processing}, 27(2):113–120, 1979. URL: \sphinxurl{https://doi.org/10.1109/TASSP.1979.1163209}.
\bibitem[17]{References:id33}
\sphinxAtStartPar
William Havard, Laurent Besacier, and Olivier Rosec. Speech\sphinxhyphen{}coco: 600k visually grounded spoken captions aligned to mscoco data set. \sphinxstyleemphasis{arXiv preprint arXiv:1707.08435}, 2017. URL: \sphinxurl{https://doi.org/10.21437/GLU.2017-9}.
\bibitem[18]{References:id26}
\sphinxAtStartPar
James L McClelland and Jeffrey L Elman. The trace model of speech perception. \sphinxstyleemphasis{Cognitive psychology}, 18(1):1–86, 1986. URL: \sphinxurl{https://doi.org/10.1016/0010-0285(86)90015-0}.
\bibitem[19]{References:id20}
\sphinxAtStartPar
Okko Räsänen. Computational modeling of phonetic and lexical learning in early language acquisition: existing models and future directions. \sphinxstyleemphasis{Speech Communication}, 54(9):975–997, 2012. URL: \sphinxurl{https://doi.org/10.1016/j.specom.2012.05.001}.
\bibitem[20]{References:id34}
\sphinxAtStartPar
Emmanuel Dupoux. Cognitive science in the era of artificial intelligence: a roadmap for reverse\sphinxhyphen{}engineering the infant language\sphinxhyphen{}learner. \sphinxstyleemphasis{Cognition}, 173:43–59, 2018. URL: \sphinxurl{https://doi.org/10.1016/j.cognition.2017.11.008}.
\bibitem[21]{References:id14}
\sphinxAtStartPar
Luc Steels. The synthetic modeling of language origins. \sphinxstyleemphasis{Evolution of communication}, 1(1):1–34, 1997. URL: \sphinxurl{https://doi.org/10.1075/eoc.1.1.02ste}.
\bibitem[22]{References:id29}
\sphinxAtStartPar
Simon Kirby. Natural language from artificial life. \sphinxstyleemphasis{Artificial life}, 8(2):185–215, 2002. URL: \sphinxurl{https://doi.org/10.1162/106454602320184248}.
\bibitem[23]{References:id28}
\sphinxAtStartPar
David Marr. \sphinxstyleemphasis{Vision: A computational investigation into the human representation and processing of visual information}. W.H. Freeman and Company, 1982.
\bibitem[24]{References:id12}
\sphinxAtStartPar
Andrea Weber and Odette Scharenborg. Models of spoken\sphinxhyphen{}word recognition. \sphinxstyleemphasis{Wiley Interdisciplinary Reviews: Cognitive Science}, 3(3):387–401, 2012. \sphinxhref{https://doi.org/10.1002/wcs.1178}{doi:10.1002/wcs.1178}.
\bibitem[25]{References:id27}
\sphinxAtStartPar
James S Magnuson, Heejo You, Sahil Luthra, Monica Li, Hosung Nam, Monty Escabi, Kevin Brown, Paul D Allopenna, Rachel M Theodore, Nicholas Monto, and others. Earshot: a minimal neural network model of incremental human speech recognition. \sphinxstyleemphasis{Cognitive science}, 44(4):e12823, 2020. URL: \sphinxurl{https://doi.org/10.1111/cogs.12823}.
\bibitem[26]{References:id11}
\sphinxAtStartPar
Janet F Werker and Richard C Tees. Cross\sphinxhyphen{}language speech perception: evidence for perceptual reorganization during the first year of life. \sphinxstyleemphasis{Infant behavior and development}, 7(1):49–63, 1984. URL: \sphinxurl{https://doi.org/10.1016/S0163-6383(84)80022-3}.
\bibitem[27]{References:id16}
\sphinxAtStartPar
Jenny R Saffran, Richard N Aslin, and Elissa L Newport. Statistical learning by 8\sphinxhyphen{}month\sphinxhyphen{}old infants. \sphinxstyleemphasis{Science}, 274(5294):1926–1928, 1996. URL: \sphinxurl{https://doi.org/10.1126/science.274.5294.1926}.
\bibitem[28]{References:id24}
\sphinxAtStartPar
Jessica Maye, Janet F Werker, and LouAnn Gerken. Infant sensitivity to distributional information can affect phonetic discrimination. \sphinxstyleemphasis{Cognition}, 82(3):B101–B111, 2002. URL: \sphinxurl{https://doi.org/10.1016/S0010-0277(01)00157-3}.
\bibitem[29]{References:id15}
\sphinxAtStartPar
Jenny R Saffran and Natasha Z Kirkham. Infant statistical learning. \sphinxstyleemphasis{Annual review of psychology}, 69:181–203, 2018. URL: \sphinxurl{https://doi.org/10.1146/annurev-psych-122216-011805}.
\bibitem[30]{References:id23}
\sphinxAtStartPar
Tasha Nagamine, Michael L Seltzer, and Nima Mesgarani. Exploring how deep neural networks form phonemic categories. In \sphinxstyleemphasis{Sixteenth Annual Conference of the International Speech Communication Association}. 2015. URL: \sphinxurl{https://www.isca-speech.org/archive\_v0/interspeech\_2015/papers/i15\_1912.pdf}.
\bibitem[31]{References:id19}
\sphinxAtStartPar
Okko Räsänen and Heikki Rasilo. A joint model of word segmentation and meaning acquisition through cross\sphinxhyphen{}situational learning. \sphinxstyleemphasis{Psychological review}, 122(4):792, 2015. URL: \sphinxurl{https://psycnet.apa.org/doi/10.1037/a0039702}.
\bibitem[32]{References:id31}
\sphinxAtStartPar
Sofoklis Kakouros and Okko Räsänen. 3pro–an unsupervised method for the automatic detection of sentence prominence in speech. \sphinxstyleemphasis{Speech Communication}, 82:67–84, 2016. URL: \sphinxurl{https://doi.org/10.1016/j.specom.2016.06.004}.
\bibitem[33]{References:id30}
\sphinxAtStartPar
Herman Kamper, Aren Jansen, and Sharon Goldwater. A segmental framework for fully\sphinxhyphen{}unsupervised large\sphinxhyphen{}vocabulary speech recognition. \sphinxstyleemphasis{Computer Speech \& Language}, 46:154–174, 2017. URL: \sphinxurl{https://doi.org/10.1016/j.csl.2017.04.008}.
\bibitem[34]{References:id17}
\sphinxAtStartPar
Okko Räsänen, Gabriel Doyle, and Michael C Frank. Pre\sphinxhyphen{}linguistic segmentation of speech into syllable\sphinxhyphen{}like units. \sphinxstyleemphasis{Cognition}, 171:130–150, 2018. URL: \sphinxurl{https://doi.org/10.1016/j.cognition.2017.11.003}.
\bibitem[35]{References:id22}
\sphinxAtStartPar
Dennis Norris. Shortlist: a connectionist model of continuous speech recognition. \sphinxstyleemphasis{Cognition}, 52(3):189–234, 1994. URL: \sphinxurl{https://doi.org/10.1016/0010-0277(94)90043-4}.
\bibitem[36]{References:id25}
\sphinxAtStartPar
Shinji Maeda. Improved articulatory models. \sphinxstyleemphasis{The Journal of the Acoustical Society of America}, 84(S1):S146–S146, 1988. URL: \sphinxurl{https://doi.org/10.1121/1.2025845}.
\bibitem[37]{References:id36}
\sphinxAtStartPar
Peter Birkholz. \sphinxstyleemphasis{3D\sphinxhyphen{}Artikulatorische Sprachsynthese}. PhD thesis, der Universität Rostock, 2005. URL: \sphinxurl{https://www.vocaltractlab.de/publications/birkholz-2005-dissertation.pdf}.
\bibitem[38]{References:id35}
\sphinxAtStartPar
Peter Birkholz, Lucia Martin, Klaus Willmes, Bernd J Kröger, and Christiane Neuschaefer\sphinxhyphen{}Rube. The contribution of phonation type to the perception of vocal emotions in german: an articulatory synthesis study. \sphinxstyleemphasis{The Journal of the Acoustical Society of America}, 137(3):1503–1512, 2015. URL: \sphinxurl{https://doi.org/10.1121/1.4906836}.
\bibitem[39]{References:id13}
\sphinxAtStartPar
Jason A Tourville and Frank H Guenther. The diva model: a neural theory of speech acquisition and production. \sphinxstyleemphasis{Language and cognitive processes}, 26(7):952–981, 2011. URL: \sphinxurl{https://doi.org/10.1080/01690960903498424}.
\bibitem[40]{References:id32}
\sphinxAtStartPar
Ian S Howard and Piers Messum. Learning to pronounce first words in three languages: an investigation of caregiver and infant behavior using a computational model of an infant. \sphinxstyleemphasis{PLoS One}, 9(10):e110334, 2014. URL: \sphinxurl{https://doi.org/10.1371/journal.pone.0110334}.
\bibitem[41]{References:id18}
\sphinxAtStartPar
Heikki Rasilo and Okko Räsänen. An online model for vowel imitation learning. \sphinxstyleemphasis{Speech Communication}, 86:1–23, 2017. URL: \sphinxurl{https://doi.org/10.1016/j.specom.2016.10.010}.
\bibitem[42]{References:id21}
\sphinxAtStartPar
Pierre\sphinxhyphen{}Yves Oudeyer, George Kachergis, and William Schueller. Computational and robotic models of early language development: a review. In J.S. Horst and J. von Koss Torkildsen, editors, \sphinxstyleemphasis{International handbook of language acquisition}. Routledge/Taylor \& Francis Group, 2019. URL: \sphinxurl{https://psycnet.apa.org/doi/10.4324/9781315110622-5}.
\bibitem[43]{References:id40}
\sphinxAtStartPar
Xabier Lareo. Smart speakers and virtual assistants. \sphinxstyleemphasis{TechDispatch \#1:}, 2019. URL: \sphinxurl{https://data.europa.eu/doi/10.2804/004275}.
\bibitem[44]{References:id48}
\sphinxAtStartPar
Andreas Nautsch, Catherine Jasserand, Els Kindt, Massimiliano Todisco, Isabel Trancoso, and Nicholas Evans. The gdpr \& speech data: reflections of legal and technology communities, first steps towards a common understanding. \sphinxstyleemphasis{arXiv preprint arXiv:1907.03458}, 2019. URL: \sphinxurl{https://doi.org/10.21437/Interspeech.2019-2647}.
\bibitem[45]{References:id41}
\sphinxAtStartPar
Jessica Gasiorek. \sphinxstyleemphasis{Message processing: The science of creating understanding}. UH Mānoa Outreach College, 2018. URL: \sphinxurl{http://pressbooks-dev.oer.hawaii.edu/messageprocessing/}.
\bibitem[46]{References:id2}
\sphinxAtStartPar
Rachel L Finn, David Wright, and Michael Friedewald. Seven types of privacy. In \sphinxstyleemphasis{European data protection: coming of age}, pages 3–32. Springer, 2013. URL: \sphinxurl{https://doi.org/10.1007/978-94-007-5170-5\_1}.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Index}
\printindex
\end{document}